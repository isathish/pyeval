{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PyEval","text":"![PyEval](https://img.shields.io/badge/PyEval-v1.0.0-blue?style=for-the-badge)  **A Comprehensive Pure Python Evaluation Framework**  *Evaluate everything, depend on nothing.*  [![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/) [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT) [![Dependencies](https://img.shields.io/badge/dependencies-none-brightgreen.svg)](.) [![Tests](https://img.shields.io/badge/tests-302%20passed-success.svg)](.)"},{"location":"#what-is-pyeval","title":"What is PyEval?","text":"<p>PyEval is a zero-dependency evaluation library for Machine Learning, NLP, LLM, RAG, Fairness, Speech, and Recommender systems. Every metric is implemented in pure Python \u2014 no NumPy, no scikit-learn, no external packages required.</p>"},{"location":"#why-choose-pyeval","title":"Why Choose PyEval?","text":"Feature Description \ud83d\udeab Zero Dependencies Works anywhere Python runs \u2014 edge devices, serverless, restricted environments \ud83d\udce6 327+ Public APIs The most comprehensive evaluation library available \ud83d\udd27 Unified Interface Consistent API design across all domains \ud83e\uddea Battle-Tested 302 tests ensure reliability and correctness \ud83d\udcca Built-in Viz ASCII charts, confusion matrices, sparklines included"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Using pip (recommended):</p> <pre><code>pip install pyeval\n</code></pre> <p>From source:</p> <pre><code>git clone https://github.com/isathish/pyeval.git\ncd pyeval\npip install -e .\n</code></pre>"},{"location":"#30-second-example","title":"30-Second Example","text":"<pre><code>from pyeval import accuracy_score, f1_score, bleu_score, confusion_matrix\n\n# ML Classification\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]\n\nprint(f\"Accuracy: {accuracy_score(y_true, y_pred):.2%}\")  # 83.33%\nprint(f\"F1 Score: {f1_score(y_true, y_pred):.4f}\")       # 0.8571\n\n# NLP Generation\nref = \"The quick brown fox jumps over the lazy dog\"\nhyp = \"A fast brown fox leaps over the lazy dog\"\nprint(f\"BLEU: {bleu_score(ref, hyp):.4f}\")              # 0.4234\n\n# Confusion Matrix (ASCII visualization!)\nprint(confusion_matrix(y_true, y_pred, display=True))\n</code></pre>"},{"location":"#documentation-overview","title":"Documentation Overview","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>New to PyEval? Start with the Getting Started Guide for installation and basic usage.</p>"},{"location":"#api-reference","title":"API Reference","text":"Module Description Link ML Metrics Classification, Regression, Clustering \u2014 40+ metrics View API NLP Metrics BLEU, ROUGE, METEOR, TER, BERTScore, and more View API LLM Evaluation Toxicity, Hallucination, Coherence, Bias detection View API RAG Evaluation Context Relevance, Groundedness, Faithfulness View API Fairness Metrics Demographic Parity, Equalized Odds, Disparate Impact View API Speech Metrics WER, CER, MER, and speech quality evaluation View API Recommender Metrics Precision@K, NDCG, MAP, MRR, Diversity, Coverage View API Statistical Utilities Hypothesis testing, confidence intervals, distributions View API Visualization ASCII charts, sparklines, progress bars View API"},{"location":"#advanced-features","title":"Advanced Features","text":"Feature Description Link Pipelines Chain evaluation steps together Learn More Decorators Add validation, logging, retry logic Learn More Validators Type checking and data validation Learn More Design Patterns Reusable patterns for evaluation Learn More Functional Utilities Map, filter, reduce for metrics Learn More"},{"location":"#feature-highlights","title":"Feature Highlights","text":""},{"location":"#complete-domain-coverage","title":"Complete Domain Coverage","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        PyEval v1.0.0                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  ML            \u2502  NLP           \u2502  LLM           \u2502  RAG         \u2502\n\u2502  \u251c\u2500 classify   \u2502  \u251c\u2500 bleu       \u2502  \u251c\u2500 toxicity   \u2502  \u251c\u2500 context  \u2502\n\u2502  \u251c\u2500 regress    \u2502  \u251c\u2500 rouge      \u2502  \u251c\u2500 coherence  \u2502  \u251c\u2500 ground   \u2502\n\u2502  \u251c\u2500 cluster    \u2502  \u251c\u2500 meteor     \u2502  \u251c\u2500 hallucin   \u2502  \u251c\u2500 faithful \u2502\n\u2502  \u2514\u2500 rank       \u2502  \u2514\u2500 ter        \u2502  \u2514\u2500 bias       \u2502  \u2514\u2500 answer   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Fairness      \u2502  Speech        \u2502  Recommender   \u2502  Utilities   \u2502\n\u2502  \u251c\u2500 parity     \u2502  \u251c\u2500 wer        \u2502  \u251c\u2500 precision  \u2502  \u251c\u2500 stats    \u2502\n\u2502  \u251c\u2500 equality   \u2502  \u251c\u2500 cer        \u2502  \u251c\u2500 ndcg       \u2502  \u251c\u2500 viz      \u2502\n\u2502  \u2514\u2500 calibrate  \u2502  \u2514\u2500 mer        \u2502  \u2514\u2500 diversity  \u2502  \u2514\u2500 valid    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#zero-dependencies","title":"Zero Dependencies","text":"<p>PyEval works in environments where other libraries can't:</p> <ul> <li>Edge Devices \u2014 Raspberry Pi, microcontrollers, IoT</li> <li>Serverless \u2014 AWS Lambda, Azure Functions, Google Cloud Functions</li> <li>Restricted Environments \u2014 Air-gapped systems, secure facilities</li> <li>Embedded Systems \u2014 No package manager required</li> <li>Minimal Docker \u2014 Tiny container images</li> </ul>"},{"location":"#consistent-api-design","title":"Consistent API Design","text":"<p>All metrics follow the same patterns:</p> <pre><code># Pattern 1: Simple comparison\nscore = metric_function(y_true, y_pred)\n\n# Pattern 2: With options\nscore = metric_function(y_true, y_pred, **options)\n\n# Pattern 3: Batch processing  \nscores = [metric_function(t, p) for t, p in zip(true_batch, pred_batch)]\n</code></pre>"},{"location":"#quick-examples-by-domain","title":"Quick Examples by Domain","text":""},{"location":"#machine-learning","title":"Machine Learning","text":"<pre><code>from pyeval import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    mean_squared_error, r2_score, silhouette_score\n)\n\n# Classification\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0]\n\nprint(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\nprint(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\nprint(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\nprint(f\"F1:        {f1_score(y_true, y_pred):.4f}\")\n</code></pre>"},{"location":"#natural-language-processing","title":"Natural Language Processing","text":"<pre><code>from pyeval import bleu_score, rouge_score, meteor_score\n\nreference = \"The cat sat on the mat\"\nhypothesis = \"A cat was sitting on the mat\"\n\nprint(f\"BLEU:   {bleu_score(reference, hypothesis):.4f}\")\nprint(f\"ROUGE:  {rouge_score(reference, hypothesis)}\")\nprint(f\"METEOR: {meteor_score(reference, hypothesis):.4f}\")\n</code></pre>"},{"location":"#llm-evaluation","title":"LLM Evaluation","text":"<pre><code>from pyeval import (\n    toxicity_score, coherence_score, \n    hallucination_score, readability_score\n)\n\ntext = \"This is a sample generated response from an LLM.\"\ncontext = \"Information about the topic being discussed.\"\n\nprint(f\"Toxicity:     {toxicity_score(text):.4f}\")\nprint(f\"Coherence:    {coherence_score(text):.4f}\")\nprint(f\"Readability:  {readability_score(text):.4f}\")\n</code></pre>"},{"location":"#rag-evaluation","title":"RAG Evaluation","text":"<pre><code>from pyeval import (\n    context_relevance_score, groundedness_score,\n    answer_relevance_score, faithfulness_score\n)\n\nquery = \"What is machine learning?\"\ncontext = \"Machine learning is a subset of AI that enables systems to learn.\"\nanswer = \"Machine learning is an AI technique for learning from data.\"\n\nprint(f\"Context Relevance: {context_relevance_score(query, context):.4f}\")\nprint(f\"Groundedness:      {groundedness_score(answer, context):.4f}\")\nprint(f\"Answer Relevance:  {answer_relevance_score(query, answer):.4f}\")\n</code></pre>"},{"location":"#comparison-with-other-libraries","title":"Comparison with Other Libraries","text":"Feature PyEval scikit-learn Evaluate (HF) TorchMetrics Dependencies None NumPy, SciPy 15+ packages PyTorch ML Metrics \u2705 40+ \u2705 30+ \u26a0\ufe0f Limited \u2705 25+ NLP Metrics \u2705 20+ \u274c \u2705 20+ \u26a0\ufe0f Limited LLM Metrics \u2705 15+ \u274c \u26a0\ufe0f Limited \u274c RAG Metrics \u2705 10+ \u274c \u274c \u274c Fairness \u2705 10+ \u274c \u274c \u274c Speech \u2705 5+ \u274c \u26a0\ufe0f Limited \u274c Recommender \u2705 10+ \u274c \u274c \u274c Edge Deploy \u2705 \u274c \u274c \u274c Serverless \u2705 \u26a0\ufe0f \u26a0\ufe0f \u274c"},{"location":"#get-involved","title":"Get Involved","text":"<ul> <li>GitHub Repository \u2014 Star us, report issues, contribute</li> <li>Contributing Guide \u2014 How to contribute to PyEval</li> <li>Changelog \u2014 What's new in each release</li> </ul>"},{"location":"#license","title":"License","text":"<p>PyEval is released under the MIT License. See the LICENSE file for details.</p>   **Made with \u2764\ufe0f for the ML community**  *Zero dependencies. Maximum evaluation coverage.*"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to PyEval will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Nothing yet</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Nothing yet</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":"<ul> <li>Nothing yet</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Nothing yet</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Nothing yet</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>Nothing yet</li> </ul>"},{"location":"changelog/#100-2024-01-15","title":"1.0.0 - 2024-01-15","text":""},{"location":"changelog/#added_1","title":"Added","text":""},{"location":"changelog/#core-metrics","title":"Core Metrics","text":"<ul> <li>ML Classification Metrics</li> <li><code>accuracy_score</code> - Overall accuracy measurement</li> <li><code>precision_score</code> - Precision with multi-class support</li> <li><code>recall_score</code> - Recall with multi-class support</li> <li><code>f1_score</code> - F1 score with averaging options</li> <li><code>fbeta_score</code> - F\u03b2 score with configurable beta</li> <li><code>confusion_matrix</code> - Full confusion matrix computation</li> <li><code>roc_auc_score</code> - ROC AUC for binary and multi-class</li> <li><code>average_precision_score</code> - Average precision (PR-AUC)</li> <li><code>matthews_corrcoef</code> - Matthews Correlation Coefficient</li> <li><code>cohen_kappa_score</code> - Cohen's Kappa</li> <li><code>hamming_loss</code> - Hamming loss</li> <li><code>hinge_loss</code> - Hinge loss</li> <li><code>log_loss</code> - Log loss (cross-entropy)</li> <li><code>jaccard_score</code> - Jaccard similarity</li> <li><code>balanced_accuracy_score</code> - Balanced accuracy</li> <li> <p><code>top_k_accuracy_score</code> - Top-K accuracy</p> </li> <li> <p>ML Regression Metrics</p> </li> <li><code>mean_squared_error</code> (MSE)</li> <li><code>root_mean_squared_error</code> (RMSE)</li> <li><code>mean_absolute_error</code> (MAE)</li> <li><code>mean_absolute_percentage_error</code> (MAPE)</li> <li><code>r2_score</code> - R\u00b2 coefficient of determination</li> <li><code>explained_variance_score</code></li> <li><code>max_error</code></li> <li><code>median_absolute_error</code></li> <li><code>mean_squared_log_error</code> (MSLE)</li> <li><code>mean_poisson_deviance</code></li> <li><code>mean_gamma_deviance</code></li> <li><code>mean_tweedie_deviance</code></li> <li> <p><code>d2_tweedie_score</code></p> </li> <li> <p>NLP Metrics</p> </li> <li><code>bleu_score</code> - BLEU (1-4 grams)</li> <li><code>rouge_score</code> - ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum</li> <li><code>meteor_score</code> - METEOR</li> <li><code>ter_score</code> - Translation Edit Rate</li> <li><code>wer</code> - Word Error Rate</li> <li><code>cer</code> - Character Error Rate</li> <li><code>chrF_score</code> - chrF and chrF++</li> <li><code>bert_score</code> - BERTScore (precision, recall, F1)</li> <li><code>perplexity</code> - Language model perplexity</li> <li><code>sentence_bleu</code> - Sentence-level BLEU</li> <li> <p><code>corpus_bleu</code> - Corpus-level BLEU</p> </li> <li> <p>LLM Metrics</p> </li> <li><code>faithfulness</code> - Response faithfulness to context</li> <li><code>relevance</code> - Response relevance to query</li> <li><code>coherence</code> - Response coherence</li> <li><code>fluency</code> - Language fluency</li> <li><code>toxicity</code> - Toxicity detection</li> <li><code>bias_detection</code> - Bias detection across categories</li> <li><code>hallucination_score</code> - Hallucination detection</li> <li><code>consistency</code> - Response consistency</li> <li><code>groundedness</code> - Response groundedness to sources</li> <li> <p><code>instruction_following</code> - Instruction adherence</p> </li> <li> <p>RAG Metrics</p> </li> <li><code>context_precision</code> - Precision of retrieved context</li> <li><code>context_recall</code> - Recall of retrieved context</li> <li><code>answer_relevance</code> - Answer relevance to query</li> <li><code>answer_faithfulness</code> - Answer faithfulness to context</li> <li><code>answer_correctness</code> - Answer correctness</li> <li><code>context_utilization</code> - Context utilization efficiency</li> <li><code>retrieval_precision</code> - Retrieval precision</li> <li><code>retrieval_recall</code> - Retrieval recall</li> <li><code>retrieval_f1</code> - Retrieval F1 score</li> <li><code>answer_similarity</code> - Answer semantic similarity</li> <li> <p><code>noise_robustness</code> - Robustness to noise in context</p> </li> <li> <p>Fairness Metrics</p> </li> <li><code>demographic_parity</code> - Demographic parity ratio</li> <li><code>equalized_odds</code> - Equalized odds (TPR/FPR)</li> <li><code>equal_opportunity</code> - Equal opportunity (TPR)</li> <li><code>predictive_parity</code> - Predictive parity (PPV)</li> <li><code>calibration</code> - Calibration fairness</li> <li><code>disparate_impact</code> - Disparate impact ratio</li> <li><code>statistical_parity_difference</code></li> <li><code>average_odds_difference</code></li> <li><code>individual_fairness</code> - Individual fairness</li> <li> <p><code>counterfactual_fairness</code> - Counterfactual fairness</p> </li> <li> <p>Speech Metrics</p> </li> <li><code>wer</code> - Word Error Rate</li> <li><code>cer</code> - Character Error Rate</li> <li><code>mer</code> - Match Error Rate</li> <li><code>wil</code> - Word Information Lost</li> <li><code>wip</code> - Word Information Preserved</li> <li><code>phoneme_error_rate</code> (PER)</li> <li><code>pesq</code> - PESQ score</li> <li><code>stoi</code> - STOI score</li> <li><code>si_sdr</code> - SI-SDR</li> <li><code>snr</code> - Signal-to-Noise Ratio</li> <li> <p><code>real_time_factor</code> (RTF)</p> </li> <li> <p>Recommender Metrics</p> </li> <li><code>precision_at_k</code> - Precision@K</li> <li><code>recall_at_k</code> - Recall@K</li> <li><code>ndcg_at_k</code> - NDCG@K</li> <li><code>map_at_k</code> - MAP@K</li> <li><code>mrr</code> - Mean Reciprocal Rank</li> <li><code>hit_rate</code> - Hit Rate</li> <li><code>coverage</code> - Catalog coverage</li> <li><code>diversity</code> - Recommendation diversity</li> <li><code>novelty</code> - Recommendation novelty</li> <li><code>serendipity</code> - Recommendation serendipity</li> <li><code>personalization</code> - Personalization score</li> </ul>"},{"location":"changelog/#framework-features","title":"Framework Features","text":"<ul> <li>Decorators</li> <li><code>@timed</code> - Execution time measurement</li> <li><code>@memoize</code> - Result caching</li> <li><code>@retry</code> - Automatic retry with backoff</li> <li><code>@logged</code> - Automatic logging</li> <li><code>@require_same_length</code> - Input validation</li> <li><code>@require_non_empty</code> - Non-empty validation</li> <li><code>@deprecated</code> - Deprecation warnings</li> <li> <p><code>@experimental</code> - Experimental feature marking</p> </li> <li> <p>Validators</p> </li> <li><code>TypeValidator</code> - Type checking</li> <li><code>ListValidator</code> - List/array validation</li> <li><code>NumericValidator</code> - Numeric range/type validation</li> <li><code>StringValidator</code> - String format validation</li> <li> <p><code>SchemaValidator</code> - JSON schema validation</p> </li> <li> <p>Patterns</p> </li> <li>Strategy Pattern - Swappable metric implementations</li> <li>Factory Pattern - Metric creation</li> <li>Composite Pattern - Combined metrics</li> <li>Builder Pattern - Fluent configuration</li> <li> <p>Observer Pattern - Metric event handling</p> </li> <li> <p>Pipelines</p> </li> <li><code>Pipeline</code> - Fluent evaluation pipeline</li> <li><code>ConditionalPipeline</code> - Conditional branching</li> <li><code>CompositePipeline</code> - Pipeline composition</li> <li> <p><code>ParallelPipeline</code> - Parallel execution</p> </li> <li> <p>Functional Utilities</p> </li> <li><code>compose</code> / <code>pipe</code> - Function composition</li> <li><code>curry</code> / <code>partial</code> - Partial application</li> <li><code>memoize</code> / <code>memoize_with_ttl</code> - Caching</li> <li> <p><code>Maybe</code> / <code>Either</code> / <code>Result</code> - Monads</p> </li> <li> <p>Statistical Utilities</p> </li> <li>Confidence intervals (bootstrap, normal, Wilson)</li> <li>Significance tests (paired t-test, McNemar, bootstrap)</li> <li>Multiple comparison corrections (Bonferroni, Holm)</li> <li> <p>Effect size calculations (Cohen's d, Cliff's delta)</p> </li> <li> <p>Visualization</p> </li> <li>Confusion matrix plots</li> <li>ROC/PR curves</li> <li>Calibration plots</li> <li>Metric comparison charts</li> <li>Dashboard generation</li> </ul>"},{"location":"changelog/#notes","title":"Notes","text":"<ul> <li>Zero external dependencies (pure Python)</li> <li>Python 3.12+ compatible</li> <li>302 comprehensive tests</li> <li>327+ public exports</li> </ul>"},{"location":"changelog/#version-history-summary","title":"Version History Summary","text":"Version Date Highlights 1.0.0 2026-01-04 Initial release with 327+ metrics and utilities"},{"location":"contributing/","title":"Contributing to PyEval","text":"<p>Thank you for your interest in contributing to PyEval! This guide will help you get started.</p>"},{"location":"contributing/#code-of-conduct","title":"\ud83d\udccb Code of Conduct","text":"<p>By participating in this project, you agree to maintain a respectful and inclusive environment for everyone.</p>"},{"location":"contributing/#ways-to-contribute","title":"\ud83c\udfaf Ways to Contribute","text":"Contribution Type Difficulty Impact \ud83d\udc1b Bug Reports Easy High \ud83d\udcdd Documentation Easy Medium \u2728 Feature Requests Easy Medium \ud83e\uddea Writing Tests Medium High \ud83d\udd27 Bug Fixes Medium High \ud83d\ude80 New Features Hard High"},{"location":"contributing/#reporting-bugs","title":"\ud83d\udc1b Reporting Bugs","text":"<ol> <li>Search existing issues to avoid duplicates</li> <li>Create a new issue with the following template:</li> </ol> <p><pre><code>### Bug Report\n\n**Description:** Brief description of the bug\n\n**Steps to Reproduce:**\n1. Step one\n2. Step two\n3. Step three\n\n**Expected Behavior:** What should happen\n\n**Actual Behavior:** What actually happens\n\n**Environment:**\n- Python: 3.x.x\n- PyEval: x.x.x\n- OS: [Windows/macOS/Linux]\n\n**Minimal Example:**\n```python\nfrom pyeval import metric_name\n# Code that demonstrates the bug\n</code></pre> <pre><code>---\n\n## \u2728 Suggesting Features\n\n1. **Check existing issues** for similar suggestions\n2. **Create a feature request** with:\n   - Clear use case\n   - Proposed API design\n   - Examples of how it would work\n\n---\n\n## \ud83d\udcbb Contributing Code\n\n### Setting Up Development Environment\n\n```bash\n# Fork and clone the repository\ngit clone https://github.com/YOUR_USERNAME/pyeval.git\ncd pyeval\n\n# Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Verify setup\npytest tests/ -q\nruff check pyeval/\nblack --check pyeval/ tests/\n</code></pre></p>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<pre><code>graph LR\n    A[Fork] --&gt; B[Branch]\n    B --&gt; C[Code]\n    C --&gt; D[Test]\n    D --&gt; E[Lint]\n    E --&gt; F[PR]\n    F --&gt; G[Review]\n    G --&gt; H[Merge]\n</code></pre> <ol> <li> <p>Create a branch from <code>main</code>:    <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/bug-description\n</code></pre></p> </li> <li> <p>Make your changes following our coding standards</p> </li> <li> <p>Add tests for new functionality</p> </li> <li> <p>Run the test suite:    <pre><code>pytest tests/ -v\npytest tests/ --cov=pyeval  # With coverage\n</code></pre></p> </li> <li> <p>Run linting and formatting:    <pre><code>ruff check pyeval/\nblack pyeval/ tests/\n</code></pre></p> </li> <li> <p>Commit your changes (use conventional commits):    <pre><code>git add .\ngit commit -m \"feat: add new metric for X\"\n# or: fix: correct calculation in Y\n# or: docs: update API reference for Z\n</code></pre></p> </li> <li> <p>Push and create a Pull Request:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> </ol>"},{"location":"contributing/#coding-standards","title":"\ud83d\udccf Coding Standards","text":""},{"location":"contributing/#style-guide","title":"Style Guide","text":"Rule Standard Style PEP 8 Formatter Black (88 chars) Linter Ruff Type Hints Required Docstrings Google style"},{"location":"contributing/#code-template","title":"Code Template","text":"<pre><code>\"\"\"Module docstring describing the module's purpose.\"\"\"\n\nfrom typing import List, Optional, Union\n\n__all__ = ['public_function', 'PublicClass']\n\n\ndef public_function(\n    param1: List[int],\n    param2: str,\n    *,\n    optional_param: Optional[float] = None\n) -&gt; float:\n    \"\"\"\n    Brief description of the function.\n\n    Longer description if needed. Explain the algorithm,\n    edge cases, or any important details.\n\n    Args:\n        param1: Description of first parameter.\n        param2: Description of second parameter.\n        optional_param: Description of optional parameter.\n            Defaults to None.\n\n    Returns:\n        Description of the return value.\n\n    Raises:\n        ValueError: When param1 is empty.\n        TypeError: When param2 is not a string.\n\n    Examples:\n        &gt;&gt;&gt; public_function([1, 2, 3], \"test\")\n        0.75\n\n        &gt;&gt;&gt; public_function([1, 2], \"test\", optional_param=0.5)\n        0.80\n\n    Notes:\n        Any additional notes about the implementation.\n\n    References:\n        - Paper or source reference if applicable\n    \"\"\"\n    if not param1:\n        raise ValueError(\"param1 cannot be empty\")\n\n    # Implementation\n    result = 0.0\n\n    return result\n</code></pre>"},{"location":"contributing/#docstring-format","title":"Docstring Format","text":"<p>We use Google-style docstrings:</p> <pre><code>def example_metric(y_true: List[int], y_pred: List[int]) -&gt; float:\n    \"\"\"\n    Calculate the example metric.\n\n    Args:\n        y_true: Ground truth labels.\n        y_pred: Predicted labels.\n\n    Returns:\n        The computed metric value between 0 and 1.\n\n    Raises:\n        ValueError: If inputs have different lengths.\n\n    Examples:\n        &gt;&gt;&gt; example_metric([1, 0, 1], [1, 0, 0])\n        0.667\n    \"\"\"\n</code></pre>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>Always use type hints:</p> <pre><code>from typing import List, Dict, Optional, Union, Tuple, Callable\n\ndef metric(\n    y_true: List[int],\n    y_pred: List[int],\n    *,\n    average: str = 'binary',\n    sample_weight: Optional[List[float]] = None\n) -&gt; Union[float, Dict[str, float]]:\n    ...\n</code></pre>"},{"location":"contributing/#adding-new-metrics","title":"Adding New Metrics","text":""},{"location":"contributing/#metric-implementation-checklist","title":"Metric Implementation Checklist","text":"<ul> <li>[ ] Function follows the standard signature pattern</li> <li>[ ] Has comprehensive docstring with examples</li> <li>[ ] Includes type hints</li> <li>[ ] Has input validation</li> <li>[ ] Has unit tests (aim for &gt;95% coverage)</li> <li>[ ] Added to <code>__all__</code> in module</li> <li>[ ] Added to module's <code>__init__.py</code></li> <li>[ ] Added to main <code>pyeval/__init__.py</code></li> <li>[ ] Documented in API reference</li> </ul>"},{"location":"contributing/#example-adding-a-new-metric","title":"Example: Adding a New Metric","text":"<pre><code># In pyeval/ml/classification.py\n\nfrom typing import List\n\n__all__ = [..., 'new_metric_score']\n\n\ndef new_metric_score(\n    y_true: List[int],\n    y_pred: List[int],\n    *,\n    normalize: bool = True\n) -&gt; float:\n    \"\"\"\n    Calculate the new metric score.\n\n    The new metric measures... [description]\n\n    Args:\n        y_true: Ground truth binary labels.\n        y_pred: Predicted binary labels.\n        normalize: Whether to normalize the result.\n            Defaults to True.\n\n    Returns:\n        The metric value. If normalize=True, returns\n        a value between 0 and 1.\n\n    Raises:\n        ValueError: If y_true and y_pred have different lengths.\n        ValueError: If inputs contain values other than 0 and 1.\n\n    Examples:\n        &gt;&gt;&gt; new_metric_score([1, 0, 1, 1], [1, 0, 0, 1])\n        0.75\n\n        &gt;&gt;&gt; new_metric_score([1, 0, 1], [1, 1, 1], normalize=False)\n        2.0\n\n    References:\n        - Author et al. \"Paper Title\". Conference/Journal, Year.\n          https://doi.org/...\n    \"\"\"\n    # Validation\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"y_true and y_pred must have same length, \"\n            f\"got {len(y_true)} and {len(y_pred)}\"\n        )\n\n    if not y_true:\n        raise ValueError(\"Inputs cannot be empty\")\n\n    # Implementation\n    score = sum(t == p for t, p in zip(y_true, y_pred))\n\n    if normalize:\n        score = score / len(y_true)\n\n    return score\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<pre><code># In tests/test_ml/test_classification.py\n\nimport pytest\nfrom pyeval import new_metric_score\n\n\nclass TestNewMetricScore:\n    \"\"\"Tests for new_metric_score function.\"\"\"\n\n    def test_perfect_predictions(self):\n        \"\"\"Test with perfect predictions.\"\"\"\n        y_true = [1, 0, 1, 0]\n        y_pred = [1, 0, 1, 0]\n        assert new_metric_score(y_true, y_pred) == 1.0\n\n    def test_all_wrong_predictions(self):\n        \"\"\"Test with all incorrect predictions.\"\"\"\n        y_true = [1, 0, 1, 0]\n        y_pred = [0, 1, 0, 1]\n        assert new_metric_score(y_true, y_pred) == 0.0\n\n    def test_partial_correct(self):\n        \"\"\"Test with partially correct predictions.\"\"\"\n        y_true = [1, 0, 1, 1]\n        y_pred = [1, 0, 0, 1]\n        assert new_metric_score(y_true, y_pred) == 0.75\n\n    def test_normalize_false(self):\n        \"\"\"Test with normalize=False.\"\"\"\n        y_true = [1, 0, 1]\n        y_pred = [1, 1, 1]\n        result = new_metric_score(y_true, y_pred, normalize=False)\n        assert result == 2.0\n\n    def test_empty_input_raises(self):\n        \"\"\"Test that empty inputs raise ValueError.\"\"\"\n        with pytest.raises(ValueError, match=\"cannot be empty\"):\n            new_metric_score([], [])\n\n    def test_length_mismatch_raises(self):\n        \"\"\"Test that mismatched lengths raise ValueError.\"\"\"\n        with pytest.raises(ValueError, match=\"same length\"):\n            new_metric_score([1, 0], [1, 0, 1])\n\n    def test_single_element(self):\n        \"\"\"Test with single element inputs.\"\"\"\n        assert new_metric_score([1], [1]) == 1.0\n        assert new_metric_score([1], [0]) == 0.0\n\n    @pytest.mark.parametrize(\"y_true,y_pred,expected\", [\n        ([1, 1, 1, 1], [1, 1, 1, 1], 1.0),\n        ([0, 0, 0, 0], [0, 0, 0, 0], 1.0),\n        ([1, 0], [0, 1], 0.0),\n        ([1, 0, 1], [1, 0, 0], 2/3),\n    ])\n    def test_various_cases(self, y_true, y_pred, expected):\n        \"\"\"Test various input combinations.\"\"\"\n        result = new_metric_score(y_true, y_pred)\n        assert abs(result - expected) &lt; 1e-10\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#updating-documentation","title":"Updating Documentation","text":"<ol> <li>API Reference: Update <code>docs/api/</code> when adding new functions</li> <li>Examples: Add to <code>docs/examples/</code> for complex features</li> <li>Changelog: Update <code>docs/changelog.md</code> for all changes</li> </ol>"},{"location":"contributing/#building-documentation-locally","title":"Building Documentation Locally","text":"<pre><code># Install MkDocs and dependencies\npip install mkdocs mkdocs-material\n\n# Serve documentation locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow Conventional Commits:</p> <pre><code>type(scope): description\n\n[optional body]\n\n[optional footer]\n</code></pre>"},{"location":"contributing/#types","title":"Types","text":"<ul> <li><code>feat</code>: New feature</li> <li><code>fix</code>: Bug fix</li> <li><code>docs</code>: Documentation only</li> <li><code>style</code>: Code style (formatting, etc.)</li> <li><code>refactor</code>: Code refactoring</li> <li><code>test</code>: Adding/updating tests</li> <li><code>chore</code>: Maintenance tasks</li> </ul>"},{"location":"contributing/#examples","title":"Examples","text":"<pre><code>feat(ml): add balanced accuracy score metric\nfix(nlp): handle empty reference in BLEU score\ndocs(api): add examples for fairness metrics\ntest(rag): add tests for context precision\nrefactor(core): simplify validation logic\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Update documentation if needed</li> <li>Add tests for new functionality</li> <li>Ensure all tests pass: <code>pytest</code></li> <li>Update CHANGELOG.md with your changes</li> <li>Request review from maintainers</li> </ol>"},{"location":"contributing/#pr-title-format","title":"PR Title Format","text":"<pre><code>type(scope): Brief description\n</code></pre>"},{"location":"contributing/#pr-description-template","title":"PR Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Documentation update\n- [ ] Refactoring\n\n## Checklist\n- [ ] Tests added/updated\n- [ ] Documentation updated\n- [ ] Changelog updated\n- [ ] All tests passing\n- [ ] Code follows style guidelines\n\n## Related Issues\nFixes #123\n</code></pre>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open a Discussion</li> <li>Check existing Issues</li> <li>Review the Documentation</li> </ul> <p>Thank you for contributing! \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get up and running with PyEval in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#using-pip-recommended","title":"Using pip (recommended)","text":"<pre><code>pip install pyeval\n</code></pre>"},{"location":"getting-started/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/isathish/pyeval.git\ncd pyeval\npip install -e .\n</code></pre>"},{"location":"getting-started/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/isathish/pyeval.git\ncd pyeval\npip install -e \".[dev]\"  # Includes test dependencies\n</code></pre>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":"<pre><code>import pyeval\nprint(f\"PyEval version: {pyeval.__version__}\")\nprint(f\"Available exports: {len([x for x in dir(pyeval) if not x.startswith('_')])}\")\n# Output: PyEval version: 1.0.0\n# Output: Available exports: 327\n</code></pre>"},{"location":"getting-started/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>No external dependencies!</li> </ul> <p>Note: PyEval is a pure Python library with zero dependencies. It works anywhere Python runs \u2014 edge devices, serverless functions, restricted environments.</p>"},{"location":"getting-started/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"getting-started/#ml-classification","title":"ML Classification","text":"<pre><code>from pyeval import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, roc_auc_score, matthews_corrcoef,\n    ClassificationMetrics\n)\n\n# Sample data\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n\n# Individual metrics\nprint(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\nprint(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\nprint(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\nprint(f\"F1 Score:  {f1_score(y_true, y_pred):.4f}\")\nprint(f\"MCC:       {matthews_corrcoef(y_true, y_pred):.4f}\")\n\n# Or compute all at once\ncm = ClassificationMetrics()\nresults = cm.compute(y_true, y_pred)\nprint(results)\n</code></pre>"},{"location":"getting-started/#ml-regression","title":"ML Regression","text":"<pre><code>from pyeval import (\n    mean_squared_error, root_mean_squared_error, \n    mean_absolute_error, r2_score\n)\n\ny_true = [3.0, -0.5, 2.0, 7.0, 4.5]\ny_pred = [2.5, 0.0, 2.1, 7.8, 4.0]\n\nprint(f\"MSE:  {mean_squared_error(y_true, y_pred):.4f}\")\nprint(f\"RMSE: {root_mean_squared_error(y_true, y_pred):.4f}\")\nprint(f\"MAE:  {mean_absolute_error(y_true, y_pred):.4f}\")\nprint(f\"R\u00b2:   {r2_score(y_true, y_pred):.4f}\")\n</code></pre>"},{"location":"getting-started/#nlp-generation","title":"NLP Generation","text":"<pre><code>from pyeval import bleu_score, rouge_score, meteor_score\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A fast brown fox leaps over the lazy dog\"\n\nprint(f\"BLEU:    {bleu_score(reference, hypothesis):.4f}\")\nprint(f\"ROUGE-L: {rouge_score(reference, hypothesis, rouge_type='l')['f']:.4f}\")\nprint(f\"METEOR:  {meteor_score(reference, hypothesis):.4f}\")\n</code></pre>"},{"location":"getting-started/#llm-evaluation","title":"LLM Evaluation","text":"<pre><code>from pyeval import toxicity_score, coherence_score, hallucination_score\n\nresponse = \"Machine learning is a subset of AI that enables computers to learn from data.\"\ncontext = \"Machine learning is a type of artificial intelligence.\"\n\nprint(f\"Toxicity:      {toxicity_score(response)['toxicity_score']:.4f}\")\nprint(f\"Coherence:     {coherence_score(response)['coherence_score']:.4f}\")\nprint(f\"Hallucination: {hallucination_score(response, context)['hallucination_score']:.4f}\")\n</code></pre>"},{"location":"getting-started/#rag-evaluation","title":"RAG Evaluation","text":"<pre><code>from pyeval import context_relevance, groundedness_score, answer_correctness\n\nquery = \"What is machine learning?\"\ncontext = \"Machine learning is a subset of AI that enables systems to learn from data.\"\nresponse = \"Machine learning is an AI technique that allows computers to learn from data.\"\nground_truth = \"Machine learning is a type of artificial intelligence.\"\n\nprint(f\"Context Relevance: {context_relevance(query, context):.4f}\")\nprint(f\"Groundedness:      {groundedness_score(response, context):.4f}\")\nprint(f\"Answer Correct:    {answer_correctness(response, ground_truth):.4f}\")\n</code></pre>"},{"location":"getting-started/#fairness-evaluation","title":"Fairness Evaluation","text":"<pre><code>from pyeval import demographic_parity, equalized_odds, disparate_impact\n\ny_true = [1, 1, 0, 0, 1, 1, 0, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\ndp = demographic_parity(y_pred, sensitive)\nprint(f\"Demographic Parity: {dp['dp_difference']:.4f}\")\n\neo = equalized_odds(y_true, y_pred, sensitive)\nprint(f\"Equalized Odds:     {eo['eo_difference']:.4f}\")\n</code></pre>"},{"location":"getting-started/#recommender-systems","title":"Recommender Systems","text":"<pre><code>from pyeval import precision_at_k, recall_at_k, ndcg_at_k, mrr\n\n# User preferences (1 = relevant, 0 = not relevant)\nactual = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]\npredicted_ranking = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n\nprint(f\"Precision@5: {precision_at_k(actual, predicted_ranking, k=5):.4f}\")\nprint(f\"Recall@5:    {recall_at_k(actual, predicted_ranking, k=5):.4f}\")\nprint(f\"NDCG@5:      {ndcg_at_k(actual, predicted_ranking, k=5):.4f}\")\nprint(f\"MRR:         {mrr(actual, predicted_ranking):.4f}\")\n</code></pre>"},{"location":"getting-started/#speech-recognition","title":"Speech Recognition","text":"<pre><code>from pyeval import word_error_rate, character_error_rate\n\nreference = \"the quick brown fox jumps over the lazy dog\"\nhypothesis = \"the quick brown fox jumped over a lazy dog\"\n\nprint(f\"WER: {word_error_rate(reference, hypothesis):.4f}\")\nprint(f\"CER: {character_error_rate(reference, hypothesis):.4f}\")\n</code></pre>"},{"location":"getting-started/#built-in-visualization","title":"Built-in Visualization","text":"<p>PyEval includes ASCII-based visualization that works anywhere:</p>"},{"location":"getting-started/#confusion-matrix","title":"Confusion Matrix","text":"<pre><code>from pyeval import confusion_matrix\n\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0]\n\n# Display ASCII confusion matrix\ncm = confusion_matrix(y_true, y_pred, display=True)\n</code></pre> <p>Output: <pre><code>         Predicted\n         0    1\n       \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510\n    0  \u2502 3  \u2502 1  \u2502\nActual \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2524\n    1  \u2502 1  \u2502 3  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"getting-started/#sparklines","title":"Sparklines","text":"<pre><code>from pyeval import sparkline\n\nvalues = [1, 4, 2, 8, 5, 7, 3, 9, 6]\nprint(sparkline(values))\n# Output: \u2581\u2583\u2582\u2588\u2584\u2586\u2582\u2587\u2585\n</code></pre>"},{"location":"getting-started/#progress-bars","title":"Progress Bars","text":"<pre><code>from pyeval import progress_bar\n\n# Simple progress\nprint(progress_bar(75, 100))\n# Output: [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 75%\n</code></pre>"},{"location":"getting-started/#api-design-principles","title":"API Design Principles","text":""},{"location":"getting-started/#consistent-patterns","title":"Consistent Patterns","text":"<p>All PyEval functions follow consistent patterns:</p> <pre><code># Pattern 1: Compare y_true vs y_pred\nscore = metric(y_true, y_pred)\n\n# Pattern 2: Evaluate single input\nscore = metric(text)\n\n# Pattern 3: Evaluate with context\nscore = metric(text, context)\n\n# Pattern 4: Return detailed results\nresult = metric(y_true, y_pred, detailed=True)\n</code></pre>"},{"location":"getting-started/#return-types","title":"Return Types","text":"Return Type Example When Used <code>float</code> <code>0.8571</code> Single metric score <code>dict</code> <code>{'precision': 0.8, 'recall': 0.7}</code> Multiple related scores <code>list</code> <code>[0.8, 0.7, 0.9]</code> Per-class or per-sample scores"},{"location":"getting-started/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/#model-comparison","title":"Model Comparison","text":"<pre><code>from pyeval import accuracy_score, f1_score, matthews_corrcoef\n\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\n\nmodels = {\n    'Model A': [1, 0, 0, 1, 0, 1, 1, 0],\n    'Model B': [1, 0, 1, 1, 0, 1, 0, 1],\n    'Model C': [1, 1, 1, 1, 0, 0, 0, 0],\n}\n\nprint(\"Model Comparison\")\nprint(\"-\" * 40)\nfor name, y_pred in models.items():\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    mcc = matthews_corrcoef(y_true, y_pred)\n    print(f\"{name}: Acc={acc:.4f}, F1={f1:.4f}, MCC={mcc:.4f}\")\n</code></pre>"},{"location":"getting-started/#batch-evaluation","title":"Batch Evaluation","text":"<pre><code>from pyeval import bleu_score\n\nreferences = [\n    \"The cat sat on the mat\",\n    \"Hello world\",\n    \"Machine learning is fascinating\"\n]\n\nhypotheses = [\n    \"A cat was sitting on the mat\",\n    \"Hello there world\",\n    \"ML is really interesting\"\n]\n\nscores = [bleu_score(ref, hyp) for ref, hyp in zip(references, hypotheses)]\navg_bleu = sum(scores) / len(scores)\nprint(f\"Average BLEU: {avg_bleu:.4f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>API Overview \u2014 Quick reference for all 327+ APIs</li> <li>ML Examples \u2014 Detailed machine learning examples</li> <li>NLP Examples \u2014 Text evaluation examples</li> <li>LLM Examples \u2014 Large language model evaluation</li> <li>RAG Examples \u2014 Retrieval-augmented generation</li> </ul>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're using Python 3.12+:</p> <pre><code>python --version  # Should be 3.12 or higher\n</code></pre>"},{"location":"getting-started/#package-conflicts","title":"Package Conflicts","text":"<p>PyEval has no dependencies, so conflicts are unlikely. If you have issues, try a clean virtual environment:</p> <pre><code>python -m venv pyeval_env\nsource pyeval_env/bin/activate  # On Windows: pyeval_env\\Scripts\\activate\npip install pyeval\n</code></pre>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues \u2014 Report bugs or request features</li> <li>Contributing Guide \u2014 How to contribute</li> </ul>"},{"location":"advanced/decorators/","title":"Decorators","text":"<p>PyEval provides useful decorators for metric functions.</p>"},{"location":"advanced/decorators/#performance-decorators","title":"Performance Decorators","text":""},{"location":"advanced/decorators/#timed","title":"@timed","text":"<p>Measure and report function execution time.</p> <pre><code>from pyeval import timed\n\n@timed\ndef compute_metrics(y_true, y_pred):\n    \"\"\"Computes multiple metrics.\"\"\"\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'f1': f1_score(y_true, y_pred)\n    }\n\nresult = compute_metrics(y_true, y_pred)\n# Output: compute_metrics took 0.0023s\n\n# Access timing info\nprint(result)  # Normal return value\n</code></pre>"},{"location":"advanced/decorators/#timed-with-custom-output","title":"@timed with custom output","text":"<pre><code>from pyeval import timed\n\n@timed(output='return')  # Include timing in return\ndef compute_metrics(y_true, y_pred):\n    return accuracy_score(y_true, y_pred)\n\nresult, elapsed = compute_metrics(y_true, y_pred)\nprint(f\"Accuracy: {result}, Time: {elapsed:.4f}s\")\n</code></pre>"},{"location":"advanced/decorators/#memoize","title":"@memoize","text":"<p>Cache function results for identical inputs.</p> <pre><code>from pyeval import memoize\n\n@memoize\ndef expensive_metric(y_true, y_pred):\n    \"\"\"Expensive computation that benefits from caching.\"\"\"\n    # Simulated expensive operation\n    result = complex_computation(y_true, y_pred)\n    return result\n\n# First call - computes\nresult1 = expensive_metric([1,0,1], [1,0,0])  # Takes time\n\n# Second call with same args - returns cached\nresult2 = expensive_metric([1,0,1], [1,0,0])  # Instant\n\n# Different args - computes again\nresult3 = expensive_metric([1,1,1], [1,0,1])  # Takes time\n</code></pre>"},{"location":"advanced/decorators/#memoize-with-max-size","title":"@memoize with max size","text":"<pre><code>from pyeval import memoize\n\n@memoize(maxsize=100)  # Keep at most 100 cached results\ndef metric_function(y_true, y_pred):\n    return compute(y_true, y_pred)\n</code></pre>"},{"location":"advanced/decorators/#lru_cache_decorator","title":"@lru_cache_decorator","text":"<p>LRU (Least Recently Used) caching.</p> <pre><code>from pyeval import lru_cache_decorator\n\n@lru_cache_decorator(maxsize=128)\ndef cached_metric(data_tuple):  # Note: args must be hashable\n    return expensive_computation(data_tuple)\n</code></pre>"},{"location":"advanced/decorators/#validation-decorators","title":"Validation Decorators","text":""},{"location":"advanced/decorators/#require_same_length","title":"@require_same_length","text":"<p>Validate that input sequences have the same length.</p> <pre><code>from pyeval import require_same_length\n\n@require_same_length\ndef custom_accuracy(y_true, y_pred):\n    \"\"\"Requires y_true and y_pred to have same length.\"\"\"\n    return sum(1 for t, p in zip(y_true, y_pred) if t == p) / len(y_true)\n\n# Works fine\nresult = custom_accuracy([1, 0, 1], [1, 0, 0])\n\n# Raises ValueError\nresult = custom_accuracy([1, 0, 1], [1, 0])  \n# ValueError: y_true and y_pred must have the same length\n</code></pre>"},{"location":"advanced/decorators/#require_non_empty","title":"@require_non_empty","text":"<p>Validate that inputs are non-empty.</p> <pre><code>from pyeval import require_non_empty\n\n@require_non_empty\ndef mean_metric(values):\n    \"\"\"Requires non-empty input.\"\"\"\n    return sum(values) / len(values)\n\n# Works\nresult = mean_metric([1, 2, 3])\n\n# Raises ValueError\nresult = mean_metric([])  # ValueError: Input cannot be empty\n</code></pre>"},{"location":"advanced/decorators/#validate_range","title":"@validate_range","text":"<p>Validate that values are within a range.</p> <pre><code>from pyeval import validate_range\n\n@validate_range(min_val=0, max_val=1)\ndef process_probabilities(probs):\n    \"\"\"Requires all values between 0 and 1.\"\"\"\n    return probs\n\n# Works\nresult = process_probabilities([0.1, 0.5, 0.9])\n\n# Raises ValueError  \nresult = process_probabilities([0.1, 1.5, 0.9])\n# ValueError: Values must be between 0 and 1\n</code></pre>"},{"location":"advanced/decorators/#error-handling-decorators","title":"Error Handling Decorators","text":""},{"location":"advanced/decorators/#retry","title":"@retry","text":"<p>Retry function on failure.</p> <pre><code>from pyeval import retry\n\n@retry(max_attempts=3, delay=0.1)\ndef unreliable_metric(data):\n    \"\"\"May fail occasionally.\"\"\"\n    result = external_service_call(data)\n    return result\n\n# Will retry up to 3 times with 0.1s delay between attempts\nresult = unreliable_metric(data)\n</code></pre>"},{"location":"advanced/decorators/#retry-with-exponential-backoff","title":"@retry with exponential backoff","text":"<pre><code>from pyeval import retry\n\n@retry(max_attempts=5, delay=0.1, backoff=2)\ndef api_metric(data):\n    \"\"\"Calls external API that may be rate-limited.\"\"\"\n    return api_call(data)\n\n# Delays: 0.1s, 0.2s, 0.4s, 0.8s, 1.6s\n</code></pre>"},{"location":"advanced/decorators/#retry-with-specific-exceptions","title":"@retry with specific exceptions","text":"<pre><code>from pyeval import retry\n\n@retry(max_attempts=3, exceptions=(TimeoutError, ConnectionError))\ndef network_metric(data):\n    \"\"\"Only retry on network errors.\"\"\"\n    return fetch_from_server(data)\n</code></pre>"},{"location":"advanced/decorators/#fallback","title":"@fallback","text":"<p>Provide fallback value on error.</p> <pre><code>from pyeval import fallback\n\n@fallback(default_value=0.0)\ndef safe_metric(y_true, y_pred):\n    \"\"\"Returns 0.0 if computation fails.\"\"\"\n    return risky_computation(y_true, y_pred)\n\nresult = safe_metric(y_true, y_pred)  # Returns 0.0 on error\n</code></pre>"},{"location":"advanced/decorators/#fallback-with-handler","title":"@fallback with handler","text":"<pre><code>from pyeval import fallback\n\ndef error_handler(func, error, *args, **kwargs):\n    print(f\"Error in {func.__name__}: {error}\")\n    return -1.0\n\n@fallback(handler=error_handler)\ndef metric_with_handler(y_true, y_pred):\n    return computation(y_true, y_pred)\n</code></pre>"},{"location":"advanced/decorators/#logging-decorators","title":"Logging Decorators","text":""},{"location":"advanced/decorators/#logged","title":"@logged","text":"<p>Log function calls and results.</p> <pre><code>from pyeval import logged\n\n@logged\ndef tracked_metric(y_true, y_pred):\n    \"\"\"Logs all calls.\"\"\"\n    return accuracy_score(y_true, y_pred)\n\nresult = tracked_metric([1,0,1], [1,0,0])\n# Log: tracked_metric called with args=([1,0,1], [1,0,0])\n# Log: tracked_metric returned 0.667\n</code></pre>"},{"location":"advanced/decorators/#logged-with-custom-logger","title":"@logged with custom logger","text":"<pre><code>import logging\nfrom pyeval import logged\n\nlogger = logging.getLogger('metrics')\n\n@logged(logger=logger, level=logging.DEBUG)\ndef debug_metric(y_true, y_pred):\n    return f1_score(y_true, y_pred)\n</code></pre>"},{"location":"advanced/decorators/#deprecated","title":"@deprecated","text":"<p>Mark functions as deprecated.</p> <pre><code>from pyeval import deprecated\n\n@deprecated(message=\"Use accuracy_score instead\", version=\"2.0\")\ndef old_accuracy(y_true, y_pred):\n    return accuracy_score(y_true, y_pred)\n\nresult = old_accuracy([1,0], [1,0])\n# Warning: old_accuracy is deprecated since version 2.0. Use accuracy_score instead\n</code></pre>"},{"location":"advanced/decorators/#type-decorators","title":"Type Decorators","text":""},{"location":"advanced/decorators/#enforce_types","title":"@enforce_types","text":"<p>Enforce argument types at runtime.</p> <pre><code>from pyeval import enforce_types\n\n@enforce_types\ndef typed_metric(y_true: list, y_pred: list, threshold: float = 0.5) -&gt; float:\n    \"\"\"Enforces type annotations.\"\"\"\n    return compute(y_true, y_pred, threshold)\n\n# Works\nresult = typed_metric([1,0], [1,0], 0.5)\n\n# Raises TypeError\nresult = typed_metric(\"not a list\", [1,0])\n# TypeError: y_true must be list, got str\n</code></pre>"},{"location":"advanced/decorators/#combining-decorators","title":"Combining Decorators","text":"<p>Decorators can be stacked (applied bottom-up):</p> <pre><code>from pyeval import timed, memoize, require_same_length, logged\n\n@logged\n@timed\n@memoize\n@require_same_length\ndef comprehensive_metric(y_true, y_pred):\n    \"\"\"\n    1. Validates lengths (require_same_length)\n    2. Caches results (memoize)\n    3. Times execution (timed)\n    4. Logs call (logged)\n    \"\"\"\n    return expensive_computation(y_true, y_pred)\n</code></pre> <p>Order matters! Inner decorators execute first: 1. <code>require_same_length</code> - validates 2. <code>memoize</code> - checks cache 3. <code>timed</code> - measures time 4. <code>logged</code> - logs</p>"},{"location":"advanced/decorators/#creating-custom-decorators","title":"Creating Custom Decorators","text":"<pre><code>from functools import wraps\n\ndef threshold_check(min_score=0.0):\n    \"\"\"Custom decorator to validate metric scores.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            result = func(*args, **kwargs)\n            if result &lt; min_score:\n                raise ValueError(f\"Score {result} below minimum {min_score}\")\n            return result\n        return wrapper\n    return decorator\n\n@threshold_check(min_score=0.5)\ndef quality_metric(y_true, y_pred):\n    return f1_score(y_true, y_pred)\n</code></pre>"},{"location":"advanced/decorators/#best-practices","title":"Best Practices","text":"<ol> <li>Order decorators carefully - validation before caching before timing</li> <li>Use <code>@memoize</code> for expensive computations - significant speedup</li> <li>Use <code>@retry</code> for external calls - improved reliability</li> <li>Use <code>@require_same_length</code> - catches common errors early</li> <li>Use <code>@timed</code> during development - identify bottlenecks</li> <li>Use <code>@logged</code> sparingly - can produce verbose output</li> </ol>"},{"location":"advanced/functional/","title":"Functional Utilities","text":"<p>PyEval provides functional programming utilities for composing evaluation functions.</p>"},{"location":"advanced/functional/#function-composition","title":"Function Composition","text":""},{"location":"advanced/functional/#compose","title":"compose","text":"<p>Compose multiple functions into a single function.</p> <pre><code>from pyeval import compose\n\n# Compose functions: right to left execution\npipeline = compose(\n    lambda x: x * 2,      # Third: multiply by 2\n    lambda x: x + 10,     # Second: add 10\n    lambda x: x ** 2      # First: square\n)\n\nresult = pipeline(5)  # 5\u00b2 = 25 \u2192 25 + 10 = 35 \u2192 35 * 2 = 70\n</code></pre>"},{"location":"advanced/functional/#pipe","title":"pipe","text":"<p>Pipe functions in left to right order.</p> <pre><code>from pyeval import pipe\n\n# Pipe functions: left to right execution\npipeline = pipe(\n    lambda x: x ** 2,     # First: square\n    lambda x: x + 10,     # Second: add 10\n    lambda x: x * 2       # Third: multiply by 2\n)\n\nresult = pipeline(5)  # 5\u00b2 = 25 \u2192 25 + 10 = 35 \u2192 35 * 2 = 70\n</code></pre>"},{"location":"advanced/functional/#currying-and-partial-application","title":"Currying and Partial Application","text":""},{"location":"advanced/functional/#curry","title":"curry","text":"<p>Convert a function to curried form.</p> <pre><code>from pyeval import curry\n\n@curry\ndef weighted_sum(a, b, weight_a, weight_b):\n    return a * weight_a + b * weight_b\n\n# Partial application\nwith_weights = weighted_sum(weight_a=0.3, weight_b=0.7)\nresult = with_weights(10, 20)  # 10 * 0.3 + 20 * 0.7 = 17.0\n\n# Step by step\nf = weighted_sum(10)\ng = f(20)\nh = g(0.3)\nresult = h(0.7)  # 17.0\n</code></pre>"},{"location":"advanced/functional/#partial","title":"partial","text":"<p>Create a partial function with pre-filled arguments.</p> <pre><code>from pyeval import partial\n\ndef f1_score(y_true, y_pred, average='binary', zero_division=0):\n    # ... implementation\n    pass\n\n# Create specialized versions\nbinary_f1 = partial(f1_score, average='binary')\nmacro_f1 = partial(f1_score, average='macro')\nweighted_f1 = partial(f1_score, average='weighted', zero_division=1)\n\n# Use them\nresult = binary_f1(y_true, y_pred)\nresult = macro_f1(y_true, y_pred)\n</code></pre>"},{"location":"advanced/functional/#higher-order-functions","title":"Higher-Order Functions","text":""},{"location":"advanced/functional/#map_over","title":"map_over","text":"<p>Apply a function over collections.</p> <pre><code>from pyeval import map_over, accuracy_score\n\n# Apply metric to multiple datasets\ndatasets = [\n    ([1, 0, 1], [1, 0, 1]),\n    ([1, 1, 0], [1, 0, 0]),\n    ([0, 0, 1], [0, 1, 1]),\n]\n\nresults = map_over(\n    lambda data: accuracy_score(data[0], data[1]),\n    datasets\n)\n# [1.0, 0.333, 0.667]\n</code></pre>"},{"location":"advanced/functional/#filter_results","title":"filter_results","text":"<p>Filter results based on conditions.</p> <pre><code>from pyeval import filter_results\n\nresults = {\n    'accuracy': 0.85,\n    'precision': 0.72,\n    'recall': 0.91,\n    'f1': 0.80,\n    'specificity': 0.65\n}\n\n# Filter to only high-performing metrics\nhigh_performers = filter_results(lambda v: v &gt;= 0.8, results)\n# {'accuracy': 0.85, 'recall': 0.91, 'f1': 0.80}\n</code></pre>"},{"location":"advanced/functional/#reduce_results","title":"reduce_results","text":"<p>Reduce results to a single value.</p> <pre><code>from pyeval import reduce_results\n\nresults = {\n    'accuracy': 0.85,\n    'precision': 0.72,\n    'recall': 0.91,\n    'f1': 0.80\n}\n\n# Compute average\naverage = reduce_results(\n    lambda acc, v: acc + v,\n    results,\n    initial=0\n) / len(results)\n# 0.82\n</code></pre>"},{"location":"advanced/functional/#function-factories","title":"Function Factories","text":""},{"location":"advanced/functional/#create_metric","title":"create_metric","text":"<p>Create a metric function with configuration.</p> <pre><code>from pyeval import create_metric\n\n# Create configured accuracy\naccuracy = create_metric(\n    'accuracy',\n    normalize=True,\n    sample_weight=None\n)\n\nresult = accuracy(y_true, y_pred)\n</code></pre>"},{"location":"advanced/functional/#create_threshold_checker","title":"create_threshold_checker","text":"<p>Create a threshold checking function.</p> <pre><code>from pyeval import create_threshold_checker\n\n# Create checker with thresholds\ncheck_quality = create_threshold_checker({\n    'accuracy': {'min': 0.8, 'max': 1.0},\n    'precision': {'min': 0.75},\n    'recall': {'min': 0.75},\n    'f1': {'min': 0.8}\n})\n\nresults = {\n    'accuracy': 0.85,\n    'precision': 0.72,  # Below threshold!\n    'recall': 0.91,\n    'f1': 0.80\n}\n\nreport = check_quality(results)\n# {\n#     'passed': False,\n#     'failures': ['precision: 0.72 &lt; 0.75'],\n#     'warnings': []\n# }\n</code></pre>"},{"location":"advanced/functional/#create_aggregator","title":"create_aggregator","text":"<p>Create result aggregation functions.</p> <pre><code>from pyeval import create_aggregator\n\n# Weighted aggregator\nweighted_avg = create_aggregator(\n    'weighted_average',\n    weights={\n        'accuracy': 0.1,\n        'precision': 0.2,\n        'recall': 0.2,\n        'f1': 0.5\n    }\n)\n\nresults = {\n    'accuracy': 0.85,\n    'precision': 0.72,\n    'recall': 0.91,\n    'f1': 0.80\n}\n\nscore = weighted_avg(results)\n# 0.85 * 0.1 + 0.72 * 0.2 + 0.91 * 0.2 + 0.80 * 0.5 = 0.811\n</code></pre>"},{"location":"advanced/functional/#memoization","title":"Memoization","text":""},{"location":"advanced/functional/#memoize","title":"memoize","text":"<p>Cache function results for repeated calls.</p> <pre><code>from pyeval import memoize\n\n@memoize\ndef expensive_metric(y_true, y_pred):\n    \"\"\"Compute an expensive metric.\"\"\"\n    # ... expensive computation\n    return result\n\n# First call computes\nresult1 = expensive_metric(y_true, y_pred)  # Slow\n\n# Second call uses cache\nresult2 = expensive_metric(y_true, y_pred)  # Fast!\n</code></pre>"},{"location":"advanced/functional/#memoize_with_ttl","title":"memoize_with_ttl","text":"<p>Cache with time-to-live expiration.</p> <pre><code>from pyeval import memoize_with_ttl\n\n@memoize_with_ttl(seconds=60)\ndef api_metric(y_true, y_pred):\n    \"\"\"Metric that calls an external API.\"\"\"\n    # ... API call\n    return result\n\n# Results cached for 60 seconds\n</code></pre>"},{"location":"advanced/functional/#error-handling","title":"Error Handling","text":""},{"location":"advanced/functional/#safe_call","title":"safe_call","text":"<p>Safely call functions with error handling.</p> <pre><code>from pyeval import safe_call\n\nresult = safe_call(\n    risky_metric,\n    y_true, y_pred,\n    default=0.0,\n    on_error=lambda e: print(f\"Error: {e}\")\n)\n# Returns 0.0 if error, otherwise metric value\n</code></pre>"},{"location":"advanced/functional/#try_metrics","title":"try_metrics","text":"<p>Try multiple metrics, return first successful.</p> <pre><code>from pyeval import try_metrics\n\nresult = try_metrics(\n    [metric_1, metric_2, metric_3],\n    y_true, y_pred,\n    default=None\n)\n# Returns result of first metric that succeeds\n</code></pre>"},{"location":"advanced/functional/#retry","title":"retry","text":"<p>Retry function calls with backoff.</p> <pre><code>from pyeval import retry\n\n@retry(max_attempts=3, delay=1.0, backoff=2.0)\ndef flaky_metric(y_true, y_pred):\n    \"\"\"A metric that might fail temporarily.\"\"\"\n    # ... implementation that might fail\n    return result\n\n# Will retry up to 3 times with exponential backoff\n</code></pre>"},{"location":"advanced/functional/#monads","title":"Monads","text":""},{"location":"advanced/functional/#maybe","title":"Maybe","text":"<p>Handle optional values gracefully.</p> <pre><code>from pyeval import Maybe\n\ndef safe_divide(a, b):\n    if b == 0:\n        return Maybe.nothing()\n    return Maybe.just(a / b)\n\nresult = (\n    Maybe.just(10)\n    .bind(lambda x: safe_divide(x, 2))\n    .bind(lambda x: safe_divide(x, 0))  # Returns Nothing\n    .map(lambda x: x * 100)\n    .get_or_else(0)  # Returns 0 because Nothing\n)\n</code></pre>"},{"location":"advanced/functional/#either","title":"Either","text":"<p>Handle success/failure with context.</p> <pre><code>from pyeval import Either, Left, Right\n\ndef compute_metric(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        return Left(\"Length mismatch\")\n    if len(y_true) == 0:\n        return Left(\"Empty inputs\")\n    return Right(sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true))\n\nresult = (\n    compute_metric(y_true, y_pred)\n    .map(lambda x: round(x, 4))\n    .map(lambda x: f\"Accuracy: {x}\")\n    .fold(\n        on_left=lambda err: f\"Error: {err}\",\n        on_right=lambda val: val\n    )\n)\n</code></pre>"},{"location":"advanced/functional/#result","title":"Result","text":"<p>Rust-style Result type.</p> <pre><code>from pyeval import Result, Ok, Err\n\ndef validate_inputs(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        return Err(ValueError(\"Length mismatch\"))\n    return Ok((y_true, y_pred))\n\ndef compute_accuracy(inputs):\n    y_true, y_pred = inputs\n    return Ok(sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true))\n\nresult = (\n    validate_inputs(y_true, y_pred)\n    .and_then(compute_accuracy)\n    .map(lambda x: round(x, 4))\n    .unwrap_or(0.0)\n)\n</code></pre>"},{"location":"advanced/functional/#lazy-evaluation","title":"Lazy Evaluation","text":""},{"location":"advanced/functional/#lazy","title":"lazy","text":"<p>Create lazily evaluated values.</p> <pre><code>from pyeval import lazy\n\n# Metric not computed until needed\nlazy_accuracy = lazy(accuracy_score, y_true, y_pred)\n\n# ... later ...\nresult = lazy_accuracy.value  # Computed now\nresult = lazy_accuracy.value  # Cached\n</code></pre>"},{"location":"advanced/functional/#lazypipeline","title":"LazyPipeline","text":"<p>Build lazily evaluated pipelines.</p> <pre><code>from pyeval import LazyPipeline\n\n# Build pipeline without executing\npipeline = (\n    LazyPipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('f1', f1_score)\n    .filter(lambda k, v: v &gt; 0.7)\n    .map_values(lambda v: round(v, 4))\n)\n\n# Execute when needed\nresults = pipeline.evaluate(y_true, y_pred)\n</code></pre>"},{"location":"advanced/functional/#complete-example","title":"Complete Example","text":"<pre><code>from pyeval import (\n    pipe, compose, curry, partial, memoize,\n    create_metric, create_threshold_checker, create_aggregator,\n    Either, Right, Left,\n    accuracy_score, precision_score, recall_score, f1_score\n)\n\n# Create curried metric functions\n@curry\ndef compute_with_config(config, y_true, y_pred):\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred),\n        'recall': recall_score(y_true, y_pred),\n        'f1': f1_score(y_true, y_pred)\n    }\n\n    if config.get('round', False):\n        metrics = {k: round(v, config.get('decimals', 4)) \n                   for k, v in metrics.items()}\n\n    return metrics\n\n# Create configured evaluator\nmy_evaluator = compute_with_config({\n    'round': True,\n    'decimals': 3\n})\n\n# Create threshold checker\ncheck_thresholds = create_threshold_checker({\n    'accuracy': {'min': 0.8},\n    'f1': {'min': 0.75}\n})\n\n# Create aggregator\nweighted_score = create_aggregator('weighted_average', weights={\n    'accuracy': 0.1,\n    'precision': 0.2,\n    'recall': 0.2,\n    'f1': 0.5\n})\n\n# Build evaluation pipeline\ndef validate_inputs(data):\n    y_true, y_pred = data\n    if len(y_true) != len(y_pred):\n        return Left(\"Length mismatch\")\n    if len(y_true) == 0:\n        return Left(\"Empty inputs\")\n    return Right(data)\n\ndef compute_metrics(data):\n    y_true, y_pred = data\n    return Right(my_evaluator(y_true, y_pred))\n\ndef add_aggregate(metrics):\n    metrics['weighted_score'] = weighted_score(\n        {k: v for k, v in metrics.items() if k != 'weighted_score'}\n    )\n    return Right(metrics)\n\ndef check_quality(metrics):\n    report = check_thresholds(metrics)\n    if not report['passed']:\n        return Left(f\"Quality check failed: {report['failures']}\")\n    return Right(metrics)\n\n# Run the pipeline\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0]\n\nresult = (\n    validate_inputs((y_true, y_pred))\n    .and_then(compute_metrics)\n    .and_then(add_aggregate)\n    .and_then(check_quality)\n    .fold(\n        on_left=lambda err: {'status': 'error', 'message': err},\n        on_right=lambda metrics: {'status': 'success', 'metrics': metrics}\n    )\n)\n\nprint(result)\n# {\n#     'status': 'success',\n#     'metrics': {\n#         'accuracy': 0.75,\n#         'precision': 0.667,\n#         'recall': 0.8,\n#         'f1': 0.727,\n#         'weighted_score': 0.738\n#     }\n# }\n</code></pre>"},{"location":"advanced/functional/#best-practices","title":"Best Practices","text":"<ol> <li>Use composition - Build complex functions from simple ones</li> <li>Prefer immutability - Don't mutate input data</li> <li>Use Either/Result - Handle errors explicitly</li> <li>Memoize expensive computations - Cache repeated calculations</li> <li>Be explicit about side effects - Use monads for clarity</li> </ol>"},{"location":"advanced/patterns/","title":"Design Patterns","text":"<p>PyEval provides several design patterns for flexible and extensible metric computation.</p>"},{"location":"advanced/patterns/#strategy-pattern","title":"Strategy Pattern","text":"<p>The Strategy pattern allows switching between different metric algorithms at runtime.</p>"},{"location":"advanced/patterns/#basic-usage","title":"Basic Usage","text":"<pre><code>from pyeval import (\n    MetricCalculator, \n    AccuracyStrategy, \n    F1Strategy, \n    PrecisionStrategy,\n    RecallStrategy\n)\n\n# Create calculator with initial strategy\ncalculator = MetricCalculator(AccuracyStrategy())\n\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\n# Compute accuracy\naccuracy = calculator.calculate(y_true, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# Switch strategy\ncalculator.set_strategy(F1Strategy())\nf1 = calculator.calculate(y_true, y_pred)\nprint(f\"F1: {f1:.4f}\")\n\n# Switch again\ncalculator.set_strategy(PrecisionStrategy())\nprecision = calculator.calculate(y_true, y_pred)\nprint(f\"Precision: {precision:.4f}\")\n</code></pre>"},{"location":"advanced/patterns/#creating-custom-strategies","title":"Creating Custom Strategies","text":"<pre><code>from pyeval import MetricStrategy, MetricCalculator\n\nclass BalancedAccuracyStrategy(MetricStrategy):\n    \"\"\"Custom strategy for balanced accuracy.\"\"\"\n\n    def compute(self, y_true, y_pred, **kwargs):\n        # Get unique classes\n        classes = set(y_true)\n\n        # Compute recall for each class\n        recalls = []\n        for cls in classes:\n            true_positives = sum(1 for t, p in zip(y_true, y_pred) \n                               if t == cls and p == cls)\n            actual_positives = sum(1 for t in y_true if t == cls)\n\n            if actual_positives &gt; 0:\n                recalls.append(true_positives / actual_positives)\n\n        # Return average recall\n        return sum(recalls) / len(recalls) if recalls else 0.0\n\n# Use custom strategy\ncalculator = MetricCalculator(BalancedAccuracyStrategy())\nresult = calculator.calculate(y_true, y_pred)\n</code></pre>"},{"location":"advanced/patterns/#factory-pattern","title":"Factory Pattern","text":"<p>The Factory pattern creates metric instances based on type.</p>"},{"location":"advanced/patterns/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from pyeval import MetricFactory, MetricType\n\nfactory = MetricFactory()\n\n# Create metrics by type\naccuracy_metric = factory.create(MetricType.ACCURACY)\nf1_metric = factory.create(MetricType.F1)\nprecision_metric = factory.create(MetricType.PRECISION)\n\n# Compute\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\nprint(f\"Accuracy: {accuracy_metric.compute(y_true, y_pred):.4f}\")\nprint(f\"F1: {f1_metric.compute(y_true, y_pred):.4f}\")\nprint(f\"Precision: {precision_metric.compute(y_true, y_pred):.4f}\")\n</code></pre>"},{"location":"advanced/patterns/#available-metric-types","title":"Available Metric Types","text":"<pre><code>from pyeval import MetricType\n\n# Classification\nMetricType.ACCURACY\nMetricType.PRECISION\nMetricType.RECALL\nMetricType.F1\nMetricType.ROC_AUC\n\n# Regression\nMetricType.MSE\nMetricType.RMSE\nMetricType.MAE\nMetricType.R2\n\n# NLP\nMetricType.BLEU\nMetricType.ROUGE\nMetricType.METEOR\n</code></pre>"},{"location":"advanced/patterns/#registering-custom-metrics","title":"Registering Custom Metrics","text":"<pre><code>from pyeval import MetricFactory, MetricType, Metric\n\nclass CustomMetric(Metric):\n    def compute(self, y_true, y_pred, **kwargs):\n        # Custom computation\n        return custom_score\n\n# Register with factory\nfactory = MetricFactory()\nfactory.register('custom', CustomMetric)\n\n# Use registered metric\nmetric = factory.create('custom')\n</code></pre>"},{"location":"advanced/patterns/#composite-pattern","title":"Composite Pattern","text":"<p>The Composite pattern groups multiple metrics for batch computation.</p>"},{"location":"advanced/patterns/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from pyeval import CompositeMetric, SingleMetric\nfrom pyeval import accuracy_score, precision_score, recall_score, f1_score\n\n# Create composite\nclassification_metrics = CompositeMetric('classification')\n\n# Add individual metrics\nclassification_metrics.add(SingleMetric('accuracy', accuracy_score))\nclassification_metrics.add(SingleMetric('precision', precision_score))\nclassification_metrics.add(SingleMetric('recall', recall_score))\nclassification_metrics.add(SingleMetric('f1', f1_score))\n\n# Compute all at once\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\nresults = classification_metrics.compute(y_true, y_pred)\nprint(results)\n# {'accuracy': 0.8, 'precision': 0.667, 'recall': 0.667, 'f1': 0.667}\n</code></pre>"},{"location":"advanced/patterns/#nested-composites","title":"Nested Composites","text":"<pre><code>from pyeval import CompositeMetric, SingleMetric\n\n# Create nested structure\nall_metrics = CompositeMetric('all')\n\n# Classification group\nclassification = CompositeMetric('classification')\nclassification.add(SingleMetric('accuracy', accuracy_score))\nclassification.add(SingleMetric('f1', f1_score))\n\n# Regression group\nregression = CompositeMetric('regression')\nregression.add(SingleMetric('mse', mean_squared_error))\nregression.add(SingleMetric('r2', r2_score))\n\n# Add groups to parent\nall_metrics.add(classification)\nall_metrics.add(regression)\n\n# Compute returns nested results\nresults = all_metrics.compute(y_true, y_pred)\n</code></pre>"},{"location":"advanced/patterns/#builder-pattern","title":"Builder Pattern","text":"<p>The Builder pattern constructs complex metric configurations step by step.</p>"},{"location":"advanced/patterns/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from pyeval import MetricBuilder\n\n# Build metric configuration\nconfig = (\n    MetricBuilder()\n    .with_metric('accuracy')\n    .with_metric('f1', average='weighted')\n    .with_metric('precision', average='macro')\n    .with_threshold(0.5)\n    .with_callbacks([ProgressCallback()])\n    .build()\n)\n\n# Execute\nresults = config.compute(y_true, y_pred)\n</code></pre>"},{"location":"advanced/patterns/#fluent-configuration","title":"Fluent Configuration","text":"<pre><code>from pyeval import EvaluationBuilder\n\nevaluation = (\n    EvaluationBuilder('classification')\n    .add_classification_metrics()\n    .add_roc_auc()\n    .with_confidence_intervals(n_bootstrap=1000)\n    .with_statistical_tests()\n    .save_to('results.json')\n    .build()\n)\n\nreport = evaluation.run(y_true, y_pred)\n</code></pre>"},{"location":"advanced/patterns/#observer-pattern","title":"Observer Pattern","text":"<p>The Observer pattern allows monitoring metric computation events.</p>"},{"location":"advanced/patterns/#basic-usage_4","title":"Basic Usage","text":"<pre><code>from pyeval import MetricObserver, MetricSubject\n\nclass LoggingObserver(MetricObserver):\n    def on_compute_start(self, metric_name, **kwargs):\n        print(f\"Starting: {metric_name}\")\n\n    def on_compute_end(self, metric_name, result, **kwargs):\n        print(f\"Completed: {metric_name} = {result}\")\n\n    def on_error(self, metric_name, error, **kwargs):\n        print(f\"Error in {metric_name}: {error}\")\n\n# Create subject and attach observer\nsubject = MetricSubject()\nsubject.attach(LoggingObserver())\n\n# Compute with observation\nresult = subject.compute_with_observation('accuracy', y_true, y_pred)\n</code></pre>"},{"location":"advanced/patterns/#multiple-observers","title":"Multiple Observers","text":"<pre><code>class TimingObserver(MetricObserver):\n    def __init__(self):\n        self.start_times = {}\n\n    def on_compute_start(self, metric_name, **kwargs):\n        self.start_times[metric_name] = time.time()\n\n    def on_compute_end(self, metric_name, result, **kwargs):\n        elapsed = time.time() - self.start_times[metric_name]\n        print(f\"{metric_name} took {elapsed:.4f}s\")\n\nclass ThresholdObserver(MetricObserver):\n    def __init__(self, thresholds):\n        self.thresholds = thresholds\n\n    def on_compute_end(self, metric_name, result, **kwargs):\n        if metric_name in self.thresholds:\n            threshold = self.thresholds[metric_name]\n            if result &lt; threshold:\n                print(f\"WARNING: {metric_name} ({result}) below threshold ({threshold})\")\n\n# Attach multiple observers\nsubject.attach(TimingObserver())\nsubject.attach(ThresholdObserver({'accuracy': 0.9, 'f1': 0.85}))\n</code></pre>"},{"location":"advanced/patterns/#chain-of-responsibility","title":"Chain of Responsibility","text":"<p>Process metrics through a chain of handlers.</p> <pre><code>from pyeval import MetricHandler, ValidationHandler, ComputationHandler, LoggingHandler\n\n# Create chain\nchain = (\n    ValidationHandler()\n    .set_next(ComputationHandler())\n    .set_next(LoggingHandler())\n)\n\n# Process through chain\nresult = chain.handle({\n    'y_true': y_true,\n    'y_pred': y_pred,\n    'metric': 'accuracy'\n})\n</code></pre>"},{"location":"advanced/patterns/#best-practices","title":"Best Practices","text":""},{"location":"advanced/patterns/#choosing-the-right-pattern","title":"Choosing the Right Pattern","text":"Use Case Recommended Pattern Switch algorithms at runtime Strategy Create metrics by type Factory Group related metrics Composite Complex configuration Builder Event monitoring Observer Sequential processing Chain of Responsibility"},{"location":"advanced/patterns/#combining-patterns","title":"Combining Patterns","text":"<pre><code>from pyeval import (\n    MetricFactory, CompositeMetric, MetricCalculator,\n    AccuracyStrategy, MetricSubject, LoggingObserver\n)\n\n# Factory creates metrics\nfactory = MetricFactory()\n\n# Composite groups them\ncomposite = CompositeMetric('evaluation')\ncomposite.add(factory.create(MetricType.ACCURACY))\ncomposite.add(factory.create(MetricType.F1))\n\n# Observer monitors execution\nsubject = MetricSubject()\nsubject.attach(LoggingObserver())\n\n# Strategy allows switching\ncalculator = MetricCalculator(composite)\n\n# Execute with full infrastructure\nresults = subject.compute_with_observation('evaluation', y_true, y_pred)\n</code></pre>"},{"location":"advanced/pipelines/","title":"Pipelines","text":"<p>PyEval provides a fluent pipeline API for building evaluation workflows.</p>"},{"location":"advanced/pipelines/#basic-pipeline","title":"Basic Pipeline","text":""},{"location":"advanced/pipelines/#creating-pipelines","title":"Creating Pipelines","text":"<pre><code>from pyeval import Pipeline\nfrom pyeval import accuracy_score, precision_score, recall_score, f1_score\n\n# Create a simple pipeline\npipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('precision', precision_score)\n    .add_metric('recall', recall_score)\n    .add_metric('f1', f1_score)\n)\n\n# Run pipeline\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\nresults = pipeline.run(y_true, y_pred)\nprint(results)\n# {\n#     'accuracy': 0.8,\n#     'precision': 0.667,\n#     'recall': 0.667,\n#     'f1': 0.667\n# }\n</code></pre>"},{"location":"advanced/pipelines/#pipeline-features","title":"Pipeline Features","text":""},{"location":"advanced/pipelines/#validation-steps","title":"Validation Steps","text":"<pre><code>from pyeval import Pipeline\n\npipeline = (\n    Pipeline()\n    # Add validation\n    .validate(\n        lambda x: len(x[0]) == len(x[1]), \n        \"y_true and y_pred must have same length\"\n    )\n    .validate(\n        lambda x: len(x[0]) &gt; 0,\n        \"Inputs cannot be empty\"\n    )\n    # Add metrics\n    .add_metric('accuracy', accuracy_score)\n)\n\n# Valid data works\nresults = pipeline.run(y_true, y_pred)\n\n# Invalid data raises error with message\nresults = pipeline.run([1, 0, 1], [1, 0])  # ValueError: y_true and y_pred must have same length\n</code></pre>"},{"location":"advanced/pipelines/#preprocessing-steps","title":"Preprocessing Steps","text":"<pre><code>from pyeval import Pipeline\n\ndef binarize(y_true, y_pred):\n    \"\"\"Convert probabilities to binary predictions.\"\"\"\n    y_pred_binary = [1 if p &gt; 0.5 else 0 for p in y_pred]\n    return y_true, y_pred_binary\n\npipeline = (\n    Pipeline()\n    .preprocess(binarize)\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('f1', f1_score)\n)\n\ny_true = [1, 0, 1, 1, 0]\ny_prob = [0.9, 0.1, 0.4, 0.8, 0.3]\n\nresults = pipeline.run(y_true, y_prob)\n# Automatically converts probabilities to binary before computing metrics\n</code></pre>"},{"location":"advanced/pipelines/#postprocessing-steps","title":"Postprocessing Steps","text":"<pre><code>from pyeval import Pipeline\n\ndef round_results(results):\n    \"\"\"Round all results to 4 decimal places.\"\"\"\n    return {k: round(v, 4) for k, v in results.items()}\n\ndef add_summary(results):\n    \"\"\"Add summary statistics.\"\"\"\n    values = list(results.values())\n    results['mean'] = sum(values) / len(values)\n    results['min'] = min(values)\n    results['max'] = max(values)\n    return results\n\npipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('f1', f1_score)\n    .postprocess(round_results)\n    .postprocess(add_summary)\n)\n</code></pre>"},{"location":"advanced/pipelines/#aggregation","title":"Aggregation","text":"<pre><code>from pyeval import Pipeline\n\npipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('precision', precision_score)\n    .add_metric('recall', recall_score)\n    .add_metric('f1', f1_score)\n    .aggregate('mean', lambda r: sum(r.values()) / len(r))\n    .aggregate('weighted_mean', lambda r: (\n        r['accuracy'] * 0.1 + \n        r['f1'] * 0.9\n    ))\n)\n\nresults = pipeline.run(y_true, y_pred)\n# Includes 'mean' and 'weighted_mean' in results\n</code></pre>"},{"location":"advanced/pipelines/#preset-pipelines","title":"Preset Pipelines","text":""},{"location":"advanced/pipelines/#classification-pipeline","title":"Classification Pipeline","text":"<pre><code>from pyeval import create_classification_pipeline\n\n# Comprehensive classification evaluation\npipeline = create_classification_pipeline()\n\nresults = pipeline.run(y_true, y_pred)\nprint(results)\n# {\n#     'accuracy': 0.8,\n#     'precision': 0.667,\n#     'recall': 0.667,\n#     'f1': 0.667,\n#     'specificity': 1.0,\n#     'mcc': 0.577,\n#     ...\n# }\n</code></pre>"},{"location":"advanced/pipelines/#regression-pipeline","title":"Regression Pipeline","text":"<pre><code>from pyeval import create_regression_pipeline\n\npipeline = create_regression_pipeline()\n\ny_true = [3.0, -0.5, 2.0, 7.0]\ny_pred = [2.5, 0.0, 2.0, 8.0]\n\nresults = pipeline.run(y_true, y_pred)\n# {\n#     'mse': 0.375,\n#     'rmse': 0.612,\n#     'mae': 0.5,\n#     'r2': 0.948,\n#     ...\n# }\n</code></pre>"},{"location":"advanced/pipelines/#nlp-pipeline","title":"NLP Pipeline","text":"<pre><code>from pyeval import create_nlp_pipeline\n\npipeline = create_nlp_pipeline()\n\nreference = \"The quick brown fox\"\nhypothesis = \"A fast brown fox\"\n\nresults = pipeline.run(reference, hypothesis)\n# {\n#     'bleu': 0.45,\n#     'rouge_1': {...},\n#     'rouge_l': {...},\n#     'meteor': 0.72,\n#     ...\n# }\n</code></pre>"},{"location":"advanced/pipelines/#conditional-pipelines","title":"Conditional Pipelines","text":""},{"location":"advanced/pipelines/#branching","title":"Branching","text":"<pre><code>from pyeval import Pipeline, ConditionalPipeline\n\n# Different pipelines for different conditions\nbinary_pipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('f1', f1_score)\n    .add_metric('roc_auc', roc_auc_score)\n)\n\nmulticlass_pipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('macro_f1', lambda t, p: f1_score(t, p, average='macro'))\n    .add_metric('weighted_f1', lambda t, p: f1_score(t, p, average='weighted'))\n)\n\n# Conditional based on number of classes\ndef is_binary(y_true, y_pred):\n    return len(set(y_true)) == 2\n\nconditional = ConditionalPipeline()\nconditional.when(is_binary, binary_pipeline)\nconditional.otherwise(multiclass_pipeline)\n\nresults = conditional.run(y_true, y_pred)\n</code></pre>"},{"location":"advanced/pipelines/#pipeline-composition","title":"Pipeline Composition","text":""},{"location":"advanced/pipelines/#combining-pipelines","title":"Combining Pipelines","text":"<pre><code>from pyeval import Pipeline, CompositePipeline\n\n# Create component pipelines\naccuracy_pipeline = Pipeline().add_metric('accuracy', accuracy_score)\nf1_pipeline = Pipeline().add_metric('f1', f1_score)\n\n# Combine them\ncombined = (\n    CompositePipeline()\n    .add_pipeline('accuracy_metrics', accuracy_pipeline)\n    .add_pipeline('f1_metrics', f1_pipeline)\n)\n\nresults = combined.run(y_true, y_pred)\n# {\n#     'accuracy_metrics': {'accuracy': 0.8},\n#     'f1_metrics': {'f1': 0.667}\n# }\n</code></pre>"},{"location":"advanced/pipelines/#sequential-pipelines","title":"Sequential Pipelines","text":"<pre><code>from pyeval import SequentialPipeline\n\n# Run pipelines in sequence, passing results through\nsequential = (\n    SequentialPipeline()\n    .add_stage(preprocess_pipeline)\n    .add_stage(compute_pipeline)\n    .add_stage(postprocess_pipeline)\n)\n\nresults = sequential.run(y_true, y_pred)\n</code></pre>"},{"location":"advanced/pipelines/#parallel-execution","title":"Parallel Execution","text":""},{"location":"advanced/pipelines/#parallel-metrics","title":"Parallel Metrics","text":"<pre><code>from pyeval import ParallelPipeline\n\n# Compute multiple expensive metrics in parallel\nparallel = (\n    ParallelPipeline(n_jobs=4)\n    .add_metric('metric_1', expensive_metric_1)\n    .add_metric('metric_2', expensive_metric_2)\n    .add_metric('metric_3', expensive_metric_3)\n)\n\nresults = parallel.run(y_true, y_pred)\n</code></pre>"},{"location":"advanced/pipelines/#pipeline-callbacks","title":"Pipeline Callbacks","text":""},{"location":"advanced/pipelines/#adding-callbacks","title":"Adding Callbacks","text":"<pre><code>from pyeval import Pipeline, ProgressCallback, ThresholdCallback\n\nprogress = ProgressCallback(show_bar=True)\nthreshold = ThresholdCallback({\n    'accuracy': {'min': 0.8},\n    'f1': {'min': 0.75}\n})\n\npipeline = (\n    Pipeline()\n    .add_callback(progress)\n    .add_callback(threshold)\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('f1', f1_score)\n)\n\nresults = pipeline.run(y_true, y_pred)\n# Shows progress bar\n# Alerts if metrics below threshold\n</code></pre>"},{"location":"advanced/pipelines/#saving-and-loading","title":"Saving and Loading","text":""},{"location":"advanced/pipelines/#export-results","title":"Export Results","text":"<pre><code>from pyeval import Pipeline\n\npipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('f1', f1_score)\n    .save_to('results.json')  # Auto-save results\n)\n\nresults = pipeline.run(y_true, y_pred)\n# Results automatically saved to results.json\n</code></pre>"},{"location":"advanced/pipelines/#load-pipeline-configuration","title":"Load Pipeline Configuration","text":"<pre><code>from pyeval import Pipeline\n\n# Save pipeline config\npipeline.save_config('pipeline_config.json')\n\n# Load pipeline config\nloaded_pipeline = Pipeline.from_config('pipeline_config.json')\n</code></pre>"},{"location":"advanced/pipelines/#complete-example","title":"Complete Example","text":"<pre><code>from pyeval import (\n    Pipeline, \n    accuracy_score, precision_score, recall_score, f1_score,\n    ProgressCallback, ThresholdCallback\n)\n\ndef preprocess_data(y_true, y_pred):\n    \"\"\"Clean and validate data.\"\"\"\n    # Remove invalid entries\n    valid = [(t, p) for t, p in zip(y_true, y_pred) \n             if t is not None and p is not None]\n    return [t for t, p in valid], [p for t, p in valid]\n\ndef format_results(results):\n    \"\"\"Format results for display.\"\"\"\n    return {k: f\"{v:.4f}\" for k, v in results.items()}\n\n# Build comprehensive pipeline\nevaluation_pipeline = (\n    Pipeline('Classification Evaluation')\n\n    # Validation\n    .validate(lambda x: len(x[0]) == len(x[1]), \"Length mismatch\")\n    .validate(lambda x: len(x[0]) &gt; 0, \"Empty inputs\")\n\n    # Preprocessing\n    .preprocess(preprocess_data)\n\n    # Callbacks\n    .add_callback(ProgressCallback(show_bar=True))\n    .add_callback(ThresholdCallback({'accuracy': {'min': 0.7}}))\n\n    # Metrics\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('precision', precision_score)\n    .add_metric('recall', recall_score)\n    .add_metric('f1', f1_score)\n\n    # Aggregation\n    .aggregate('mean_score', lambda r: sum(r.values()) / len(r))\n\n    # Postprocessing\n    .postprocess(format_results)\n\n    # Save results\n    .save_to('evaluation_results.json')\n)\n\n# Run evaluation\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0]\n\nresults = evaluation_pipeline.run(y_true, y_pred)\nprint(results)\n</code></pre>"},{"location":"advanced/pipelines/#best-practices","title":"Best Practices","text":"<ol> <li>Validate early - Add validation steps at the start</li> <li>Name your metrics - Use descriptive names</li> <li>Use preset pipelines - Don't reinvent common patterns</li> <li>Add callbacks for monitoring - Track progress and thresholds</li> <li>Save results - Auto-save for reproducibility</li> <li>Compose pipelines - Build complex from simple</li> </ol>"},{"location":"advanced/validators/","title":"Validators","text":"<p>PyEval provides flexible validators for input validation.</p>"},{"location":"advanced/validators/#quick-validation-functions","title":"Quick Validation Functions","text":""},{"location":"advanced/validators/#validate_predictions","title":"validate_predictions","text":"<p>Validate prediction arrays.</p> <pre><code>from pyeval import validate_predictions\n\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\n# Validates and returns True, or raises ValueError\nvalidate_predictions(y_true, y_pred)\n\n# With options\nvalidate_predictions(\n    y_true, y_pred,\n    allow_empty=False,\n    check_same_length=True,\n    check_same_classes=True\n)\n</code></pre>"},{"location":"advanced/validators/#validate_probabilities","title":"validate_probabilities","text":"<p>Validate probability arrays.</p> <pre><code>from pyeval import validate_probabilities\n\nprobs = [0.1, 0.3, 0.6]\n\n# Basic validation (values between 0 and 1)\nvalidate_probabilities(probs)\n\n# Require sum to 1 (for single prediction)\nvalidate_probabilities(probs, require_sum_one=True)\n\n# Multi-class probabilities\nmulti_probs = [[0.1, 0.9], [0.7, 0.3], [0.5, 0.5]]\nvalidate_probabilities(multi_probs, require_sum_one=True)\n</code></pre>"},{"location":"advanced/validators/#composable-validators","title":"Composable Validators","text":""},{"location":"advanced/validators/#typevalidator","title":"TypeValidator","text":"<p>Validate value types.</p> <pre><code>from pyeval import TypeValidator\n\n# Single type\nint_validator = TypeValidator(int)\nresult = int_validator.validate(42)\nprint(result.is_valid)  # True\n\nresult = int_validator.validate(\"42\")\nprint(result.is_valid)  # False\nprint(result.errors)    # [\"Expected int, got str\"]\n\n# Multiple types\nnum_validator = TypeValidator((int, float))\nresult = num_validator.validate(3.14)\nprint(result.is_valid)  # True\n</code></pre>"},{"location":"advanced/validators/#listvalidator","title":"ListValidator","text":"<p>Validate list properties.</p> <pre><code>from pyeval import ListValidator\n\n# Basic list validation\nvalidator = ListValidator()\nresult = validator.validate([1, 2, 3])\nprint(result.is_valid)  # True\n\n# With constraints\nvalidator = ListValidator(\n    element_type=int,\n    min_length=1,\n    max_length=100,\n    unique=False\n)\n\nresult = validator.validate([1, 2, 3])\nprint(result.is_valid)  # True\n\nresult = validator.validate([])\nprint(result.is_valid)  # False\nprint(result.errors)    # [\"List length 0 below minimum 1\"]\n\nresult = validator.validate([1, 2, \"3\"])\nprint(result.is_valid)  # False\nprint(result.errors)    # [\"Element at index 2 is not int\"]\n</code></pre>"},{"location":"advanced/validators/#numericvalidator","title":"NumericValidator","text":"<p>Validate numeric values.</p> <pre><code>from pyeval import NumericValidator\n\n# Range validation\nprob_validator = NumericValidator(min_val=0, max_val=1)\nresult = prob_validator.validate(0.5)\nprint(result.is_valid)  # True\n\nresult = prob_validator.validate(1.5)\nprint(result.is_valid)  # False\nprint(result.errors)    # [\"Value 1.5 exceeds maximum 1\"]\n\n# Integer validation\nint_validator = NumericValidator(integer_only=True)\nresult = int_validator.validate(3.14)\nprint(result.is_valid)  # False\n\n# Positive only\npos_validator = NumericValidator(min_val=0, exclude_min=True)\nresult = pos_validator.validate(0)\nprint(result.is_valid)  # False (0 excluded)\n</code></pre>"},{"location":"advanced/validators/#schema-validation","title":"Schema Validation","text":""},{"location":"advanced/validators/#schemavalidator","title":"SchemaValidator","text":"<p>Validate complex data structures.</p> <pre><code>from pyeval import SchemaValidator, FieldSchema, ListValidator, NumericValidator\n\n# Define schema\nschema = SchemaValidator([\n    FieldSchema('y_true', ListValidator(min_length=1)),\n    FieldSchema('y_pred', ListValidator(min_length=1)),\n    FieldSchema('threshold', NumericValidator(0, 1), required=False, default=0.5),\n    FieldSchema('average', TypeValidator(str), required=False, default='binary')\n])\n\n# Validate data\ndata = {\n    'y_true': [1, 0, 1, 1, 0],\n    'y_pred': [1, 0, 0, 1, 0]\n}\n\nresult = schema.validate(data)\nprint(result.is_valid)  # True\nprint(result.validated_data)\n# {'y_true': [1,0,1,1,0], 'y_pred': [1,0,0,1,0], 'threshold': 0.5, 'average': 'binary'}\n\n# Invalid data\ndata = {\n    'y_true': [],\n    'y_pred': [1, 0]\n}\n\nresult = schema.validate(data)\nprint(result.is_valid)  # False\nprint(result.errors)    # [\"y_true: List length 0 below minimum 1\"]\n</code></pre>"},{"location":"advanced/validators/#nested-schemas","title":"Nested Schemas","text":"<pre><code>from pyeval import SchemaValidator, FieldSchema, ListValidator\n\n# Nested schema for evaluation config\nconfig_schema = SchemaValidator([\n    FieldSchema('name', TypeValidator(str)),\n    FieldSchema('metrics', ListValidator(element_type=str, min_length=1)),\n    FieldSchema('data', SchemaValidator([\n        FieldSchema('y_true', ListValidator()),\n        FieldSchema('y_pred', ListValidator())\n    ]))\n])\n\nconfig = {\n    'name': 'experiment_1',\n    'metrics': ['accuracy', 'f1'],\n    'data': {\n        'y_true': [1, 0, 1],\n        'y_pred': [1, 0, 0]\n    }\n}\n\nresult = config_schema.validate(config)\nprint(result.is_valid)  # True\n</code></pre>"},{"location":"advanced/validators/#custom-validators","title":"Custom Validators","text":""},{"location":"advanced/validators/#creating-custom-validators","title":"Creating Custom Validators","text":"<pre><code>from pyeval import Validator, ValidationResult\n\nclass BinaryLabelsValidator(Validator):\n    \"\"\"Validates that all labels are binary (0 or 1).\"\"\"\n\n    def validate(self, value):\n        if not isinstance(value, (list, tuple)):\n            return ValidationResult(\n                is_valid=False,\n                errors=[\"Input must be a list or tuple\"]\n            )\n\n        invalid = [v for v in value if v not in (0, 1)]\n        if invalid:\n            return ValidationResult(\n                is_valid=False,\n                errors=[f\"Invalid labels found: {invalid}. Expected 0 or 1.\"]\n            )\n\n        return ValidationResult(is_valid=True)\n\n# Use custom validator\nvalidator = BinaryLabelsValidator()\nresult = validator.validate([1, 0, 1, 0])\nprint(result.is_valid)  # True\n\nresult = validator.validate([1, 0, 2, 0])\nprint(result.is_valid)  # False\n</code></pre>"},{"location":"advanced/validators/#combining-validators","title":"Combining Validators","text":"<pre><code>from pyeval import CompositeValidator, ListValidator, TypeValidator\n\n# Combine multiple validators\ncombined = CompositeValidator([\n    TypeValidator(list),\n    ListValidator(min_length=1, max_length=1000),\n    BinaryLabelsValidator()\n])\n\nresult = combined.validate([1, 0, 1, 0])\nprint(result.is_valid)  # True\n\nresult = combined.validate([1, 0, 2])\nprint(result.is_valid)  # False\nprint(result.errors)    # Shows all validation errors\n</code></pre>"},{"location":"advanced/validators/#validation-results","title":"Validation Results","text":""},{"location":"advanced/validators/#validationresult","title":"ValidationResult","text":"<pre><code>from pyeval import ValidationResult\n\n# Success\nresult = ValidationResult(is_valid=True, validated_value=[1, 0, 1])\n\nprint(result.is_valid)        # True\nprint(result.errors)          # []\nprint(result.validated_value) # [1, 0, 1]\n\n# Failure\nresult = ValidationResult(\n    is_valid=False,\n    errors=[\"Length mismatch\", \"Invalid values\"],\n    context={'field': 'y_pred'}\n)\n\nprint(result.is_valid)  # False\nprint(result.errors)    # [\"Length mismatch\", \"Invalid values\"]\nprint(result.context)   # {'field': 'y_pred'}\n\n# Raise on invalid\nresult.raise_if_invalid()  # Raises ValueError if not valid\n</code></pre>"},{"location":"advanced/validators/#validation-decorators","title":"Validation Decorators","text":""},{"location":"advanced/validators/#using-validators-as-decorators","title":"Using validators as decorators","text":"<pre><code>from pyeval import validated\n\n@validated({\n    'y_true': ListValidator(min_length=1),\n    'y_pred': ListValidator(min_length=1),\n    'threshold': NumericValidator(0, 1)\n})\ndef custom_metric(y_true, y_pred, threshold=0.5):\n    \"\"\"Automatically validates inputs.\"\"\"\n    return compute(y_true, y_pred, threshold)\n\n# Valid call\nresult = custom_metric([1,0,1], [1,0,0], 0.5)\n\n# Invalid call - raises ValueError\nresult = custom_metric([], [1,0,0])\n# ValueError: y_true: List length 0 below minimum 1\n</code></pre>"},{"location":"advanced/validators/#complete-validator-reference","title":"Complete Validator Reference","text":"Validator Description <code>TypeValidator</code> Validates value types <code>ListValidator</code> Validates list properties <code>NumericValidator</code> Validates numeric ranges <code>SchemaValidator</code> Validates complex schemas <code>FieldSchema</code> Defines schema fields <code>CompositeValidator</code> Combines multiple validators <code>validate_predictions</code> Quick prediction validation <code>validate_probabilities</code> Quick probability validation"},{"location":"advanced/validators/#best-practices","title":"Best Practices","text":"<ol> <li>Validate early - Check inputs at function entry</li> <li>Provide clear errors - Help users fix issues</li> <li>Use schemas for complex data - Better organization</li> <li>Combine validators - Build complex validation from simple parts</li> <li>Cache validation results - Avoid re-validating same data</li> </ol>"},{"location":"api/fairness/","title":"Fairness Metrics API Reference","text":"<p>Metrics for evaluating model fairness and bias.</p>"},{"location":"api/fairness/#group-fairness-metrics","title":"Group Fairness Metrics","text":""},{"location":"api/fairness/#demographic_parity","title":"demographic_parity","text":"<p>Evaluate demographic parity (statistical parity) across groups.</p> <pre><code>from pyeval import demographic_parity\n\ny_pred = [1, 0, 1, 1, 0, 0, 1, 1]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nresult = demographic_parity(y_pred, sensitive)\nprint(result)\n# {\n#     'dp_difference': 0.25,\n#     'is_fair': False,  # |difference| &gt; 0.1\n#     'group_rates': {'A': 0.75, 'B': 0.50},\n#     'reference_group': 'A',\n#     'privileged_group': 'A'\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_pred</code> list required Predicted labels <code>sensitive_attr</code> list required Sensitive attribute values <code>threshold</code> float 0.1 Fairness threshold <p>Returns: <code>dict</code> - Demographic parity analysis</p>"},{"location":"api/fairness/#equalized_odds","title":"equalized_odds","text":"<p>Evaluate equalized odds (equal TPR and FPR across groups).</p> <pre><code>from pyeval import equalized_odds\n\ny_true = [1, 1, 0, 0, 1, 1, 0, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nresult = equalized_odds(y_true, y_pred, sensitive)\nprint(result)\n# {\n#     'eo_difference': 0.15,\n#     'tpr_difference': 0.0,\n#     'fpr_difference': 0.25,\n#     'is_fair': False,\n#     'group_tpr': {'A': 0.5, 'B': 0.5},\n#     'group_fpr': {'A': 0.0, 'B': 0.25}\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_true</code> list required Ground truth labels <code>y_pred</code> list required Predicted labels <code>sensitive_attr</code> list required Sensitive attribute values <code>threshold</code> float 0.1 Fairness threshold <p>Returns: <code>dict</code> - Equalized odds analysis</p>"},{"location":"api/fairness/#disparate_impact","title":"disparate_impact","text":"<p>Compute disparate impact ratio.</p> <pre><code>from pyeval import disparate_impact\n\ny_pred = [1, 0, 1, 1, 0, 0, 1, 1]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nresult = disparate_impact(y_pred, sensitive)\nprint(result)\n# {\n#     'di_ratio': 0.67,  # Rate_B / Rate_A\n#     'is_fair': False,  # ratio &lt; 0.8 (4/5 rule)\n#     'group_rates': {'A': 0.75, 'B': 0.50},\n#     'four_fifths_rule': False\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_pred</code> list required Predicted labels <code>sensitive_attr</code> list required Sensitive attribute values <code>threshold</code> float 0.8 Four-fifths rule threshold <p>Returns: <code>dict</code> - Disparate impact analysis</p>"},{"location":"api/fairness/#true_positive_rate_difference","title":"true_positive_rate_difference","text":"<p>Compute True Positive Rate (TPR) difference between groups.</p> <pre><code>from pyeval import true_positive_rate_difference\n\ny_true = [1, 1, 0, 0, 1, 1, 0, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nresult = true_positive_rate_difference(y_true, y_pred, sensitive)\nprint(result)\n# {\n#     'tpr_difference': 0.5,\n#     'group_tpr': {'A': 0.5, 'B': 1.0},\n#     'is_fair': False\n# }\n</code></pre> <p>Returns: <code>dict</code> - TPR difference analysis</p>"},{"location":"api/fairness/#false_positive_rate_difference","title":"false_positive_rate_difference","text":"<p>Compute False Positive Rate (FPR) difference between groups.</p> <pre><code>from pyeval import false_positive_rate_difference\n\ny_true = [1, 1, 0, 0, 1, 1, 0, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nresult = false_positive_rate_difference(y_true, y_pred, sensitive)\nprint(result)\n# {\n#     'fpr_difference': 0.5,\n#     'group_fpr': {'A': 0.0, 'B': 0.5},\n#     'is_fair': False\n# }\n</code></pre> <p>Returns: <code>dict</code> - FPR difference analysis</p>"},{"location":"api/fairness/#calibration-metrics","title":"Calibration Metrics","text":""},{"location":"api/fairness/#calibration_by_group","title":"calibration_by_group","text":"<p>Evaluate prediction calibration across groups.</p> <pre><code>from pyeval import calibration_by_group\n\ny_true = [1, 0, 1, 0, 1, 0, 1, 0]\ny_prob = [0.9, 0.1, 0.8, 0.2, 0.7, 0.3, 0.6, 0.4]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nresult = calibration_by_group(y_true, y_prob, sensitive)\nprint(result)\n# {\n#     'calibration_difference': 0.05,\n#     'group_calibration': {'A': 0.85, 'B': 0.80},\n#     'is_calibrated': True\n# }\n</code></pre> <p>Returns: <code>dict</code> - Calibration analysis per group</p>"},{"location":"api/fairness/#predictive_parity","title":"predictive_parity","text":"<p>Evaluate if precision is equal across groups.</p> <pre><code>from pyeval import predictive_parity\n\ny_true = [1, 1, 0, 0, 1, 1, 0, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nresult = predictive_parity(y_true, y_pred, sensitive)\nprint(result)\n# {\n#     'pp_difference': 0.17,\n#     'group_precision': {'A': 1.0, 'B': 0.67},\n#     'is_fair': False\n# }\n</code></pre> <p>Returns: <code>dict</code> - Predictive parity analysis</p>"},{"location":"api/fairness/#individual-fairness-metrics","title":"Individual Fairness Metrics","text":""},{"location":"api/fairness/#individual_fairness","title":"individual_fairness","text":"<p>Evaluate individual fairness (similar individuals get similar predictions).</p> <pre><code>from pyeval import individual_fairness\n\n# Features for each individual\nX = [\n    [0.5, 0.3],\n    [0.5, 0.35],  # Similar to first\n    [0.9, 0.8],\n    [0.1, 0.2]\n]\n\n# Predictions\ny_pred = [1, 0, 1, 0]\n\nresult = individual_fairness(X, y_pred)\nprint(result)\n# {\n#     'individual_fairness_score': 0.75,\n#     'violations': 1,  # Similar inputs with different outputs\n#     'consistency_score': 0.75\n# }\n</code></pre> <p>Returns: <code>dict</code> - Individual fairness analysis</p>"},{"location":"api/fairness/#counterfactual_fairness","title":"counterfactual_fairness","text":"<p>Evaluate counterfactual fairness.</p> <pre><code>from pyeval import counterfactual_fairness\n\n# Original predictions\ny_pred_original = [1, 0, 1, 0, 1]\nsensitive_original = ['A', 'A', 'B', 'B', 'A']\n\n# Counterfactual predictions (with flipped sensitive attribute)\ny_pred_counterfactual = [1, 0, 0, 1, 1]\n\nresult = counterfactual_fairness(\n    y_pred_original, \n    y_pred_counterfactual\n)\nprint(result)\n# {\n#     'cf_score': 0.6,\n#     'changed_predictions': 2,\n#     'total_predictions': 5\n# }\n</code></pre> <p>Returns: <code>dict</code> - Counterfactual fairness analysis</p>"},{"location":"api/fairness/#comprehensive-analysis","title":"Comprehensive Analysis","text":""},{"location":"api/fairness/#fairness_report","title":"fairness_report","text":"<p>Generate a comprehensive fairness report.</p> <pre><code>from pyeval import fairness_report\n\ny_true = [1, 1, 0, 0, 1, 1, 0, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\nreport = fairness_report(y_true, y_pred, sensitive)\nprint(report)\n# {\n#     'demographic_parity': {...},\n#     'equalized_odds': {...},\n#     'disparate_impact': {...},\n#     'predictive_parity': {...},\n#     'overall_fairness_score': 0.72,\n#     'recommendations': [\n#         'Consider rebalancing training data',\n#         'Review features correlated with sensitive attribute'\n#     ]\n# }\n</code></pre>"},{"location":"api/fairness/#metric-class","title":"Metric Class","text":""},{"location":"api/fairness/#fairnessmetrics","title":"FairnessMetrics","text":"<p>Compute all fairness metrics at once.</p> <pre><code>from pyeval import FairnessMetrics\n\nfm = FairnessMetrics()\n\nresults = fm.compute(\n    y_true=y_true,\n    y_pred=y_pred,\n    sensitive_attr=sensitive\n)\n\nprint(results)\n# {\n#     'demographic_parity_diff': 0.25,\n#     'equalized_odds_diff': 0.15,\n#     'disparate_impact_ratio': 0.67,\n#     'tpr_difference': 0.0,\n#     'fpr_difference': 0.25,\n#     'is_fair': False,\n#     ...\n# }\n</code></pre>"},{"location":"api/fairness/#complete-fairness-metrics-list","title":"Complete Fairness Metrics List","text":"Metric Function Description Demographic Parity <code>demographic_parity</code> Equal positive rate across groups Equalized Odds <code>equalized_odds</code> Equal TPR and FPR across groups Disparate Impact <code>disparate_impact</code> Ratio of positive rates (4/5 rule) TPR Difference <code>true_positive_rate_difference</code> True positive rate gap FPR Difference <code>false_positive_rate_difference</code> False positive rate gap Calibration <code>calibration_by_group</code> Prediction calibration per group Predictive Parity <code>predictive_parity</code> Equal precision across groups Individual Fairness <code>individual_fairness</code> Similar treatment for similar individuals Counterfactual <code>counterfactual_fairness</code> Fairness under attribute change"},{"location":"api/fairness/#fairness-criteria-reference","title":"Fairness Criteria Reference","text":"Criterion Definition When to Use Demographic Parity P(\u0176=1|A=a) = P(\u0176=1|A=b) When outcome should be independent of sensitive attribute Equalized Odds P(\u0176=1|Y=y,A=a) = P(\u0176=1|Y=y,A=b) When error rates should be equal Disparate Impact P(\u0176=1|A=a) / P(\u0176=1|A=b) \u2265 0.8 Legal compliance (US 4/5 rule) Predictive Parity P(Y=1|\u0176=1,A=a) = P(Y=1|\u0176=1,A=b) When precision should be equal Individual Fairness Similar \u2192 Similar predictions When individual treatment matters"},{"location":"api/fairness/#usage-tips","title":"Usage Tips","text":""},{"location":"api/fairness/#handling-multiple-sensitive-attributes","title":"Handling Multiple Sensitive Attributes","text":"<pre><code>from pyeval import demographic_parity\n\n# Create intersectional groups\ndef intersect_attributes(attr1, attr2):\n    return [f\"{a}_{b}\" for a, b in zip(attr1, attr2)]\n\ngender = ['M', 'M', 'F', 'F', 'M', 'F']\nrace = ['A', 'B', 'A', 'B', 'A', 'B']\n\nintersectional = intersect_attributes(gender, race)\n# ['M_A', 'M_B', 'F_A', 'F_B', 'M_A', 'F_B']\n\nresult = demographic_parity(y_pred, intersectional)\n</code></pre>"},{"location":"api/fairness/#setting-fairness-thresholds","title":"Setting Fairness Thresholds","text":"<pre><code>from pyeval import equalized_odds\n\n# Strict threshold\nresult_strict = equalized_odds(y_true, y_pred, sensitive, threshold=0.05)\n\n# Lenient threshold  \nresult_lenient = equalized_odds(y_true, y_pred, sensitive, threshold=0.2)\n</code></pre>"},{"location":"api/fairness/#comparing-models","title":"Comparing Models","text":"<pre><code>from pyeval import FairnessMetrics\n\nfm = FairnessMetrics()\n\n# Evaluate multiple models\nmodels = {\n    'model_a': predictions_a,\n    'model_b': predictions_b,\n    'model_c': predictions_c\n}\n\nfairness_scores = {}\nfor name, preds in models.items():\n    result = fm.compute(y_true, preds, sensitive)\n    fairness_scores[name] = {\n        'dp_diff': result['demographic_parity_diff'],\n        'eo_diff': result['equalized_odds_diff'],\n        'di_ratio': result['disparate_impact_ratio']\n    }\n\n# Find fairest model\nfairest = min(fairness_scores.items(), \n              key=lambda x: abs(x[1]['dp_diff']))\nprint(f\"Fairest model: {fairest[0]}\")\n</code></pre>"},{"location":"api/llm/","title":"LLM Metrics API Reference","text":"<p>Metrics for evaluating Large Language Model outputs.</p>"},{"location":"api/llm/#content-quality-metrics","title":"Content Quality Metrics","text":""},{"location":"api/llm/#toxicity_score","title":"toxicity_score","text":"<p>Evaluate toxicity in generated text using keyword and pattern matching.</p> <pre><code>from pyeval import toxicity_score\n\nresponse = \"This is a friendly and helpful response.\"\nresult = toxicity_score(response)\n\nprint(result)\n# {\n#     'toxicity_score': 0.05,\n#     'is_toxic': False,\n#     'toxic_patterns_found': [],\n#     'severity': 'none'\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>text</code> str required Text to evaluate <code>threshold</code> float 0.5 Toxicity threshold <p>Returns: <code>dict</code> - Toxicity analysis results</p>"},{"location":"api/llm/#coherence_score","title":"coherence_score","text":"<p>Evaluate the coherence and logical flow of text.</p> <pre><code>from pyeval import coherence_score\n\nresponse = \"\"\"\nMachine learning is a subset of artificial intelligence.\nIt enables computers to learn from data.\nThis learning improves their performance over time.\n\"\"\"\n\nresult = coherence_score(response)\nprint(result['coherence_score'])  # 0.85\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>text</code> str Text to evaluate <p>Returns: <code>dict</code> - {'coherence_score': float, 'sentence_count': int, 'transition_quality': float}</p>"},{"location":"api/llm/#hallucination_score","title":"hallucination_score","text":"<p>Detect hallucinations by comparing response to reference context.</p> <pre><code>from pyeval import hallucination_score\n\ncontext = \"Python was created by Guido van Rossum in 1991.\"\nresponse = \"Python was created by Guido van Rossum in 1989.\"\n\nresult = hallucination_score(response, context)\nprint(result)\n# {\n#     'hallucination_score': 0.3,\n#     'grounded_ratio': 0.7,\n#     'ungrounded_claims': ['1989']\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>response</code> str Generated response <code>context</code> str Reference context/facts <p>Returns: <code>dict</code> - Hallucination analysis</p>"},{"location":"api/llm/#bias_detection_score","title":"bias_detection_score","text":"<p>Detect potential bias in text.</p> <pre><code>from pyeval import bias_detection_score\n\ntext = \"Engineers are typically men who work long hours.\"\n\nresult = bias_detection_score(text)\nprint(result)\n# {\n#     'bias_score': 0.6,\n#     'bias_types': ['gender'],\n#     'flagged_phrases': ['typically men'],\n#     'severity': 'moderate'\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>text</code> str Text to analyze <p>Returns: <code>dict</code> - Bias detection results</p>"},{"location":"api/llm/#instruction_following_score","title":"instruction_following_score","text":"<p>Evaluate how well the response follows the given instruction.</p> <pre><code>from pyeval import instruction_following_score\n\ninstruction = \"Explain quantum computing in simple terms\"\nresponse = \"Quantum computing uses quantum bits that can be 0 and 1 at the same time...\"\n\nresult = instruction_following_score(instruction, response)\nprint(result['instruction_following_score'])  # 0.85\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>instruction</code> str The original instruction/prompt <code>response</code> str Generated response <p>Returns: <code>dict</code> - Instruction following analysis</p>"},{"location":"api/llm/#safety-metrics","title":"Safety Metrics","text":""},{"location":"api/llm/#safety_score","title":"safety_score","text":"<p>Evaluate overall safety of generated content.</p> <pre><code>from pyeval import safety_score\n\nresponse = \"Here's how to bake a delicious chocolate cake...\"\n\nresult = safety_score(response)\nprint(result)\n# {\n#     'safety_score': 0.95,\n#     'is_safe': True,\n#     'risk_categories': [],\n#     'confidence': 0.9\n# }\n</code></pre> <p>Returns: <code>dict</code> - Safety evaluation results</p>"},{"location":"api/llm/#harmful_content_score","title":"harmful_content_score","text":"<p>Detect harmful or dangerous content.</p> <pre><code>from pyeval import harmful_content_score\n\ntext = \"Always consult a doctor before taking any medication.\"\nresult = harmful_content_score(text)\n</code></pre> <p>Returns: <code>dict</code> - {'harmful_score': float, 'categories': list, 'is_harmful': bool}</p>"},{"location":"api/llm/#conversation-metrics","title":"Conversation Metrics","text":""},{"location":"api/llm/#multi_turn_coherence","title":"multi_turn_coherence","text":"<p>Evaluate coherence across a multi-turn conversation.</p> <pre><code>from pyeval import multi_turn_coherence\n\nconversation = [\n    {\"role\": \"user\", \"content\": \"What is Python?\"},\n    {\"role\": \"assistant\", \"content\": \"Python is a programming language.\"},\n    {\"role\": \"user\", \"content\": \"What can I build with it?\"},\n    {\"role\": \"assistant\", \"content\": \"You can build web apps, ML models, and more.\"}\n]\n\nresult = multi_turn_coherence(conversation)\nprint(result['coherence_score'])  # 0.88\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>conversation</code> list[dict] List of conversation turns <p>Returns: <code>dict</code> - Multi-turn coherence analysis</p>"},{"location":"api/llm/#response_diversity","title":"response_diversity","text":"<p>Measure diversity across multiple responses.</p> <pre><code>from pyeval import response_diversity\n\nresponses = [\n    \"Machine learning is great for data analysis.\",\n    \"ML helps analyze large datasets efficiently.\",\n    \"Using ML, we can process data at scale.\"\n]\n\nresult = response_diversity(responses)\nprint(result['diversity_score'])  # 0.72\n</code></pre> <p>Returns: <code>dict</code> - {'diversity_score': float, 'unique_ngrams': int}</p>"},{"location":"api/llm/#summarization-metrics","title":"Summarization Metrics","text":""},{"location":"api/llm/#summarization_quality","title":"summarization_quality","text":"<p>Evaluate summary quality against source document.</p> <pre><code>from pyeval import summarization_quality\n\nsource = \"\"\"\nMachine learning is a method of data analysis that automates \nanalytical model building. It is a branch of artificial intelligence \nbased on the idea that systems can learn from data, identify patterns \nand make decisions with minimal human intervention.\n\"\"\"\n\nsummary = \"Machine learning automates data analysis using AI to find patterns.\"\n\nresult = summarization_quality(source, summary)\nprint(result)\n# {\n#     'quality_score': 0.82,\n#     'coverage': 0.75,\n#     'compression_ratio': 0.15,\n#     'informativeness': 0.85\n# }\n</code></pre> <p>Returns: <code>dict</code> - Summarization quality metrics</p>"},{"location":"api/llm/#factuality-metrics","title":"Factuality Metrics","text":""},{"location":"api/llm/#factuality_score","title":"factuality_score","text":"<p>Evaluate factual accuracy of a response.</p> <pre><code>from pyeval import factuality_score\n\nfacts = [\n    \"The Earth orbits the Sun\",\n    \"Water boils at 100\u00b0C at sea level\",\n    \"Python was created in 1991\"\n]\n\nresponse = \"The Earth orbits the Sun, and water boils at 100 degrees Celsius.\"\n\nresult = factuality_score(response, facts)\nprint(result['factuality_score'])  # 0.9\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>response</code> str Text to evaluate <code>facts</code> list[str] Known facts to verify against <p>Returns: <code>dict</code> - Factuality analysis</p>"},{"location":"api/llm/#metric-class","title":"Metric Class","text":""},{"location":"api/llm/#llmmetrics","title":"LLMMetrics","text":"<p>Compute all LLM metrics at once.</p> <pre><code>from pyeval import LLMMetrics\n\nllm_metrics = LLMMetrics()\n\nresult = llm_metrics.compute(\n    response=\"Machine learning enables computers to learn from data.\",\n    context=\"ML is a subset of AI focusing on learning algorithms.\",\n    instruction=\"Explain machine learning briefly\"\n)\n\nprint(result)\n# {\n#     'toxicity': 0.02,\n#     'coherence': 0.92,\n#     'hallucination': 0.15,\n#     'instruction_following': 0.88,\n#     'safety': 0.98,\n#     ...\n# }\n</code></pre>"},{"location":"api/llm/#complete-llm-metrics-list","title":"Complete LLM Metrics List","text":"Metric Function Description Toxicity <code>toxicity_score</code> Harmful language detection Coherence <code>coherence_score</code> Logical flow evaluation Hallucination <code>hallucination_score</code> Factual grounding check Bias <code>bias_detection_score</code> Bias pattern detection Instruction Following <code>instruction_following_score</code> Prompt adherence Safety <code>safety_score</code> Content safety evaluation Harmful Content <code>harmful_content_score</code> Dangerous content detection Multi-turn Coherence <code>multi_turn_coherence</code> Conversation consistency Response Diversity <code>response_diversity</code> Output variation Summarization Quality <code>summarization_quality</code> Summary evaluation Factuality <code>factuality_score</code> Fact verification"},{"location":"api/llm/#usage-tips","title":"Usage Tips","text":""},{"location":"api/llm/#combining-multiple-metrics","title":"Combining Multiple Metrics","text":"<pre><code>from pyeval import (\n    toxicity_score, coherence_score, \n    hallucination_score, safety_score\n)\n\ndef comprehensive_evaluation(response, context=None):\n    results = {\n        'toxicity': toxicity_score(response),\n        'coherence': coherence_score(response),\n        'safety': safety_score(response)\n    }\n\n    if context:\n        results['hallucination'] = hallucination_score(response, context)\n\n    # Compute overall quality score\n    scores = [r.get('toxicity_score', r.get('coherence_score', r.get('safety_score', 0))) \n              for r in results.values()]\n    results['overall'] = sum(scores) / len(scores)\n\n    return results\n</code></pre>"},{"location":"api/llm/#batch-evaluation","title":"Batch Evaluation","text":"<pre><code>from pyeval import LLMMetrics\n\nmetrics = LLMMetrics()\n\nresponses = [\n    \"Response 1...\",\n    \"Response 2...\",\n    \"Response 3...\"\n]\n\nresults = [metrics.compute(response=r) for r in responses]\n\n# Aggregate results\navg_toxicity = sum(r['toxicity'] for r in results) / len(results)\n</code></pre>"},{"location":"api/ml/","title":"ML Metrics API Reference","text":"<p>Machine Learning metrics for Classification, Regression, and Clustering evaluation.</p>"},{"location":"api/ml/#classification-metrics","title":"Classification Metrics","text":""},{"location":"api/ml/#accuracy_score","title":"accuracy_score","text":"<p>Compute the accuracy classification score.</p> <pre><code>from pyeval import accuracy_score\n\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\nscore = accuracy_score(y_true, y_pred)\n# Returns: 0.8\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth labels <code>y_pred</code> list Predicted labels <p>Returns: <code>float</code> - Accuracy score between 0 and 1</p>"},{"location":"api/ml/#precision_score","title":"precision_score","text":"<p>Compute the precision score.</p> <pre><code>from pyeval import precision_score\n\nscore = precision_score(y_true, y_pred)\nscore = precision_score(y_true, y_pred, average='macro')  # For multiclass\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_true</code> list required Ground truth labels <code>y_pred</code> list required Predicted labels <code>average</code> str 'binary' 'binary', 'micro', 'macro', 'weighted' <code>pos_label</code> int 1 Positive class label for binary classification <p>Returns: <code>float</code> - Precision score between 0 and 1</p>"},{"location":"api/ml/#recall_score","title":"recall_score","text":"<p>Compute the recall score.</p> <pre><code>from pyeval import recall_score\n\nscore = recall_score(y_true, y_pred)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_true</code> list required Ground truth labels <code>y_pred</code> list required Predicted labels <code>average</code> str 'binary' 'binary', 'micro', 'macro', 'weighted' <code>pos_label</code> int 1 Positive class label for binary classification <p>Returns: <code>float</code> - Recall score between 0 and 1</p>"},{"location":"api/ml/#f1_score","title":"f1_score","text":"<p>Compute the F1 score (harmonic mean of precision and recall).</p> <pre><code>from pyeval import f1_score\n\nscore = f1_score(y_true, y_pred)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_true</code> list required Ground truth labels <code>y_pred</code> list required Predicted labels <code>average</code> str 'binary' 'binary', 'micro', 'macro', 'weighted' <code>beta</code> float 1.0 Weight of recall in F-beta score <p>Returns: <code>float</code> - F1 score between 0 and 1</p>"},{"location":"api/ml/#specificity_score","title":"specificity_score","text":"<p>Compute the specificity (true negative rate).</p> <pre><code>from pyeval import specificity_score\n\nscore = specificity_score(y_true, y_pred)\n# Returns: TN / (TN + FP)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth labels <code>y_pred</code> list Predicted labels <p>Returns: <code>float</code> - Specificity score between 0 and 1</p>"},{"location":"api/ml/#matthews_corrcoef","title":"matthews_corrcoef","text":"<p>Compute the Matthews Correlation Coefficient (MCC).</p> <pre><code>from pyeval import matthews_corrcoef\n\nmcc = matthews_corrcoef(y_true, y_pred)\n# Returns: value between -1 and 1\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth labels <code>y_pred</code> list Predicted labels <p>Returns: <code>float</code> - MCC between -1 (worst) and 1 (best), 0 indicates random</p>"},{"location":"api/ml/#cohen_kappa_score","title":"cohen_kappa_score","text":"<p>Compute Cohen's Kappa coefficient.</p> <pre><code>from pyeval import cohen_kappa_score\n\nkappa = cohen_kappa_score(y_true, y_pred)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_true</code> list required Ground truth labels <code>y_pred</code> list required Predicted labels <code>weights</code> str None None, 'linear', or 'quadratic' <p>Returns: <code>float</code> - Kappa coefficient between -1 and 1</p>"},{"location":"api/ml/#balanced_accuracy","title":"balanced_accuracy","text":"<p>Compute the balanced accuracy score.</p> <pre><code>from pyeval import balanced_accuracy\n\nscore = balanced_accuracy(y_true, y_pred)\n# Returns: Average of recall for each class\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth labels <code>y_pred</code> list Predicted labels <p>Returns: <code>float</code> - Balanced accuracy between 0 and 1</p>"},{"location":"api/ml/#log_loss","title":"log_loss","text":"<p>Compute the log loss (cross-entropy loss).</p> <pre><code>from pyeval import log_loss\n\ny_true = [1, 0, 1, 1]\ny_prob = [0.9, 0.1, 0.8, 0.7]\n\nloss = log_loss(y_true, y_prob)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>y_true</code> list required Ground truth labels <code>y_prob</code> list required Predicted probabilities <code>eps</code> float 1e-15 Small value to avoid log(0) <p>Returns: <code>float</code> - Log loss (lower is better)</p>"},{"location":"api/ml/#roc_auc_score","title":"roc_auc_score","text":"<p>Compute Area Under the ROC Curve.</p> <pre><code>from pyeval import roc_auc_score\n\ny_true = [1, 0, 1, 1, 0]\ny_prob = [0.9, 0.1, 0.8, 0.7, 0.3]\n\nauc = roc_auc_score(y_true, y_prob)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth binary labels <code>y_score</code> list Predicted scores/probabilities <p>Returns: <code>float</code> - AUC score between 0 and 1</p>"},{"location":"api/ml/#roc_curve","title":"roc_curve","text":"<p>Compute ROC curve points.</p> <pre><code>from pyeval import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_true, y_prob)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth binary labels <code>y_score</code> list Predicted scores/probabilities <p>Returns: <code>tuple</code> - (fpr, tpr, thresholds)</p>"},{"location":"api/ml/#precision_recall_curve","title":"precision_recall_curve","text":"<p>Compute precision-recall curve points.</p> <pre><code>from pyeval import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth binary labels <code>y_score</code> list Predicted scores/probabilities <p>Returns: <code>tuple</code> - (precision, recall, thresholds)</p>"},{"location":"api/ml/#confusion_matrix","title":"confusion_matrix","text":"<p>Compute confusion matrix.</p> <pre><code>from pyeval import confusion_matrix\n\nmatrix = confusion_matrix(y_true, y_pred)\n# Returns: [[TN, FP], [FN, TP]]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>y_true</code> list Ground truth labels <code>y_pred</code> list Predicted labels <p>Returns: <code>list[list]</code> - Confusion matrix</p>"},{"location":"api/ml/#regression-metrics","title":"Regression Metrics","text":""},{"location":"api/ml/#mean_squared_error","title":"mean_squared_error","text":"<p>Compute Mean Squared Error.</p> <pre><code>from pyeval import mean_squared_error\n\ny_true = [3.0, -0.5, 2.0, 7.0]\ny_pred = [2.5, 0.0, 2.0, 8.0]\n\nmse = mean_squared_error(y_true, y_pred)\n</code></pre> <p>Returns: <code>float</code> - MSE (lower is better)</p>"},{"location":"api/ml/#root_mean_squared_error","title":"root_mean_squared_error","text":"<p>Compute Root Mean Squared Error.</p> <pre><code>from pyeval import root_mean_squared_error\n\nrmse = root_mean_squared_error(y_true, y_pred)\n</code></pre> <p>Returns: <code>float</code> - RMSE (lower is better)</p>"},{"location":"api/ml/#mean_absolute_error","title":"mean_absolute_error","text":"<p>Compute Mean Absolute Error.</p> <pre><code>from pyeval import mean_absolute_error\n\nmae = mean_absolute_error(y_true, y_pred)\n</code></pre> <p>Returns: <code>float</code> - MAE (lower is better)</p>"},{"location":"api/ml/#mean_absolute_percentage_error","title":"mean_absolute_percentage_error","text":"<p>Compute Mean Absolute Percentage Error.</p> <pre><code>from pyeval import mean_absolute_percentage_error\n\nmape = mean_absolute_percentage_error(y_true, y_pred)\n</code></pre> <p>Returns: <code>float</code> - MAPE as a ratio (lower is better)</p>"},{"location":"api/ml/#r2_score","title":"r2_score","text":"<p>Compute R\u00b2 (coefficient of determination).</p> <pre><code>from pyeval import r2_score\n\nr2 = r2_score(y_true, y_pred)\n</code></pre> <p>Returns: <code>float</code> - R\u00b2 score (1 is perfect, can be negative)</p>"},{"location":"api/ml/#explained_variance_score","title":"explained_variance_score","text":"<p>Compute explained variance score.</p> <pre><code>from pyeval import explained_variance_score\n\nev = explained_variance_score(y_true, y_pred)\n</code></pre> <p>Returns: <code>float</code> - Explained variance (1 is best)</p>"},{"location":"api/ml/#clustering-metrics","title":"Clustering Metrics","text":""},{"location":"api/ml/#silhouette_score","title":"silhouette_score","text":"<p>Compute the mean Silhouette Coefficient.</p> <pre><code>from pyeval import silhouette_score\n\nX = [[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]\nlabels = [0, 0, 0, 1, 1, 1]\n\nscore = silhouette_score(X, labels)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>X</code> list[list] Feature matrix <code>labels</code> list Cluster labels <p>Returns: <code>float</code> - Silhouette score between -1 and 1</p>"},{"location":"api/ml/#adjusted_rand_index","title":"adjusted_rand_index","text":"<p>Compute Adjusted Rand Index.</p> <pre><code>from pyeval import adjusted_rand_index\n\nlabels_true = [0, 0, 1, 1, 2, 2]\nlabels_pred = [0, 0, 1, 1, 1, 2]\n\nari = adjusted_rand_index(labels_true, labels_pred)\n</code></pre> <p>Returns: <code>float</code> - ARI between -1 and 1</p>"},{"location":"api/ml/#normalized_mutual_info","title":"normalized_mutual_info","text":"<p>Compute Normalized Mutual Information.</p> <pre><code>from pyeval import normalized_mutual_info\n\nnmi = normalized_mutual_info(labels_true, labels_pred)\n</code></pre> <p>Returns: <code>float</code> - NMI between 0 and 1</p>"},{"location":"api/ml/#metric-classes","title":"Metric Classes","text":""},{"location":"api/ml/#classificationmetrics","title":"ClassificationMetrics","text":"<p>Compute all classification metrics at once.</p> <pre><code>from pyeval import ClassificationMetrics\n\ncm = ClassificationMetrics()\nresults = cm.compute(y_true, y_pred)\n\nprint(results)\n# {\n#     'accuracy': 0.85,\n#     'precision': 0.82,\n#     'recall': 0.88,\n#     'f1': 0.85,\n#     'specificity': 0.82,\n#     'mcc': 0.70,\n#     ...\n# }\n</code></pre>"},{"location":"api/ml/#regressionmetrics","title":"RegressionMetrics","text":"<p>Compute all regression metrics at once.</p> <pre><code>from pyeval import RegressionMetrics\n\nrm = RegressionMetrics()\nresults = rm.compute(y_true, y_pred)\n\nprint(results)\n# {\n#     'mse': 0.25,\n#     'rmse': 0.50,\n#     'mae': 0.40,\n#     'r2': 0.95,\n#     ...\n# }\n</code></pre>"},{"location":"api/ml/#clusteringmetrics","title":"ClusteringMetrics","text":"<p>Compute all clustering metrics at once.</p> <pre><code>from pyeval import ClusteringMetrics\n\ncm = ClusteringMetrics()\nresults = cm.compute(X, labels_true, labels_pred)\n</code></pre>"},{"location":"api/ml/#complete-classification-metrics-list","title":"Complete Classification Metrics List","text":"Metric Function Description Accuracy <code>accuracy_score</code> Overall correctness Precision <code>precision_score</code> True positives / predicted positives Recall <code>recall_score</code> True positives / actual positives F1 Score <code>f1_score</code> Harmonic mean of precision and recall Specificity <code>specificity_score</code> True negatives / actual negatives MCC <code>matthews_corrcoef</code> Matthews Correlation Coefficient Cohen's Kappa <code>cohen_kappa_score</code> Agreement beyond chance Balanced Accuracy <code>balanced_accuracy</code> Average recall per class Log Loss <code>log_loss</code> Cross-entropy loss AUC-ROC <code>roc_auc_score</code> Area under ROC curve Brier Score <code>brier_score</code> Mean squared error of probabilities Hamming Loss <code>hamming_loss</code> Fraction of wrong labels Jaccard Score <code>jaccard_score</code> Intersection over union Top-K Accuracy <code>top_k_accuracy</code> Correct in top K predictions ECE <code>expected_calibration_error</code> Calibration error"},{"location":"api/nlp/","title":"NLP Metrics API Reference","text":"<p>Natural Language Processing metrics for text generation evaluation.</p>"},{"location":"api/nlp/#text-generation-metrics","title":"Text Generation Metrics","text":""},{"location":"api/nlp/#bleu_score","title":"bleu_score","text":"<p>Compute BLEU (Bilingual Evaluation Understudy) score.</p> <pre><code>from pyeval import bleu_score\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A fast brown fox leaps over the lazy dog\"\n\n# Default: BLEU-4\nscore = bleu_score(reference, hypothesis)\n\n# Specific n-gram\nscore = bleu_score(reference, hypothesis, n=2)  # BLEU-2\n\n# With smoothing\nscore = bleu_score(reference, hypothesis, smoothing=True)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>reference</code> str/list required Reference text(s) <code>hypothesis</code> str required Generated text <code>n</code> int 4 Maximum n-gram order <code>smoothing</code> bool False Apply smoothing for short texts <code>weights</code> list None Custom n-gram weights <p>Returns: <code>float</code> - BLEU score between 0 and 1</p>"},{"location":"api/nlp/#rouge_score","title":"rouge_score","text":"<p>Compute ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores.</p> <pre><code>from pyeval import rouge_score\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A fast brown fox leaps over the lazy dog\"\n\n# ROUGE-L (Longest Common Subsequence)\nresult = rouge_score(reference, hypothesis, rouge_type='l')\nprint(result)  # {'precision': 0.78, 'recall': 0.78, 'f': 0.78}\n\n# ROUGE-1 (Unigrams)\nresult = rouge_score(reference, hypothesis, rouge_type='1')\n\n# ROUGE-2 (Bigrams)\nresult = rouge_score(reference, hypothesis, rouge_type='2')\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>reference</code> str required Reference text <code>hypothesis</code> str required Generated text <code>rouge_type</code> str 'l' '1', '2', 'l', or 's' (skip-bigram) <p>Returns: <code>dict</code> - {'precision': float, 'recall': float, 'f': float}</p>"},{"location":"api/nlp/#meteor_score","title":"meteor_score","text":"<p>Compute METEOR (Metric for Evaluation of Translation with Explicit ORdering) score.</p> <pre><code>from pyeval import meteor_score\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A fast brown fox leaps over the lazy dog\"\n\nscore = meteor_score(reference, hypothesis)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>reference</code> str required Reference text <code>hypothesis</code> str required Generated text <code>alpha</code> float 0.9 Precision weight <code>beta</code> float 3.0 Fragmentation penalty weight <code>gamma</code> float 0.5 Fragmentation penalty exponent <p>Returns: <code>float</code> - METEOR score between 0 and 1</p>"},{"location":"api/nlp/#ter_score","title":"ter_score","text":"<p>Compute TER (Translation Edit Rate) score.</p> <pre><code>from pyeval import ter_score\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A fast brown fox leaps over the lazy dog\"\n\nscore = ter_score(reference, hypothesis)\n# Returns: number of edits / reference length\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>reference</code> str Reference text <code>hypothesis</code> str Generated text <p>Returns: <code>float</code> - TER score (lower is better, can exceed 1)</p>"},{"location":"api/nlp/#chrf_score","title":"chrf_score","text":"<p>Compute chrF (character n-gram F-score).</p> <pre><code>from pyeval import chrf_score\n\nreference = \"The quick brown fox\"\nhypothesis = \"The fast brown fox\"\n\nscore = chrf_score(reference, hypothesis)\nscore = chrf_score(reference, hypothesis, n=6, beta=2)  # chrF++\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>reference</code> str required Reference text <code>hypothesis</code> str required Generated text <code>n</code> int 6 Maximum character n-gram order <code>beta</code> float 2.0 Recall weight (\u03b2 &gt; 1 favors recall) <p>Returns: <code>float</code> - chrF score between 0 and 1</p>"},{"location":"api/nlp/#distinct_n","title":"distinct_n","text":"<p>Compute Distinct-N diversity score.</p> <pre><code>from pyeval import distinct_n\n\ntexts = [\n    \"The cat sat on the mat\",\n    \"A dog ran in the park\",\n    \"Birds fly in the sky\"\n]\n\n# Distinct unigrams\nd1 = distinct_n(texts, n=1)\n\n# Distinct bigrams\nd2 = distinct_n(texts, n=2)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>texts</code> list[str] required List of generated texts <code>n</code> int 1 N-gram order <p>Returns: <code>float</code> - Ratio of unique n-grams to total n-grams</p>"},{"location":"api/nlp/#text_entropy","title":"text_entropy","text":"<p>Compute text entropy (information content).</p> <pre><code>from pyeval import text_entropy\n\ntext = \"The quick brown fox jumps over the lazy dog\"\n\nentropy = text_entropy(text)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>text</code> str required Input text <code>base</code> int 2 Logarithm base <p>Returns: <code>float</code> - Entropy value</p>"},{"location":"api/nlp/#perplexity","title":"perplexity","text":"<p>Compute perplexity of text.</p> <pre><code>from pyeval import perplexity\n\ntext = \"The quick brown fox jumps\"\nlog_probs = [-2.3, -1.5, -2.1, -1.8, -2.0]  # Log probabilities per token\n\nppl = perplexity(log_probs)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>log_probs</code> list[float] Log probabilities of each token <p>Returns: <code>float</code> - Perplexity (lower is better)</p>"},{"location":"api/nlp/#repetition_ratio","title":"repetition_ratio","text":"<p>Compute the repetition ratio in text.</p> <pre><code>from pyeval import repetition_ratio\n\ntext = \"The cat sat. The cat slept. The cat ate.\"\n\nratio = repetition_ratio(text, n=2)  # Repeated bigrams\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>text</code> str required Input text <code>n</code> int 3 N-gram size to check for repetition <p>Returns: <code>float</code> - Ratio of repeated n-grams</p>"},{"location":"api/nlp/#lexical_diversity","title":"lexical_diversity","text":"<p>Compute lexical diversity (Type-Token Ratio).</p> <pre><code>from pyeval import lexical_diversity\n\ntext = \"The quick brown fox jumps over the lazy dog\"\n\nttr = lexical_diversity(text)\n# Returns: unique_words / total_words\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>text</code> str Input text <p>Returns: <code>float</code> - Type-Token Ratio between 0 and 1</p>"},{"location":"api/nlp/#coverage_score","title":"coverage_score","text":"<p>Compute how much of the reference is covered in the hypothesis.</p> <pre><code>from pyeval import coverage_score\n\nreference = \"The quick brown fox jumps\"\nhypothesis = \"A quick brown fox leaps\"\n\nscore = coverage_score(reference, hypothesis)\n</code></pre> <p>Returns: <code>float</code> - Coverage ratio between 0 and 1</p>"},{"location":"api/nlp/#density_score","title":"density_score","text":"<p>Compute extractive density of summary.</p> <pre><code>from pyeval import density_score\n\nsource = \"The quick brown fox jumps over the lazy dog in the garden\"\nsummary = \"A brown fox jumps over a lazy dog\"\n\nscore = density_score(source, summary)\n</code></pre> <p>Returns: <code>float</code> - Density score</p>"},{"location":"api/nlp/#metric-class","title":"Metric Class","text":""},{"location":"api/nlp/#nlpmetrics","title":"NLPMetrics","text":"<p>Compute all NLP metrics at once.</p> <pre><code>from pyeval import NLPMetrics\n\nnlp = NLPMetrics()\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A fast brown fox leaps over the lazy dog\"\n\nresults = nlp.compute(reference, hypothesis)\n\nprint(results)\n# {\n#     'bleu': 0.45,\n#     'rouge_1': {'precision': 0.78, 'recall': 0.78, 'f': 0.78},\n#     'rouge_2': {'precision': 0.50, 'recall': 0.50, 'f': 0.50},\n#     'rouge_l': {'precision': 0.67, 'recall': 0.67, 'f': 0.67},\n#     'meteor': 0.72,\n#     'ter': 0.33,\n#     ...\n# }\n</code></pre>"},{"location":"api/nlp/#complete-nlp-metrics-list","title":"Complete NLP Metrics List","text":"Metric Function Description BLEU <code>bleu_score</code> N-gram precision with brevity penalty ROUGE-1 <code>rouge_score</code> Unigram overlap ROUGE-2 <code>rouge_score</code> Bigram overlap ROUGE-L <code>rouge_score</code> Longest common subsequence METEOR <code>meteor_score</code> Alignment-based with synonyms TER <code>ter_score</code> Translation edit rate chrF <code>chrf_score</code> Character n-gram F-score Distinct-N <code>distinct_n</code> N-gram diversity Entropy <code>text_entropy</code> Information content Perplexity <code>perplexity</code> Language model quality Repetition <code>repetition_ratio</code> Repeated n-grams ratio Lexical Diversity <code>lexical_diversity</code> Type-token ratio Coverage <code>coverage_score</code> Reference coverage Density <code>density_score</code> Extractive density"},{"location":"api/nlp/#usage-tips","title":"Usage Tips","text":""},{"location":"api/nlp/#choosing-the-right-metric","title":"Choosing the Right Metric","text":"Use Case Recommended Metrics Machine Translation BLEU, TER, chrF, METEOR Summarization ROUGE-1, ROUGE-2, ROUGE-L, Coverage Text Generation Distinct-N, Perplexity, Repetition Dialogue Systems BLEU, Distinct-N, Lexical Diversity"},{"location":"api/nlp/#multiple-references","title":"Multiple References","text":"<p>For multiple reference translations, pass a list:</p> <pre><code>references = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"A fast brown fox leaps over the lazy dog\"\n]\nhypothesis = \"The brown fox jumps over a lazy dog\"\n\n# BLEU with multiple references\nscore = bleu_score(references, hypothesis)\n</code></pre>"},{"location":"api/nlp/#corpus-level-scores","title":"Corpus-Level Scores","text":"<p>For corpus-level evaluation:</p> <pre><code>from pyeval import corpus_bleu\n\nreferences_list = [[\"ref1a\", \"ref1b\"], [\"ref2a\", \"ref2b\"]]\nhypotheses_list = [\"hyp1\", \"hyp2\"]\n\ncorpus_score = corpus_bleu(references_list, hypotheses_list)\n</code></pre>"},{"location":"api/overview/","title":"API Overview","text":"<p>This page provides a quick reference for all metrics available in PyEval.</p>"},{"location":"api/overview/#quick-reference-table","title":"\ud83d\udcca Quick Reference Table","text":"Domain Metrics Count Key Functions ML 40+ <code>accuracy_score</code>, <code>f1_score</code>, <code>roc_auc_score</code> NLP 15+ <code>bleu_score</code>, <code>rouge_score</code>, <code>meteor_score</code> LLM 12+ <code>toxicity_score</code>, <code>hallucination_score</code>, <code>coherence_score</code> RAG 15+ <code>context_relevance</code>, <code>groundedness_score</code>, <code>faithfulness_score</code> Fairness 10+ <code>demographic_parity</code>, <code>equalized_odds</code>, <code>disparate_impact</code> Speech 10+ <code>word_error_rate</code>, <code>character_error_rate</code> Recommender 15+ <code>precision_at_k</code>, <code>ndcg_at_k</code>, <code>mean_reciprocal_rank</code>"},{"location":"api/overview/#ml-metrics","title":"\ud83c\udfaf ML Metrics","text":""},{"location":"api/overview/#classification","title":"Classification","text":"Metric Function Returns Accuracy <code>accuracy_score(y_true, y_pred)</code> <code>float</code> Precision <code>precision_score(y_true, y_pred)</code> <code>float</code> Recall <code>recall_score(y_true, y_pred)</code> <code>float</code> F1 Score <code>f1_score(y_true, y_pred)</code> <code>float</code> ROC AUC <code>roc_auc_score(y_true, y_prob)</code> <code>float</code> Confusion Matrix <code>confusion_matrix(y_true, y_pred)</code> <code>list[list]</code> Matthews Corr. <code>matthews_corrcoef(y_true, y_pred)</code> <code>float</code> Cohen's Kappa <code>cohen_kappa_score(y_true, y_pred)</code> <code>float</code>"},{"location":"api/overview/#regression","title":"Regression","text":"Metric Function Returns MSE <code>mean_squared_error(y_true, y_pred)</code> <code>float</code> RMSE <code>root_mean_squared_error(y_true, y_pred)</code> <code>float</code> MAE <code>mean_absolute_error(y_true, y_pred)</code> <code>float</code> R\u00b2 <code>r2_score(y_true, y_pred)</code> <code>float</code> MAPE <code>mean_absolute_percentage_error(y_true, y_pred)</code> <code>float</code>"},{"location":"api/overview/#nlp-metrics","title":"\ud83d\udcdd NLP Metrics","text":"Metric Function Returns BLEU <code>bleu_score(reference, hypothesis)</code> <code>float</code> ROUGE <code>rouge_score(reference, hypothesis)</code> <code>dict</code> METEOR <code>meteor_score(reference, hypothesis)</code> <code>float</code> TER <code>ter_score(reference, hypothesis)</code> <code>float</code> chrF <code>chrf_score(reference, hypothesis)</code> <code>float</code> BERTScore <code>bert_score(reference, hypothesis)</code> <code>dict</code>"},{"location":"api/overview/#llm-metrics","title":"\ud83e\udd16 LLM Metrics","text":"Metric Function Returns Toxicity <code>toxicity_score(text)</code> <code>dict</code> Coherence <code>coherence_score(text)</code> <code>dict</code> Hallucination <code>hallucination_score(response, context)</code> <code>dict</code> Bias Detection <code>bias_detection_score(text)</code> <code>dict</code> Fluency <code>fluency_score(text)</code> <code>dict</code> Factuality <code>factuality_score(text, facts)</code> <code>dict</code>"},{"location":"api/overview/#rag-metrics","title":"\ud83d\udd0d RAG Metrics","text":"Metric Function Returns Context Relevance <code>context_relevance(query, context)</code> <code>float</code> Groundedness <code>groundedness_score(response, context)</code> <code>float</code> Faithfulness <code>faithfulness_score(response, context)</code> <code>float</code> Answer Correctness <code>answer_correctness(response, ground_truth)</code> <code>float</code> Retrieval Precision <code>retrieval_precision(retrieved, relevant)</code> <code>float</code> Retrieval Recall <code>retrieval_recall(retrieved, relevant)</code> <code>float</code>"},{"location":"api/overview/#fairness-metrics","title":"\u2696\ufe0f Fairness Metrics","text":"Metric Function Returns Demographic Parity <code>demographic_parity(y_pred, sensitive)</code> <code>dict</code> Equalized Odds <code>equalized_odds(y_true, y_pred, sensitive)</code> <code>dict</code> Equal Opportunity <code>equal_opportunity(y_true, y_pred, sensitive)</code> <code>dict</code> Disparate Impact <code>disparate_impact(y_pred, sensitive)</code> <code>dict</code>"},{"location":"api/overview/#speech-metrics","title":"\ud83c\udfa4 Speech Metrics","text":"Metric Function Returns WER <code>word_error_rate(reference, hypothesis)</code> <code>float</code> CER <code>character_error_rate(reference, hypothesis)</code> <code>float</code> MER <code>match_error_rate(reference, hypothesis)</code> <code>float</code> SER <code>sentence_error_rate(references, hypotheses)</code> <code>float</code>"},{"location":"api/overview/#recommender-metrics","title":"\u2b50 Recommender Metrics","text":"Metric Function Returns Precision@K <code>precision_at_k(recommendations, relevant, k)</code> <code>float</code> Recall@K <code>recall_at_k(recommendations, relevant, k)</code> <code>float</code> NDCG@K <code>ndcg_at_k(recommendations, relevant, k)</code> <code>float</code> MAP <code>mean_average_precision(recommendations, relevant)</code> <code>float</code> MRR <code>mean_reciprocal_rank(recommendations, relevant)</code> <code>float</code> Hit Rate <code>hit_rate(recommendations, relevant, k)</code> <code>float</code> Coverage <code>coverage(recommendations, catalog)</code> <code>float</code> Diversity <code>diversity(recommendations)</code> <code>float</code>"},{"location":"api/overview/#utility-classes","title":"\ud83d\udd27 Utility Classes","text":""},{"location":"api/overview/#metric-classes","title":"Metric Classes","text":"<pre><code>from pyeval import (\n    ClassificationMetrics,\n    RegressionMetrics,\n    NLPMetrics,\n    LLMMetrics,\n    RAGMetrics,\n    FairnessMetrics,\n    SpeechMetrics,\n    RecommenderMetrics\n)\n</code></pre>"},{"location":"api/overview/#evaluator","title":"Evaluator","text":"<pre><code>from pyeval import Evaluator\n\nevaluator = Evaluator(\"My Evaluation\")\nreport = evaluator.evaluate_classification(y_true, y_pred)\n</code></pre>"},{"location":"api/overview/#pipeline","title":"Pipeline","text":"<pre><code>from pyeval import Pipeline\n\npipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('f1', f1_score)\n)\nresults = pipeline.run(y_true, y_pred)\n</code></pre>"},{"location":"api/rag/","title":"RAG Metrics API Reference","text":"<p>Metrics for evaluating Retrieval-Augmented Generation pipelines.</p>"},{"location":"api/rag/#retrieval-metrics","title":"Retrieval Metrics","text":""},{"location":"api/rag/#context_relevance","title":"context_relevance","text":"<p>Evaluate relevance of retrieved context to the query.</p> <pre><code>from pyeval import context_relevance\n\nquery = \"What is machine learning?\"\ncontext = \"Machine learning is a subset of AI that enables systems to learn from data.\"\n\nscore = context_relevance(query, context)\n# Returns: 0.85\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>query</code> str User query <code>context</code> str Retrieved context <p>Returns: <code>float</code> - Relevance score between 0 and 1</p>"},{"location":"api/rag/#retrieval_precision","title":"retrieval_precision","text":"<p>Compute precision of retrieved documents.</p> <pre><code>from pyeval import retrieval_precision\n\nretrieved = [1, 3, 5, 7, 9]  # Retrieved document IDs\nrelevant = [1, 5, 10, 15]    # Relevant document IDs\n\nprecision = retrieval_precision(retrieved, relevant)\n# Returns: 2/5 = 0.4\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>retrieved</code> list Retrieved document IDs <code>relevant</code> list Relevant document IDs <p>Returns: <code>float</code> - Precision score</p>"},{"location":"api/rag/#retrieval_recall","title":"retrieval_recall","text":"<p>Compute recall of retrieved documents.</p> <pre><code>from pyeval import retrieval_recall\n\nretrieved = [1, 3, 5, 7, 9]\nrelevant = [1, 5, 10, 15]\n\nrecall = retrieval_recall(retrieved, relevant)\n# Returns: 2/4 = 0.5\n</code></pre> <p>Returns: <code>float</code> - Recall score</p>"},{"location":"api/rag/#retrieval_f1","title":"retrieval_f1","text":"<p>Compute F1 score for retrieval.</p> <pre><code>from pyeval import retrieval_f1\n\nretrieved = [[1, 3, 5], [2, 4, 6]]\nrelevant = [[1, 2], [4, 5]]\n\nf1 = retrieval_f1(retrieved, relevant)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>retrieved</code> list[list] Retrieved docs per query <code>relevant</code> list[list] Relevant docs per query <p>Returns: <code>float</code> - Average F1 score</p>"},{"location":"api/rag/#retrieval_mrr","title":"retrieval_mrr","text":"<p>Compute Mean Reciprocal Rank.</p> <pre><code>from pyeval import retrieval_mrr, mean_reciprocal_rank\n\nretrieved = [[3, 1, 2], [1, 2, 3], [2, 3, 1]]\nrelevant = [[1], [2], [1]]\n\nmrr = retrieval_mrr(retrieved, relevant)\n# Also available as: mean_reciprocal_rank(retrieved, relevant)\n</code></pre> <p>Returns: <code>float</code> - MRR score between 0 and 1</p>"},{"location":"api/rag/#generation-quality-metrics","title":"Generation Quality Metrics","text":""},{"location":"api/rag/#groundedness_score","title":"groundedness_score","text":"<p>Evaluate if response is grounded in the provided context.</p> <pre><code>from pyeval import groundedness_score\n\ncontext = \"Python was created by Guido van Rossum in 1991.\"\nresponse = \"Python is a programming language created by Guido van Rossum.\"\n\nscore = groundedness_score(response, context)\n# Returns: 0.9 (highly grounded)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>response</code> str Generated response <code>context</code> str Source context <p>Returns: <code>float</code> - Groundedness score between 0 and 1</p>"},{"location":"api/rag/#answer_correctness","title":"answer_correctness","text":"<p>Evaluate correctness of answer against ground truth.</p> <pre><code>from pyeval import answer_correctness\n\nresponse = \"Machine learning is a type of artificial intelligence.\"\nground_truth = \"Machine learning is a subset of AI.\"\n\nscore = answer_correctness(response, ground_truth)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>response</code> str Generated answer <code>ground_truth</code> str Expected answer <p>Returns: <code>float</code> - Correctness score between 0 and 1</p>"},{"location":"api/rag/#answer_relevance","title":"answer_relevance","text":"<p>Evaluate relevance of answer to the original query.</p> <pre><code>from pyeval import answer_relevance\n\nquery = \"What programming language should I learn first?\"\nresponse = \"Python is recommended for beginners due to its simple syntax.\"\n\nscore = answer_relevance(query, response)\n</code></pre> <p>Returns: <code>float</code> - Relevance score between 0 and 1</p>"},{"location":"api/rag/#faithfulness_score","title":"faithfulness_score","text":"<p>Evaluate if all claims in response are supported by context.</p> <pre><code>from pyeval import faithfulness_score\n\ncontext = \"The Eiffel Tower is 330 meters tall and located in Paris.\"\nresponse = \"The Eiffel Tower, at 330 meters, stands in Paris, France.\"\n\nresult = faithfulness_score(response, context)\nprint(result)\n# {\n#     'faithfulness_score': 0.95,\n#     'supported_claims': 2,\n#     'total_claims': 2,\n#     'unsupported_claims': []\n# }\n</code></pre> <p>Returns: <code>dict</code> - Faithfulness analysis</p>"},{"location":"api/rag/#entity-attribution-metrics","title":"Entity &amp; Attribution Metrics","text":""},{"location":"api/rag/#context_entity_recall","title":"context_entity_recall","text":"<p>Measure recall of entities from context in the response.</p> <pre><code>from pyeval import context_entity_recall\n\ncontext = \"Albert Einstein developed the theory of relativity in 1905.\"\nresponse = \"Einstein's theory of relativity revolutionized physics.\"\n\nscore = context_entity_recall(context, response)\n# Checks: Einstein \u2713, relativity \u2713, 1905 \u2717\n</code></pre> <p>Returns: <code>float</code> - Entity recall score</p>"},{"location":"api/rag/#answer_attribution","title":"answer_attribution","text":"<p>Evaluate if answer properly attributes information to sources.</p> <pre><code>from pyeval import answer_attribution\n\ncontexts = [\n    {\"id\": 1, \"text\": \"Python was created by Guido van Rossum.\"},\n    {\"id\": 2, \"text\": \"Python's first version was released in 1991.\"}\n]\n\nresponse = \"Python was created by Guido van Rossum in 1991.\"\n\nresult = answer_attribution(response, contexts)\nprint(result)\n# {\n#     'attribution_score': 1.0,\n#     'attributed_to': [1, 2],\n#     'unattributed_claims': []\n# }\n</code></pre> <p>Returns: <code>dict</code> - Attribution analysis</p>"},{"location":"api/rag/#context_utilization","title":"context_utilization","text":"<p>Measure how much of the context was used in generating the response.</p> <pre><code>from pyeval import context_utilization\n\ncontext = \"\"\"\nMachine learning is a subset of artificial intelligence.\nIt uses algorithms to learn from data.\nML can be supervised, unsupervised, or reinforcement learning.\n\"\"\"\n\nresponse = \"Machine learning uses algorithms to learn from data patterns.\"\n\nscore = context_utilization(context, response)\n</code></pre> <p>Returns: <code>float</code> - Utilization score between 0 and 1</p>"},{"location":"api/rag/#pipeline-metrics","title":"Pipeline Metrics","text":""},{"location":"api/rag/#rag_pipeline_score","title":"rag_pipeline_score","text":"<p>Compute overall RAG pipeline quality score.</p> <pre><code>from pyeval import rag_pipeline_score\n\nquery = \"What is Python?\"\ncontext = \"Python is a programming language created by Guido van Rossum.\"\nresponse = \"Python is a popular programming language.\"\nground_truth = \"Python is a general-purpose programming language.\"\n\nresult = rag_pipeline_score(\n    query=query,\n    context=context,\n    response=response,\n    ground_truth=ground_truth\n)\n\nprint(result)\n# {\n#     'overall_score': 0.82,\n#     'retrieval_score': 0.85,\n#     'generation_score': 0.80,\n#     'component_scores': {\n#         'context_relevance': 0.85,\n#         'groundedness': 0.90,\n#         'answer_correctness': 0.75,\n#         'faithfulness': 0.85\n#     }\n# }\n</code></pre> <p>Returns: <code>dict</code> - Comprehensive pipeline evaluation</p>"},{"location":"api/rag/#question_answer_relevance","title":"question_answer_relevance","text":"<p>Evaluate overall QA relevance in RAG context.</p> <pre><code>from pyeval import question_answer_relevance\n\nquery = \"How do neural networks learn?\"\ncontext = \"Neural networks learn by adjusting weights through backpropagation.\"\nresponse = \"Neural networks learn by updating their weights using backpropagation.\"\n\nscore = question_answer_relevance(query, context, response)\n</code></pre> <p>Returns: <code>float</code> - QA relevance score</p>"},{"location":"api/rag/#metric-class","title":"Metric Class","text":""},{"location":"api/rag/#ragmetrics","title":"RAGMetrics","text":"<p>Compute all RAG metrics at once.</p> <pre><code>from pyeval import RAGMetrics\n\nrag = RAGMetrics()\n\nresults = rag.compute(\n    query=\"What is machine learning?\",\n    context=\"Machine learning is a subset of AI...\",\n    response=\"Machine learning enables computers to learn from data.\",\n    ground_truth=\"Machine learning is a type of AI.\"\n)\n\nprint(results)\n# {\n#     'context_relevance': 0.88,\n#     'groundedness': 0.92,\n#     'answer_correctness': 0.85,\n#     'answer_relevance': 0.90,\n#     'faithfulness': 0.88,\n#     'context_utilization': 0.75,\n#     'overall_score': 0.86\n# }\n</code></pre>"},{"location":"api/rag/#complete-rag-metrics-list","title":"Complete RAG Metrics List","text":"Metric Function Description Context Relevance <code>context_relevance</code> Query-context alignment Retrieval Precision <code>retrieval_precision</code> Precision of retrieved docs Retrieval Recall <code>retrieval_recall</code> Recall of retrieved docs Retrieval F1 <code>retrieval_f1</code> F1 score for retrieval MRR <code>retrieval_mrr</code> Mean Reciprocal Rank Groundedness <code>groundedness_score</code> Response-context grounding Answer Correctness <code>answer_correctness</code> Answer accuracy Answer Relevance <code>answer_relevance</code> Answer-query alignment Faithfulness <code>faithfulness_score</code> Claim support verification Entity Recall <code>context_entity_recall</code> Entity coverage Attribution <code>answer_attribution</code> Source attribution Context Utilization <code>context_utilization</code> Context usage efficiency Pipeline Score <code>rag_pipeline_score</code> Overall RAG quality QA Relevance <code>question_answer_relevance</code> End-to-end relevance"},{"location":"api/rag/#usage-tips","title":"Usage Tips","text":""},{"location":"api/rag/#evaluating-multiple-queries","title":"Evaluating Multiple Queries","text":"<pre><code>from pyeval import RAGMetrics\n\nrag = RAGMetrics()\n\ntest_cases = [\n    {\n        \"query\": \"What is Python?\",\n        \"context\": \"Python is a programming language...\",\n        \"response\": \"Python is a popular language...\",\n        \"ground_truth\": \"Python is a general-purpose language...\"\n    },\n    # More test cases...\n]\n\nresults = []\nfor case in test_cases:\n    result = rag.compute(**case)\n    results.append(result)\n\n# Aggregate scores\navg_scores = {\n    metric: sum(r[metric] for r in results) / len(results)\n    for metric in results[0].keys()\n}\n</code></pre>"},{"location":"api/rag/#custom-retrieval-evaluation","title":"Custom Retrieval Evaluation","text":"<pre><code>from pyeval import retrieval_precision, retrieval_recall, retrieval_f1\n\ndef evaluate_retriever(queries, retrieved_docs, relevant_docs):\n    \"\"\"Evaluate retrieval performance across multiple queries.\"\"\"\n\n    metrics = {\n        'precision': [],\n        'recall': [],\n        'f1': []\n    }\n\n    for retrieved, relevant in zip(retrieved_docs, relevant_docs):\n        metrics['precision'].append(retrieval_precision(retrieved, relevant))\n        metrics['recall'].append(retrieval_recall(retrieved, relevant))\n\n    # Average metrics\n    return {\n        'avg_precision': sum(metrics['precision']) / len(metrics['precision']),\n        'avg_recall': sum(metrics['recall']) / len(metrics['recall']),\n        'f1': retrieval_f1(retrieved_docs, relevant_docs)\n    }\n</code></pre>"},{"location":"api/rag/#end-to-end-rag-evaluation","title":"End-to-End RAG Evaluation","text":"<pre><code>from pyeval import (\n    context_relevance, groundedness_score, \n    answer_correctness, faithfulness_score\n)\n\ndef evaluate_rag_response(query, contexts, response, ground_truth):\n    \"\"\"Comprehensive RAG evaluation.\"\"\"\n\n    # Combine contexts\n    combined_context = \" \".join(contexts)\n\n    # Compute metrics\n    relevance = context_relevance(query, combined_context)\n    grounded = groundedness_score(response, combined_context)\n    correct = answer_correctness(response, ground_truth)\n    faithful = faithfulness_score(response, combined_context)\n\n    # Overall score (weighted average)\n    overall = (\n        0.2 * relevance +\n        0.3 * grounded +\n        0.3 * correct +\n        0.2 * faithful['faithfulness_score']\n    )\n\n    return {\n        'context_relevance': relevance,\n        'groundedness': grounded,\n        'answer_correctness': correct,\n        'faithfulness': faithful,\n        'overall_score': overall\n    }\n</code></pre>"},{"location":"api/recommender/","title":"Recommender Metrics API Reference","text":"<p>Metrics for evaluating recommendation systems.</p>"},{"location":"api/recommender/#ranking-metrics","title":"Ranking Metrics","text":""},{"location":"api/recommender/#precision_at_k","title":"precision_at_k","text":"<p>Compute Precision@K.</p> <pre><code>from pyeval import precision_at_k\n\nrecommended = [101, 203, 45, 67, 89, 12, 34]\nrelevant = [45, 89, 78, 123]\n\np5 = precision_at_k(recommended, relevant, k=5)\n# Returns: 2/5 = 0.4 (items 45 and 89 are relevant in top 5)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>recommended</code> list required Ranked list of recommended items <code>relevant</code> list required Set of relevant items <code>k</code> int 10 Number of top items to consider <p>Returns: <code>float</code> - Precision@K score</p>"},{"location":"api/recommender/#recall_at_k","title":"recall_at_k","text":"<p>Compute Recall@K.</p> <pre><code>from pyeval import recall_at_k\n\nrecommended = [101, 203, 45, 67, 89, 12, 34]\nrelevant = [45, 89, 78, 123]\n\nr5 = recall_at_k(recommended, relevant, k=5)\n# Returns: 2/4 = 0.5 (2 out of 4 relevant items found in top 5)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>recommended</code> list required Ranked list of recommended items <code>relevant</code> list required Set of relevant items <code>k</code> int 10 Number of top items to consider <p>Returns: <code>float</code> - Recall@K score</p>"},{"location":"api/recommender/#ndcg_at_k","title":"ndcg_at_k","text":"<p>Compute Normalized Discounted Cumulative Gain at K.</p> <pre><code>from pyeval import ndcg_at_k\n\nrecommended = [101, 203, 45, 67, 89]\nrelevant = [45, 89]\n\n# Binary relevance\nndcg = ndcg_at_k(recommended, relevant, k=5)\n\n# With graded relevance\nrelevance_scores = {101: 0, 203: 0, 45: 3, 67: 0, 89: 2}\nndcg = ndcg_at_k(recommended, relevant, k=5, relevance_scores=relevance_scores)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>recommended</code> list required Ranked list of recommended items <code>relevant</code> list required Set of relevant items <code>k</code> int 10 Number of top items <code>relevance_scores</code> dict None Item \u2192 relevance mapping <p>Returns: <code>float</code> - NDCG@K score between 0 and 1</p>"},{"location":"api/recommender/#mean_average_precision","title":"mean_average_precision","text":"<p>Compute Mean Average Precision (MAP).</p> <pre><code>from pyeval import mean_average_precision\n\n# Multiple users\nrecommended_lists = [\n    [1, 3, 5, 7],\n    [2, 4, 1, 3],\n    [5, 2, 3, 1]\n]\nrelevant_lists = [\n    [1, 5],\n    [1, 2],\n    [1, 3, 5]\n]\n\nmap_score = mean_average_precision(recommended_lists, relevant_lists)\n</code></pre> <p>Returns: <code>float</code> - MAP score between 0 and 1</p>"},{"location":"api/recommender/#mean_reciprocal_rank","title":"mean_reciprocal_rank","text":"<p>Compute Mean Reciprocal Rank (MRR).</p> <pre><code>from pyeval import mean_reciprocal_rank\n\n# Multiple queries\nrecommended_lists = [\n    [3, 1, 2],  # First relevant at position 2\n    [1, 2, 3],  # First relevant at position 1\n    [2, 3, 1]   # First relevant at position 3\n]\nrelevant_lists = [\n    [1],\n    [1],\n    [1]\n]\n\nmrr = mean_reciprocal_rank(recommended_lists, relevant_lists)\n# Returns: (1/2 + 1/1 + 1/3) / 3 = 0.611\n</code></pre> <p>Returns: <code>float</code> - MRR score between 0 and 1</p>"},{"location":"api/recommender/#hit_rate","title":"hit_rate","text":"<p>Compute Hit Rate (HR@K).</p> <pre><code>from pyeval import hit_rate\n\nrecommended = [101, 203, 45, 67, 89]\nrelevant = [45]\n\nhr = hit_rate(recommended, relevant, k=5)\n# Returns: 1.0 (item 45 is in top 5)\n</code></pre> <p>Returns: <code>float</code> - 1.0 if any relevant item in top K, else 0.0</p>"},{"location":"api/recommender/#diversity-metrics","title":"Diversity Metrics","text":""},{"location":"api/recommender/#intra_list_diversity","title":"intra_list_diversity","text":"<p>Compute diversity within a recommendation list.</p> <pre><code>from pyeval import intra_list_diversity\n\n# Item features/embeddings\nitem_features = {\n    1: [0.1, 0.2, 0.3],\n    2: [0.15, 0.25, 0.35],\n    3: [0.9, 0.8, 0.7],\n    4: [0.5, 0.5, 0.5]\n}\n\nrecommended = [1, 2, 3, 4]\n\ndiversity = intra_list_diversity(recommended, item_features)\n# Higher value = more diverse recommendations\n</code></pre> <p>Returns: <code>float</code> - Diversity score</p>"},{"location":"api/recommender/#inter_list_diversity","title":"inter_list_diversity","text":"<p>Compute diversity across different users' recommendations.</p> <pre><code>from pyeval import inter_list_diversity\n\n# Recommendations for multiple users\nuser_recommendations = {\n    'user1': [1, 2, 3],\n    'user2': [1, 2, 4],\n    'user3': [1, 5, 6]\n}\n\ndiversity = inter_list_diversity(user_recommendations)\n</code></pre> <p>Returns: <code>float</code> - Inter-list diversity score</p>"},{"location":"api/recommender/#entropy_diversity","title":"entropy_diversity","text":"<p>Compute recommendation diversity using entropy.</p> <pre><code>from pyeval import entropy_diversity\n\n# Item categories\nitem_categories = {\n    1: 'action',\n    2: 'action',\n    3: 'comedy',\n    4: 'drama',\n    5: 'action'\n}\n\nrecommended = [1, 2, 3, 4, 5]\n\nentropy = entropy_diversity(recommended, item_categories)\n</code></pre> <p>Returns: <code>float</code> - Entropy-based diversity</p>"},{"location":"api/recommender/#gini_index","title":"gini_index","text":"<p>Compute Gini index for recommendation concentration.</p> <pre><code>from pyeval import gini_index\n\n# Recommendation counts per item\nrecommendation_counts = [100, 50, 30, 10, 5, 3, 2]\n\ngini = gini_index(recommendation_counts)\n# Returns: 0 (equal) to 1 (concentrated)\n</code></pre> <p>Returns: <code>float</code> - Gini coefficient</p>"},{"location":"api/recommender/#coverage-metrics","title":"Coverage Metrics","text":""},{"location":"api/recommender/#catalog_coverage","title":"catalog_coverage","text":"<p>Compute what fraction of catalog is recommended.</p> <pre><code>from pyeval import catalog_coverage\n\nall_items = list(range(1000))  # 1000 items in catalog\nrecommended_items = [1, 5, 10, 20, 50, 100, 200]\n\ncoverage = catalog_coverage(recommended_items, all_items)\n# Returns: 7/1000 = 0.007\n</code></pre> <p>Returns: <code>float</code> - Coverage ratio</p>"},{"location":"api/recommender/#user_coverage","title":"user_coverage","text":"<p>Compute what fraction of users receive recommendations.</p> <pre><code>from pyeval import user_coverage\n\nall_users = ['u1', 'u2', 'u3', 'u4', 'u5']\nusers_with_recs = ['u1', 'u2', 'u3']\n\ncoverage = user_coverage(users_with_recs, all_users)\n# Returns: 3/5 = 0.6\n</code></pre> <p>Returns: <code>float</code> - User coverage ratio</p>"},{"location":"api/recommender/#novelty-serendipity","title":"Novelty &amp; Serendipity","text":""},{"location":"api/recommender/#novelty_score","title":"novelty_score","text":"<p>Compute novelty of recommendations.</p> <pre><code>from pyeval import novelty_score\n\nrecommended = [101, 203, 45]\nitem_popularity = {\n    101: 0.8,   # Very popular\n    203: 0.05,  # Rare\n    45: 0.3     # Moderately popular\n}\n\nnovelty = novelty_score(recommended, item_popularity)\n# Higher when recommending less popular items\n</code></pre> <p>Returns: <code>float</code> - Novelty score</p>"},{"location":"api/recommender/#serendipity_score","title":"serendipity_score","text":"<p>Compute serendipity (unexpected but relevant) of recommendations.</p> <pre><code>from pyeval import serendipity_score\n\nrecommended = [1, 2, 3, 4, 5]\nrelevant = [2, 4, 5]\nexpected = [1, 2, 3]  # Items user would expect\n\nserendipity = serendipity_score(recommended, relevant, expected)\n# Measures relevant items that weren't expected\n</code></pre> <p>Returns: <code>float</code> - Serendipity score</p>"},{"location":"api/recommender/#ranking-correlation","title":"Ranking Correlation","text":""},{"location":"api/recommender/#ranking_correlation","title":"ranking_correlation","text":"<p>Compute correlation between two rankings.</p> <pre><code>from pyeval import ranking_correlation\n\nranking1 = [1, 2, 3, 4, 5]\nranking2 = [1, 3, 2, 5, 4]\n\ncorr = ranking_correlation(ranking1, ranking2, method='spearman')\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>ranking1</code> list required First ranking <code>ranking2</code> list required Second ranking <code>method</code> str 'spearman' 'spearman' or 'kendall' <p>Returns: <code>float</code> - Correlation coefficient</p>"},{"location":"api/recommender/#metric-class","title":"Metric Class","text":""},{"location":"api/recommender/#recommendermetrics","title":"RecommenderMetrics","text":"<p>Compute all recommender metrics at once.</p> <pre><code>from pyeval import RecommenderMetrics\n\nrm = RecommenderMetrics()\n\nrecommended = [101, 203, 45, 67, 89]\nrelevant = [45, 89, 123]\n\nresults = rm.compute(recommended, relevant, k=5)\n\nprint(results)\n# {\n#     'precision_at_k': 0.4,\n#     'recall_at_k': 0.67,\n#     'ndcg_at_k': 0.65,\n#     'hit_rate': 1.0,\n#     'mrr': 0.33,\n#     ...\n# }\n</code></pre>"},{"location":"api/recommender/#complete-recommender-metrics-list","title":"Complete Recommender Metrics List","text":"Metric Function Description Precision@K <code>precision_at_k</code> Relevant items in top K Recall@K <code>recall_at_k</code> Coverage of relevant items NDCG@K <code>ndcg_at_k</code> Ranking quality MAP <code>mean_average_precision</code> Average precision MRR <code>mean_reciprocal_rank</code> First relevant rank Hit Rate <code>hit_rate</code> Any relevant in top K ILD <code>intra_list_diversity</code> Within-list diversity Inter-list Diversity <code>inter_list_diversity</code> Cross-user diversity Entropy <code>entropy_diversity</code> Category diversity Gini <code>gini_index</code> Concentration Catalog Coverage <code>catalog_coverage</code> Item space coverage User Coverage <code>user_coverage</code> User space coverage Novelty <code>novelty_score</code> Unexpectedness Serendipity <code>serendipity_score</code> Surprising relevance Ranking Correlation <code>ranking_correlation</code> Ranking agreement"},{"location":"api/recommender/#usage-tips","title":"Usage Tips","text":""},{"location":"api/recommender/#evaluating-at-multiple-k-values","title":"Evaluating at Multiple K Values","text":"<pre><code>from pyeval import precision_at_k, recall_at_k, ndcg_at_k\n\nrecommended = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\nrelevant = [3, 7, 11, 15]\n\nk_values = [1, 3, 5, 10]\n\nfor k in k_values:\n    p = precision_at_k(recommended, relevant, k=k)\n    r = recall_at_k(recommended, relevant, k=k)\n    n = ndcg_at_k(recommended, relevant, k=k)\n    print(f\"K={k}: P@K={p:.3f}, R@K={r:.3f}, NDCG@K={n:.3f}\")\n</code></pre>"},{"location":"api/recommender/#multi-user-evaluation","title":"Multi-User Evaluation","text":"<pre><code>from pyeval import RecommenderMetrics\n\nrm = RecommenderMetrics()\n\n# User \u2192 (recommended, relevant)\nuser_data = {\n    'user1': ([1, 2, 3], [1, 4]),\n    'user2': ([2, 3, 4], [2, 3]),\n    'user3': ([1, 4, 5], [5]),\n}\n\nuser_scores = {}\nfor user, (rec, rel) in user_data.items():\n    user_scores[user] = rm.compute(rec, rel, k=3)\n\n# Average across users\navg_precision = sum(s['precision_at_k'] for s in user_scores.values()) / len(user_scores)\nprint(f\"Average P@3: {avg_precision:.3f}\")\n</code></pre>"},{"location":"api/recommender/#comprehensive-evaluation-report","title":"Comprehensive Evaluation Report","text":"<pre><code>from pyeval import (\n    precision_at_k, recall_at_k, ndcg_at_k,\n    novelty_score, intra_list_diversity, catalog_coverage\n)\n\ndef full_evaluation(recommended_lists, relevant_lists, item_features, catalog):\n    \"\"\"Comprehensive recommendation evaluation.\"\"\"\n\n    # Accuracy metrics\n    precisions = [precision_at_k(r, rel) for r, rel in zip(recommended_lists, relevant_lists)]\n    recalls = [recall_at_k(r, rel) for r, rel in zip(recommended_lists, relevant_lists)]\n    ndcgs = [ndcg_at_k(r, rel) for r, rel in zip(recommended_lists, relevant_lists)]\n\n    # Beyond-accuracy metrics\n    diversities = [intra_list_diversity(r, item_features) for r in recommended_lists]\n\n    # Aggregate\n    all_recommended = set(item for recs in recommended_lists for item in recs)\n    coverage = catalog_coverage(list(all_recommended), catalog)\n\n    return {\n        'avg_precision': sum(precisions) / len(precisions),\n        'avg_recall': sum(recalls) / len(recalls),\n        'avg_ndcg': sum(ndcgs) / len(ndcgs),\n        'avg_diversity': sum(diversities) / len(diversities),\n        'catalog_coverage': coverage\n    }\n</code></pre>"},{"location":"api/speech/","title":"Speech Metrics API Reference","text":"<p>Metrics for evaluating speech recognition and synthesis systems.</p>"},{"location":"api/speech/#error-rate-metrics","title":"Error Rate Metrics","text":""},{"location":"api/speech/#word_error_rate","title":"word_error_rate","text":"<p>Compute Word Error Rate (WER).</p> <pre><code>from pyeval import word_error_rate\n\nreference = \"the quick brown fox jumps over the lazy dog\"\nhypothesis = \"the quick brown fox jumps over lazy dog\"\n\nresult = word_error_rate(reference, hypothesis)\nprint(result)\n# {\n#     'wer': 0.111,\n#     'substitutions': 0,\n#     'insertions': 0,\n#     'deletions': 1,\n#     'reference_length': 9\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>reference</code> str Reference transcription <code>hypothesis</code> str Hypothesis transcription <p>Returns: <code>dict</code> - WER and error breakdown</p> <p>Formula: WER = (S + I + D) / N</p> <ul> <li>S = Substitutions</li> <li>I = Insertions</li> <li>D = Deletions</li> <li>N = Reference word count</li> </ul>"},{"location":"api/speech/#character_error_rate","title":"character_error_rate","text":"<p>Compute Character Error Rate (CER).</p> <pre><code>from pyeval import character_error_rate\n\nreference = \"hello world\"\nhypothesis = \"helo word\"\n\nresult = character_error_rate(reference, hypothesis)\nprint(result)\n# {\n#     'cer': 0.182,\n#     'substitutions': 0,\n#     'insertions': 0,\n#     'deletions': 2,\n#     'reference_length': 11\n# }\n</code></pre> <p>Returns: <code>dict</code> - CER and character-level error breakdown</p>"},{"location":"api/speech/#match_error_rate","title":"match_error_rate","text":"<p>Compute Match Error Rate (MER).</p> <pre><code>from pyeval import match_error_rate\n\nreference = \"the quick brown fox\"\nhypothesis = \"the fast brown fox\"\n\nresult = match_error_rate(reference, hypothesis)\nprint(result['mer'])  # 0.25\n</code></pre> <p>Returns: <code>dict</code> - MER analysis</p>"},{"location":"api/speech/#word_information_lost","title":"word_information_lost","text":"<p>Compute Word Information Lost (WIL).</p> <pre><code>from pyeval import word_information_lost\n\nreference = \"the quick brown fox\"\nhypothesis = \"the fast brown fox\"\n\nresult = word_information_lost(reference, hypothesis)\nprint(result['wil'])\n</code></pre> <p>Returns: <code>dict</code> - WIL score</p>"},{"location":"api/speech/#semantic-understanding-metrics","title":"Semantic Understanding Metrics","text":""},{"location":"api/speech/#slot_error_rate","title":"slot_error_rate","text":"<p>Compute Slot Error Rate for NLU evaluation.</p> <pre><code>from pyeval import slot_error_rate\n\nreference_slots = {'location': 'new york', 'date': 'tomorrow'}\nhypothesis_slots = {'location': 'new york', 'date': 'today'}\n\nresult = slot_error_rate(reference_slots, hypothesis_slots)\nprint(result)\n# {\n#     'ser': 0.5,\n#     'correct_slots': 1,\n#     'total_slots': 2,\n#     'incorrect_slots': ['date']\n# }\n</code></pre> <p>Returns: <code>dict</code> - Slot error analysis</p>"},{"location":"api/speech/#intent_accuracy","title":"intent_accuracy","text":"<p>Compute intent classification accuracy.</p> <pre><code>from pyeval import intent_accuracy\n\nreference_intents = ['book_flight', 'weather', 'book_hotel']\nhypothesis_intents = ['book_flight', 'weather', 'book_flight']\n\nresult = intent_accuracy(reference_intents, hypothesis_intents)\nprint(result)\n# {\n#     'accuracy': 0.667,\n#     'correct': 2,\n#     'total': 3,\n#     'confusion': {'book_hotel': 'book_flight'}\n# }\n</code></pre> <p>Returns: <code>dict</code> - Intent accuracy analysis</p>"},{"location":"api/speech/#phonetic-metrics","title":"Phonetic Metrics","text":""},{"location":"api/speech/#phoneme_error_rate","title":"phoneme_error_rate","text":"<p>Compute Phoneme Error Rate (PER).</p> <pre><code>from pyeval import phoneme_error_rate\n\nreference_phonemes = ['HH', 'AH', 'L', 'OW']  # \"hello\"\nhypothesis_phonemes = ['HH', 'EH', 'L', 'OW']\n\nresult = phoneme_error_rate(reference_phonemes, hypothesis_phonemes)\nprint(result['per'])  # 0.25\n</code></pre> <p>Returns: <code>dict</code> - PER analysis</p>"},{"location":"api/speech/#speaker-metrics","title":"Speaker Metrics","text":""},{"location":"api/speech/#diarization_error_rate","title":"diarization_error_rate","text":"<p>Compute Diarization Error Rate (DER).</p> <pre><code>from pyeval import diarization_error_rate\n\n# Segments: [(start, end, speaker)]\nreference_segments = [\n    (0.0, 2.0, 'A'),\n    (2.0, 4.0, 'B'),\n    (4.0, 6.0, 'A')\n]\n\nhypothesis_segments = [\n    (0.0, 1.5, 'A'),\n    (1.5, 4.0, 'B'),\n    (4.0, 6.0, 'A')\n]\n\nresult = diarization_error_rate(reference_segments, hypothesis_segments)\nprint(result)\n# {\n#     'der': 0.083,\n#     'miss_rate': 0.0,\n#     'false_alarm': 0.0,\n#     'confusion': 0.083\n# }\n</code></pre> <p>Returns: <code>dict</code> - DER breakdown</p>"},{"location":"api/speech/#keyword-wake-word-metrics","title":"Keyword &amp; Wake Word Metrics","text":""},{"location":"api/speech/#keyword_spotting_metrics","title":"keyword_spotting_metrics","text":"<p>Evaluate keyword spotting performance.</p> <pre><code>from pyeval import keyword_spotting_metrics\n\n# True positives, false positives, false negatives\ndetections = {\n    'true_positives': 45,\n    'false_positives': 5,\n    'false_negatives': 10\n}\n\nresult = keyword_spotting_metrics(\n    detections['true_positives'],\n    detections['false_positives'],\n    detections['false_negatives']\n)\nprint(result)\n# {\n#     'precision': 0.90,\n#     'recall': 0.818,\n#     'f1': 0.857,\n#     'false_rejection_rate': 0.182,\n#     'false_acceptance_rate': 0.10\n# }\n</code></pre> <p>Returns: <code>dict</code> - Keyword spotting metrics</p>"},{"location":"api/speech/#speech-quality-metrics","title":"Speech Quality Metrics","text":""},{"location":"api/speech/#mean_opinion_score","title":"mean_opinion_score","text":"<p>Estimate Mean Opinion Score (MOS) for speech quality.</p> <pre><code>from pyeval import mean_opinion_score\n\n# Quality indicators\nquality_features = {\n    'signal_to_noise_ratio': 25.0,  # dB\n    'clarity': 0.85,\n    'naturalness': 0.90\n}\n\nresult = mean_opinion_score(quality_features)\nprint(result)\n# {\n#     'mos': 4.2,\n#     'quality_category': 'good',\n#     'components': {\n#         'snr_score': 4.5,\n#         'clarity_score': 4.25,\n#         'naturalness_score': 4.5\n#     }\n# }\n</code></pre> <p>Returns: <code>dict</code> - MOS estimation (1-5 scale)</p>"},{"location":"api/speech/#fluency_score","title":"fluency_score","text":"<p>Evaluate speech fluency.</p> <pre><code>from pyeval import fluency_score\n\n# Fluency indicators\nfluency_data = {\n    'words_per_minute': 120,\n    'pause_ratio': 0.15,\n    'filler_words': ['um', 'uh'],\n    'filler_count': 3,\n    'total_words': 100\n}\n\nresult = fluency_score(fluency_data)\nprint(result)\n# {\n#     'fluency_score': 0.82,\n#     'speaking_rate': 'normal',\n#     'filler_ratio': 0.03,\n#     'pause_assessment': 'appropriate'\n# }\n</code></pre> <p>Returns: <code>dict</code> - Fluency analysis</p>"},{"location":"api/speech/#signal_to_noise_ratio","title":"signal_to_noise_ratio","text":"<p>Compute Signal-to-Noise Ratio.</p> <pre><code>from pyeval import signal_to_noise_ratio\n\n# Audio signal and noise samples\nsignal = [0.5, 0.8, 0.3, 0.9, 0.4]\nnoise = [0.01, 0.02, 0.01, 0.03, 0.01]\n\nsnr = signal_to_noise_ratio(signal, noise)\nprint(f\"SNR: {snr:.2f} dB\")\n</code></pre> <p>Returns: <code>float</code> - SNR in decibels</p>"},{"location":"api/speech/#real-time-metrics","title":"Real-time Metrics","text":""},{"location":"api/speech/#real_time_factor","title":"real_time_factor","text":"<p>Compute Real-Time Factor (RTF).</p> <pre><code>from pyeval import real_time_factor\n\naudio_duration = 10.0  # seconds\nprocessing_time = 2.5  # seconds\n\nrtf = real_time_factor(audio_duration, processing_time)\nprint(f\"RTF: {rtf:.2f}\")  # 0.25 (faster than real-time)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>audio_duration</code> float Duration of audio in seconds <code>processing_time</code> float Time to process in seconds <p>Returns: <code>float</code> - RTF (&lt; 1 is faster than real-time)</p>"},{"location":"api/speech/#latency_metrics","title":"latency_metrics","text":"<p>Compute latency metrics for streaming ASR.</p> <pre><code>from pyeval import latency_metrics\n\n# Timestamps for each word\nreference_times = [0.5, 1.0, 1.5, 2.0]\nhypothesis_times = [0.7, 1.2, 1.6, 2.2]\n\nresult = latency_metrics(reference_times, hypothesis_times)\nprint(result)\n# {\n#     'mean_latency': 0.175,\n#     'max_latency': 0.2,\n#     'min_latency': 0.1,\n#     'p90_latency': 0.2\n# }\n</code></pre> <p>Returns: <code>dict</code> - Latency statistics</p>"},{"location":"api/speech/#metric-class","title":"Metric Class","text":""},{"location":"api/speech/#speechmetrics","title":"SpeechMetrics","text":"<p>Compute all speech metrics at once.</p> <pre><code>from pyeval import SpeechMetrics\n\nsm = SpeechMetrics()\n\nresult = sm.compute(\n    reference=\"the quick brown fox\",\n    hypothesis=\"the fast brown fox\"\n)\n\nprint(result)\n# {\n#     'wer': 0.25,\n#     'cer': 0.16,\n#     'mer': 0.25,\n#     'substitutions': 1,\n#     'insertions': 0,\n#     'deletions': 0,\n#     ...\n# }\n</code></pre>"},{"location":"api/speech/#complete-speech-metrics-list","title":"Complete Speech Metrics List","text":"Metric Function Description WER <code>word_error_rate</code> Word-level error rate CER <code>character_error_rate</code> Character-level error rate MER <code>match_error_rate</code> Match error rate WIL <code>word_information_lost</code> Information loss measure SER <code>slot_error_rate</code> NLU slot errors Intent Accuracy <code>intent_accuracy</code> Intent classification PER <code>phoneme_error_rate</code> Phoneme-level errors DER <code>diarization_error_rate</code> Speaker diarization errors KWS Metrics <code>keyword_spotting_metrics</code> Keyword detection MOS <code>mean_opinion_score</code> Perceptual quality Fluency <code>fluency_score</code> Speaking fluency SNR <code>signal_to_noise_ratio</code> Signal quality RTF <code>real_time_factor</code> Processing speed Latency <code>latency_metrics</code> Streaming latency"},{"location":"api/speech/#usage-tips","title":"Usage Tips","text":""},{"location":"api/speech/#batch-evaluation","title":"Batch Evaluation","text":"<pre><code>from pyeval import word_error_rate\n\ntest_cases = [\n    (\"hello world\", \"hello world\"),\n    (\"good morning\", \"good moring\"),\n    (\"how are you\", \"how are yo\")\n]\n\ntotal_wer = 0\nfor ref, hyp in test_cases:\n    result = word_error_rate(ref, hyp)\n    total_wer += result['wer']\n\navg_wer = total_wer / len(test_cases)\nprint(f\"Average WER: {avg_wer:.3f}\")\n</code></pre>"},{"location":"api/speech/#detailed-error-analysis","title":"Detailed Error Analysis","text":"<pre><code>from pyeval import word_error_rate, character_error_rate\n\ndef detailed_analysis(reference, hypothesis):\n    wer_result = word_error_rate(reference, hypothesis)\n    cer_result = character_error_rate(reference, hypothesis)\n\n    print(f\"Reference: {reference}\")\n    print(f\"Hypothesis: {hypothesis}\")\n    print(f\"WER: {wer_result['wer']:.3f}\")\n    print(f\"  - Substitutions: {wer_result['substitutions']}\")\n    print(f\"  - Insertions: {wer_result['insertions']}\")\n    print(f\"  - Deletions: {wer_result['deletions']}\")\n    print(f\"CER: {cer_result['cer']:.3f}\")\n</code></pre>"},{"location":"api/speech/#comparing-asr-systems","title":"Comparing ASR Systems","text":"<pre><code>from pyeval import SpeechMetrics\n\nsm = SpeechMetrics()\n\nsystems = {\n    'system_a': hypotheses_a,\n    'system_b': hypotheses_b\n}\n\nresults = {}\nfor name, hypotheses in systems.items():\n    wers = []\n    for ref, hyp in zip(references, hypotheses):\n        result = sm.compute(ref, hyp)\n        wers.append(result['wer'])\n    results[name] = sum(wers) / len(wers)\n\nbest_system = min(results.items(), key=lambda x: x[1])\nprint(f\"Best system: {best_system[0]} with WER: {best_system[1]:.3f}\")\n</code></pre>"},{"location":"api/statistical/","title":"Statistical Utilities API Reference","text":"<p>Statistical testing and analysis utilities for model comparison.</p>"},{"location":"api/statistical/#confidence-intervals","title":"Confidence Intervals","text":""},{"location":"api/statistical/#bootstrap_confidence_interval","title":"bootstrap_confidence_interval","text":"<p>Compute bootstrap confidence interval for any statistic.</p> <pre><code>from pyeval import bootstrap_confidence_interval\n\ndata = [0.85, 0.87, 0.86, 0.88, 0.84, 0.89, 0.87, 0.86]\n\n# Mean with 95% CI\nresult = bootstrap_confidence_interval(data, statistic='mean', confidence=0.95)\nprint(result)\n# {\n#     'point_estimate': 0.865,\n#     'ci_lower': 0.852,\n#     'ci_upper': 0.878,\n#     'confidence': 0.95,\n#     'n_bootstrap': 1000\n# }\n\n# Median with 99% CI\nresult = bootstrap_confidence_interval(data, statistic='median', confidence=0.99)\n\n# Custom statistic\nresult = bootstrap_confidence_interval(\n    data, \n    statistic=lambda x: max(x) - min(x),  # Range\n    confidence=0.95\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>data</code> list required Sample data <code>statistic</code> str/callable 'mean' Statistic to compute <code>confidence</code> float 0.95 Confidence level <code>n_bootstrap</code> int 1000 Number of bootstrap samples <p>Returns: <code>dict</code> - Point estimate and confidence interval</p>"},{"location":"api/statistical/#hypothesis-tests","title":"Hypothesis Tests","text":""},{"location":"api/statistical/#paired_t_test","title":"paired_t_test","text":"<p>Perform paired t-test for comparing two models on same data.</p> <pre><code>from pyeval import paired_t_test\n\nmodel1_scores = [0.85, 0.87, 0.86, 0.88, 0.84]\nmodel2_scores = [0.88, 0.89, 0.87, 0.90, 0.86]\n\nresult = paired_t_test(model1_scores, model2_scores)\nprint(result)\n# {\n#     't_statistic': -3.24,\n#     'p_value': 0.032,\n#     'degrees_freedom': 4,\n#     'is_significant': True,  # p &lt; 0.05\n#     'mean_difference': -0.024\n# }\n</code></pre> <p>Returns: <code>dict</code> - Test results with significance</p>"},{"location":"api/statistical/#independent_t_test","title":"independent_t_test","text":"<p>Perform independent samples t-test.</p> <pre><code>from pyeval import independent_t_test\n\ngroup1 = [0.85, 0.87, 0.86, 0.88, 0.84]\ngroup2 = [0.92, 0.93, 0.91, 0.94, 0.90]\n\nresult = independent_t_test(group1, group2)\nprint(result)\n# {\n#     't_statistic': -8.45,\n#     'p_value': 0.0001,\n#     'degrees_freedom': 8,\n#     'is_significant': True\n# }\n</code></pre> <p>Returns: <code>dict</code> - Test results</p>"},{"location":"api/statistical/#wilcoxon_signed_rank","title":"wilcoxon_signed_rank","text":"<p>Perform Wilcoxon signed-rank test (non-parametric paired test).</p> <pre><code>from pyeval import wilcoxon_signed_rank\n\nmodel1_scores = [0.85, 0.87, 0.86, 0.88, 0.84]\nmodel2_scores = [0.88, 0.89, 0.87, 0.90, 0.86]\n\nresult = wilcoxon_signed_rank(model1_scores, model2_scores)\nprint(result)\n# {\n#     'statistic': 0.0,\n#     'p_value': 0.063,\n#     'is_significant': False\n# }\n</code></pre> <p>Returns: <code>dict</code> - Test results</p>"},{"location":"api/statistical/#mann_whitney_u","title":"mann_whitney_u","text":"<p>Perform Mann-Whitney U test (non-parametric independent test).</p> <pre><code>from pyeval import mann_whitney_u\n\ngroup1 = [0.85, 0.87, 0.86, 0.88, 0.84]\ngroup2 = [0.92, 0.93, 0.91, 0.94, 0.90]\n\nresult = mann_whitney_u(group1, group2)\nprint(result)\n# {\n#     'u_statistic': 0.0,\n#     'p_value': 0.008,\n#     'is_significant': True\n# }\n</code></pre> <p>Returns: <code>dict</code> - Test results</p>"},{"location":"api/statistical/#mcnemar_test","title":"mcnemar_test","text":"<p>Perform McNemar's test for comparing classifiers.</p> <pre><code>from pyeval import mcnemar_test\n\n# Contingency table:\n# [[both correct, only model1 correct],\n#  [only model2 correct, both wrong]]\ncontingency = [[45, 15], [5, 35]]\n\nresult = mcnemar_test(contingency)\nprint(result)\n# {\n#     'chi_square': 5.0,\n#     'p_value': 0.025,\n#     'is_significant': True,\n#     'odds_ratio': 3.0\n# }\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>contingency_table</code> list[list] 2x2 contingency table <p>Returns: <code>dict</code> - Test results</p>"},{"location":"api/statistical/#permutation_test","title":"permutation_test","text":"<p>Perform permutation test for comparing two groups.</p> <pre><code>from pyeval import permutation_test\n\ngroup1 = [0.85, 0.87, 0.86, 0.88, 0.84]\ngroup2 = [0.88, 0.89, 0.87, 0.90, 0.86]\n\nresult = permutation_test(group1, group2, n_permutations=10000)\nprint(result)\n# {\n#     'observed_difference': -0.024,\n#     'p_value': 0.035,\n#     'is_significant': True\n# }\n</code></pre> <p>Returns: <code>dict</code> - Permutation test results</p>"},{"location":"api/statistical/#effect-size-measures","title":"Effect Size Measures","text":""},{"location":"api/statistical/#cohens_d","title":"cohens_d","text":"<p>Compute Cohen's d effect size.</p> <pre><code>from pyeval import cohens_d\n\ngroup1 = [0.85, 0.87, 0.86, 0.88, 0.84]\ngroup2 = [0.88, 0.89, 0.87, 0.90, 0.86]\n\nd = cohens_d(group1, group2)\nprint(f\"Cohen's d: {d:.3f}\")\n# Interpretation: 0.2=small, 0.5=medium, 0.8=large\n</code></pre> <p>Returns: <code>float</code> - Cohen's d value</p>"},{"location":"api/statistical/#hedges_g","title":"hedges_g","text":"<p>Compute Hedges' g effect size (bias-corrected Cohen's d).</p> <pre><code>from pyeval import hedges_g\n\ng = hedges_g(group1, group2)\nprint(f\"Hedges' g: {g:.3f}\")\n</code></pre> <p>Returns: <code>float</code> - Hedges' g value</p>"},{"location":"api/statistical/#glass_delta","title":"glass_delta","text":"<p>Compute Glass's delta effect size.</p> <pre><code>from pyeval import glass_delta\n\ndelta = glass_delta(group1, group2)  # Uses group2's std as denominator\n</code></pre> <p>Returns: <code>float</code> - Glass's delta value</p>"},{"location":"api/statistical/#correlation-measures","title":"Correlation Measures","text":""},{"location":"api/statistical/#correlation_coefficient","title":"correlation_coefficient","text":"<p>Compute correlation between two variables.</p> <pre><code>from pyeval import correlation_coefficient\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 5, 4, 5]\n\n# Pearson correlation\nr = correlation_coefficient(x, y, method='pearson')\nprint(f\"Pearson r: {r:.3f}\")\n\n# Spearman correlation\nrho = correlation_coefficient(x, y, method='spearman')\nprint(f\"Spearman \u03c1: {rho:.3f}\")\n\n# Kendall correlation\ntau = correlation_coefficient(x, y, method='kendall')\nprint(f\"Kendall \u03c4: {tau:.3f}\")\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>x</code> list required First variable <code>y</code> list required Second variable <code>method</code> str 'pearson' 'pearson', 'spearman', or 'kendall' <p>Returns: <code>float</code> - Correlation coefficient</p>"},{"location":"api/statistical/#descriptive-statistics","title":"Descriptive Statistics","text":""},{"location":"api/statistical/#descriptive_stats","title":"descriptive_stats","text":"<p>Compute comprehensive descriptive statistics.</p> <pre><code>from pyeval import descriptive_stats\n\ndata = [0.85, 0.87, 0.86, 0.88, 0.84, 0.89, 0.87, 0.86]\n\nstats = descriptive_stats(data)\nprint(stats)\n# {\n#     'mean': 0.865,\n#     'median': 0.865,\n#     'std': 0.016,\n#     'variance': 0.00026,\n#     'min': 0.84,\n#     'max': 0.89,\n#     'range': 0.05,\n#     'q1': 0.855,\n#     'q3': 0.875,\n#     'iqr': 0.02,\n#     'skewness': 0.0,\n#     'kurtosis': -1.2,\n#     'n': 8\n# }\n</code></pre> <p>Returns: <code>dict</code> - Comprehensive statistics</p>"},{"location":"api/statistical/#percentile","title":"percentile","text":"<p>Compute percentile value.</p> <pre><code>from pyeval import percentile\n\ndata = [0.85, 0.87, 0.86, 0.88, 0.84, 0.89, 0.87, 0.86]\n\np50 = percentile(data, 50)  # Median\np95 = percentile(data, 95)  # 95th percentile\n</code></pre> <p>Returns: <code>float</code> - Percentile value</p>"},{"location":"api/statistical/#multiple-comparison-corrections","title":"Multiple Comparison Corrections","text":""},{"location":"api/statistical/#bonferroni_correction","title":"bonferroni_correction","text":"<p>Apply Bonferroni correction to p-values.</p> <pre><code>from pyeval import bonferroni_correction\n\np_values = [0.01, 0.03, 0.04, 0.02]\n\ncorrected = bonferroni_correction(p_values)\nprint(corrected)\n# {\n#     'corrected_p_values': [0.04, 0.12, 0.16, 0.08],\n#     'significant': [True, False, False, False],  # at \u03b1=0.05\n#     'alpha': 0.0125  # 0.05/4\n# }\n</code></pre> <p>Returns: <code>dict</code> - Corrected p-values</p>"},{"location":"api/statistical/#holm_bonferroni","title":"holm_bonferroni","text":"<p>Apply Holm-Bonferroni correction (less conservative).</p> <pre><code>from pyeval import holm_bonferroni\n\np_values = [0.01, 0.03, 0.04, 0.02]\n\ncorrected = holm_bonferroni(p_values)\n</code></pre> <p>Returns: <code>dict</code> - Corrected p-values with significance</p>"},{"location":"api/statistical/#complete-statistical-functions-list","title":"Complete Statistical Functions List","text":"Function Description <code>bootstrap_confidence_interval</code> Bootstrap CI for any statistic <code>paired_t_test</code> Paired samples t-test <code>independent_t_test</code> Independent samples t-test <code>wilcoxon_signed_rank</code> Non-parametric paired test <code>mann_whitney_u</code> Non-parametric independent test <code>mcnemar_test</code> Classifier comparison test <code>permutation_test</code> Permutation-based test <code>cohens_d</code> Cohen's d effect size <code>hedges_g</code> Hedges' g effect size <code>glass_delta</code> Glass's delta effect size <code>correlation_coefficient</code> Pearson/Spearman/Kendall correlation <code>descriptive_stats</code> Comprehensive descriptive statistics <code>percentile</code> Percentile computation <code>bonferroni_correction</code> Bonferroni multiple comparison <code>holm_bonferroni</code> Holm-Bonferroni correction"},{"location":"api/statistical/#usage-tips","title":"Usage Tips","text":""},{"location":"api/statistical/#choosing-the-right-test","title":"Choosing the Right Test","text":"Scenario Recommended Test Comparing 2 models, same data Paired t-test or Wilcoxon Comparing 2 models, different data Independent t-test or Mann-Whitney Non-normal distributions Wilcoxon or Mann-Whitney Comparing classifiers McNemar's test Small sample, no assumptions Permutation test Multiple comparisons Apply Bonferroni/Holm correction"},{"location":"api/statistical/#comprehensive-model-comparison","title":"Comprehensive Model Comparison","text":"<pre><code>from pyeval import (\n    paired_t_test, wilcoxon_signed_rank, cohens_d,\n    bootstrap_confidence_interval\n)\n\ndef compare_models(model1_scores, model2_scores, name1=\"Model1\", name2=\"Model2\"):\n    \"\"\"Comprehensive comparison of two models.\"\"\"\n\n    # Statistical tests\n    t_test = paired_t_test(model1_scores, model2_scores)\n    wilcoxon = wilcoxon_signed_rank(model1_scores, model2_scores)\n\n    # Effect size\n    effect = cohens_d(model1_scores, model2_scores)\n\n    # Confidence intervals\n    ci1 = bootstrap_confidence_interval(model1_scores)\n    ci2 = bootstrap_confidence_interval(model2_scores)\n\n    # Report\n    print(f\"=== {name1} vs {name2} ===\")\n    print(f\"{name1}: {ci1['point_estimate']:.4f} [{ci1['ci_lower']:.4f}, {ci1['ci_upper']:.4f}]\")\n    print(f\"{name2}: {ci2['point_estimate']:.4f} [{ci2['ci_lower']:.4f}, {ci2['ci_upper']:.4f}]\")\n    print(f\"Paired t-test: t={t_test['t_statistic']:.3f}, p={t_test['p_value']:.4f}\")\n    print(f\"Wilcoxon: p={wilcoxon['p_value']:.4f}\")\n    print(f\"Cohen's d: {effect:.3f}\")\n\n    if t_test['is_significant']:\n        better = name2 if ci2['point_estimate'] &gt; ci1['point_estimate'] else name1\n        print(f\"Conclusion: {better} is significantly better\")\n    else:\n        print(\"Conclusion: No significant difference\")\n</code></pre>"},{"location":"api/visualization/","title":"Visualization Utilities API Reference","text":"<p>ASCII-based visualization utilities for terminal-friendly output.</p>"},{"location":"api/visualization/#confusion-matrix","title":"Confusion Matrix","text":""},{"location":"api/visualization/#confusion_matrix_display","title":"confusion_matrix_display","text":"<p>Display a formatted confusion matrix.</p> <pre><code>from pyeval import confusion_matrix_display\n\nmatrix = [[45, 5], [10, 40]]\nlabels = ['Negative', 'Positive']\n\noutput = confusion_matrix_display(matrix, labels=labels)\nprint(output)\n</code></pre> <p>Output: <pre><code>                    Predicted\n                    Negative  Positive\nActual  Negative         45         5\n        Positive         10        40\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>matrix</code> list[list] required Confusion matrix <code>labels</code> list None Class labels <code>normalize</code> bool False Normalize values <code>show_percentages</code> bool False Show as percentages <p>Returns: <code>str</code> - Formatted confusion matrix</p>"},{"location":"api/visualization/#confusion_matrix_display-multiclass","title":"confusion_matrix_display (multiclass)","text":"<pre><code>from pyeval import confusion_matrix_display\n\nmatrix = [\n    [40, 3, 2],\n    [5, 38, 2],\n    [1, 4, 35]\n]\nlabels = ['Cat', 'Dog', 'Bird']\n\nprint(confusion_matrix_display(matrix, labels=labels))\n</code></pre> <p>Output: <pre><code>                Predicted\n            Cat    Dog   Bird\nActual Cat   40      3      2\n       Dog    5     38      2\n       Bird   1      4     35\n</code></pre></p>"},{"location":"api/visualization/#classification-report","title":"Classification Report","text":""},{"location":"api/visualization/#classification_report_display","title":"classification_report_display","text":"<p>Display a formatted classification report.</p> <pre><code>from pyeval import classification_report_display\n\ny_true = [0, 0, 1, 1, 2, 2, 0, 1, 2]\ny_pred = [0, 0, 1, 2, 2, 2, 0, 1, 1]\n\nreport = classification_report_display(y_true, y_pred)\nprint(report)\n</code></pre> <p>Output: <pre><code>              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       0.67      0.67      0.67         3\n           2       0.67      0.67      0.67         3\n\n    accuracy                           0.78         9\n   macro avg       0.78      0.78      0.78         9\nweighted avg       0.78      0.78      0.78         9\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>y_true</code> list required Ground truth labels <code>y_pred</code> list required Predicted labels <code>labels</code> list None Class labels <code>digits</code> int 2 Decimal places <p>Returns: <code>str</code> - Formatted classification report</p>"},{"location":"api/visualization/#charts","title":"Charts","text":""},{"location":"api/visualization/#horizontal_bar_chart","title":"horizontal_bar_chart","text":"<p>Create an ASCII horizontal bar chart.</p> <pre><code>from pyeval import horizontal_bar_chart\n\nmetrics = {\n    'Precision': 0.89,\n    'Recall': 0.85,\n    'F1 Score': 0.87,\n    'Accuracy': 0.88\n}\n\nchart = horizontal_bar_chart(metrics, title=\"Model Performance\", width=50)\nprint(chart)\n</code></pre> <p>Output: <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 Model Performance \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nPrecision \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591 0.890\nRecall    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 0.850\nF1 Score  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591 0.870\nAccuracy  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591 0.880\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>data</code> dict required Label \u2192 value mapping <code>title</code> str None Chart title <code>width</code> int 50 Bar width in characters <code>fill_char</code> str '\u2588' Character for filled portion <code>empty_char</code> str '\u2591' Character for empty portion <code>show_values</code> bool True Display numeric values <p>Returns: <code>str</code> - ASCII bar chart</p>"},{"location":"api/visualization/#vertical_bar_chart","title":"vertical_bar_chart","text":"<p>Create an ASCII vertical bar chart.</p> <pre><code>from pyeval import vertical_bar_chart\n\ndata = {'A': 45, 'B': 30, 'C': 55, 'D': 20}\n\nchart = vertical_bar_chart(data, height=10)\nprint(chart)\n</code></pre> <p>Output: <pre><code>     C\n    \u2588\u2588\n    \u2588\u2588 A\n    \u2588\u2588 \u2588\u2588\n    \u2588\u2588 \u2588\u2588\n    \u2588\u2588 \u2588\u2588 B\n    \u2588\u2588 \u2588\u2588 \u2588\u2588\n    \u2588\u2588 \u2588\u2588 \u2588\u2588\n    \u2588\u2588 \u2588\u2588 \u2588\u2588 D\n    \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\n A  B  C  D\n</code></pre></p> <p>Returns: <code>str</code> - Vertical bar chart</p>"},{"location":"api/visualization/#histogram_display","title":"histogram_display","text":"<p>Create an ASCII histogram.</p> <pre><code>from pyeval import histogram_display\n\ndata = [0.2, 0.3, 0.3, 0.4, 0.4, 0.4, 0.5, 0.5, 0.6, 0.7]\n\nhist = histogram_display(data, bins=5, width=40)\nprint(hist)\n</code></pre> <p>Output: <pre><code>[0.20-0.30) | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2\n[0.30-0.40) | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           | 3\n[0.40-0.50) | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       | 4\n[0.50-0.60) | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 2\n[0.60-0.70] | \u2588\u2588\u2588\u2588                   | 1\n</code></pre></p> <p>Returns: <code>str</code> - ASCII histogram</p>"},{"location":"api/visualization/#sparklines","title":"Sparklines","text":""},{"location":"api/visualization/#sparkline","title":"sparkline","text":"<p>Create a compact sparkline visualization.</p> <pre><code>from pyeval import sparkline\n\n# Training loss over epochs\nlosses = [0.9, 0.7, 0.5, 0.4, 0.35, 0.3, 0.28, 0.26]\n\nline = sparkline(losses)\nprint(f\"Training loss: {line}\")\n# Output: Training loss: \u2587\u2585\u2583\u2582\u2582\u2582\u2581\u2581\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>data</code> list required Numeric values <code>min_val</code> float None Minimum for scaling <code>max_val</code> float None Maximum for scaling <p>Returns: <code>str</code> - Sparkline characters (\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588)</p>"},{"location":"api/visualization/#sparkline_with_stats","title":"sparkline_with_stats","text":"<p>Create sparkline with summary statistics.</p> <pre><code>from pyeval import sparkline_with_stats\n\naccuracy_scores = [0.82, 0.85, 0.84, 0.88, 0.87, 0.89, 0.91, 0.90]\n\nresult = sparkline_with_stats(accuracy_scores)\nprint(result)\n# Output: \u2581\u2583\u2582\u2585\u2584\u2586\u2588\u2587 (min: 0.82, max: 0.91, avg: 0.87)\n</code></pre> <p>Returns: <code>str</code> - Sparkline with statistics</p>"},{"location":"api/visualization/#progress-indicators","title":"Progress Indicators","text":""},{"location":"api/visualization/#progress_bar","title":"progress_bar","text":"<p>Create an ASCII progress bar.</p> <pre><code>from pyeval import progress_bar\n\n# Basic progress bar\nbar = progress_bar(75, 100)\nprint(bar)\n# Output: [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 75%\n\n# With prefix\nbar = progress_bar(75, 100, prefix=\"Evaluation\")\nprint(bar)\n# Output: Evaluation [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 75%\n\n# Custom width\nbar = progress_bar(3, 10, width=20, prefix=\"Step\")\nprint(bar)\n# Output: Step [\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 30%\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>current</code> int required Current progress <code>total</code> int required Total items <code>width</code> int 30 Bar width <code>prefix</code> str '' Prefix text <code>fill</code> str '\u2588' Fill character <code>empty</code> str '\u2591' Empty character <p>Returns: <code>str</code> - Progress bar</p>"},{"location":"api/visualization/#spinner","title":"spinner","text":"<p>Get spinner animation frames.</p> <pre><code>from pyeval import spinner\n\nframes = spinner()  # Returns: ['\u280b', '\u2819', '\u2839', '\u2838', '\u283c', '\u2834', '\u2826', '\u2827', '\u2807', '\u280f']\n\n# Usage in a loop\nimport time\nfor i in range(20):\n    print(f\"\\r{frames[i % len(frames)]} Processing...\", end='')\n    time.sleep(0.1)\n</code></pre> <p>Returns: <code>list</code> - Animation frame characters</p>"},{"location":"api/visualization/#curve-visualizations","title":"Curve Visualizations","text":""},{"location":"api/visualization/#roc_curve_display","title":"roc_curve_display","text":"<p>Display ASCII ROC curve.</p> <pre><code>from pyeval import roc_curve_display\n\nfpr = [0.0, 0.1, 0.2, 0.4, 0.6, 1.0]\ntpr = [0.0, 0.5, 0.7, 0.8, 0.9, 1.0]\nauc = 0.85\n\ndisplay = roc_curve_display(fpr, tpr, auc=auc, height=10, width=40)\nprint(display)\n</code></pre> <p>Output: <pre><code>ROC Curve (AUC = 0.85)\n1.0 \u2524                                    \u25cf\n    \u2502                              \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n    \u2502                        \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n    \u2502                  \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n0.5 \u2524            \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n    \u2502      \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n    \u2502\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n0.0 \u2524\u25cf\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n     0.0                              1.0\n                    FPR\n</code></pre></p> <p>Returns: <code>str</code> - ASCII ROC curve</p>"},{"location":"api/visualization/#pr_curve_display","title":"pr_curve_display","text":"<p>Display ASCII Precision-Recall curve.</p> <pre><code>from pyeval import pr_curve_display\n\nprecision = [1.0, 0.9, 0.8, 0.7, 0.6]\nrecall = [0.0, 0.3, 0.5, 0.7, 1.0]\n\ndisplay = pr_curve_display(precision, recall, height=10, width=40)\nprint(display)\n</code></pre> <p>Returns: <code>str</code> - ASCII PR curve</p>"},{"location":"api/visualization/#tables","title":"Tables","text":""},{"location":"api/visualization/#table_display","title":"table_display","text":"<p>Create formatted ASCII table.</p> <pre><code>from pyeval import table_display\n\nheaders = ['Model', 'Accuracy', 'F1', 'Latency']\nrows = [\n    ['Model A', '0.92', '0.89', '15ms'],\n    ['Model B', '0.95', '0.93', '25ms'],\n    ['Model C', '0.91', '0.88', '10ms']\n]\n\ntable = table_display(headers, rows)\nprint(table)\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Model   \u2502 Accuracy \u2502 F1   \u2502 Latency \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Model A \u2502 0.92     \u2502 0.89 \u2502 15ms    \u2502\n\u2502 Model B \u2502 0.95     \u2502 0.93 \u2502 25ms    \u2502\n\u2502 Model C \u2502 0.91     \u2502 0.88 \u2502 10ms    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>headers</code> list required Column headers <code>rows</code> list[list] required Table rows <code>alignment</code> str 'left' Text alignment <p>Returns: <code>str</code> - Formatted table</p>"},{"location":"api/visualization/#complete-visualization-functions","title":"Complete Visualization Functions","text":"Function Description <code>confusion_matrix_display</code> Formatted confusion matrix <code>classification_report_display</code> Classification metrics report <code>horizontal_bar_chart</code> Horizontal ASCII bars <code>vertical_bar_chart</code> Vertical ASCII bars <code>histogram_display</code> Distribution histogram <code>sparkline</code> Compact trend line <code>sparkline_with_stats</code> Sparkline with statistics <code>progress_bar</code> Progress indicator <code>spinner</code> Animation frames <code>roc_curve_display</code> ASCII ROC curve <code>pr_curve_display</code> ASCII PR curve <code>table_display</code> Formatted table"},{"location":"api/visualization/#usage-tips","title":"Usage Tips","text":""},{"location":"api/visualization/#combining-visualizations","title":"Combining Visualizations","text":"<pre><code>from pyeval import (\n    confusion_matrix_display, classification_report_display,\n    horizontal_bar_chart, sparkline\n)\n\ndef evaluation_report(y_true, y_pred, training_losses):\n    \"\"\"Generate comprehensive visual evaluation report.\"\"\"\n\n    # Confusion Matrix\n    print(\"=\"*50)\n    print(\"CONFUSION MATRIX\")\n    print(\"=\"*50)\n    print(confusion_matrix_display(\n        confusion_matrix(y_true, y_pred),\n        labels=['Neg', 'Pos']\n    ))\n\n    # Classification Report\n    print(\"\\n\" + \"=\"*50)\n    print(\"CLASSIFICATION REPORT\")\n    print(\"=\"*50)\n    print(classification_report_display(y_true, y_pred))\n\n    # Metrics Bar Chart\n    metrics = {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred),\n        'Recall': recall_score(y_true, y_pred),\n        'F1': f1_score(y_true, y_pred)\n    }\n    print(\"\\n\" + \"=\"*50)\n    print(\"PERFORMANCE METRICS\")\n    print(\"=\"*50)\n    print(horizontal_bar_chart(metrics))\n\n    # Training Progress\n    print(\"\\n\" + \"=\"*50)\n    print(\"TRAINING PROGRESS\")\n    print(\"=\"*50)\n    print(f\"Loss: {sparkline(training_losses)}\")\n</code></pre>"},{"location":"api/visualization/#real-time-progress-display","title":"Real-time Progress Display","text":"<pre><code>from pyeval import progress_bar\nimport time\n\ndef process_with_progress(items):\n    \"\"\"Process items with progress visualization.\"\"\"\n    total = len(items)\n\n    for i, item in enumerate(items):\n        # Process item\n        process(item)\n\n        # Update progress\n        bar = progress_bar(i + 1, total, prefix=\"Processing\")\n        print(f\"\\r{bar}\", end='', flush=True)\n\n    print()  # New line after completion\n</code></pre>"},{"location":"examples/llm/","title":"LLM Examples","text":"<p>Comprehensive examples for Large Language Model evaluation with PyEval.</p>"},{"location":"examples/llm/#response-quality-evaluation","title":"\ud83e\udd16 Response Quality Evaluation","text":""},{"location":"examples/llm/#toxicity-detection","title":"Toxicity Detection","text":"<p>Detect harmful, offensive, or inappropriate content in LLM outputs.</p> <pre><code>from pyeval import toxicity_score\n\n# Clean response\nclean_response = \"\"\"\nMachine learning is a fascinating field of study. It enables computers \nto learn from data and make predictions. Many applications benefit from ML,\nincluding healthcare, finance, and transportation.\n\"\"\"\n\nresult = toxicity_score(clean_response)\nprint(\"Clean Response Analysis:\")\nprint(f\"  Toxicity Score: {result['toxicity_score']:.4f}\")\nprint(f\"  Is Toxic: {result['is_toxic']}\")\nprint(f\"  Severity: {result['severity']}\")\nprint(f\"  Patterns Found: {result['toxic_patterns_found']}\")\n\n# Problematic response\nproblematic = \"This is terrible and stupid. What an idiotic idea.\"\n\nresult = toxicity_score(problematic)\nprint(\"\\nProblematic Response Analysis:\")\nprint(f\"  Toxicity Score: {result['toxicity_score']:.4f}\")\nprint(f\"  Is Toxic: {result['is_toxic']}\")\nprint(f\"  Severity: {result['severity']}\")\nprint(f\"  Patterns Found: {result['toxic_patterns_found']}\")\n\n# Custom threshold\nresult = toxicity_score(clean_response, threshold=0.3)\nprint(f\"\\nWith lower threshold: Is Toxic = {result['is_toxic']}\")\n</code></pre>"},{"location":"examples/llm/#coherence-evaluation","title":"Coherence Evaluation","text":"<p>Measure logical flow and sentence connectivity.</p> <pre><code>from pyeval import coherence_score\n\n# Coherent response\ncoherent = \"\"\"\nMachine learning is a subset of artificial intelligence. It enables \ncomputers to learn from data. This learning process improves their \nperformance over time. As a result, ML systems can make accurate \npredictions without explicit programming.\n\"\"\"\n\nresult = coherence_score(coherent)\nprint(\"Coherent Response:\")\nprint(f\"  Coherence Score: {result['coherence_score']:.4f}\")\nprint(f\"  Sentence Count: {result['sentence_count']}\")\nprint(f\"  Transition Quality: {result.get('transition_quality', 'N/A')}\")\n\n# Incoherent response\nincoherent = \"\"\"\nMachine learning uses data. The weather is nice today. Cats are \nfurry animals. Python is a programming language. Pizza is delicious.\n\"\"\"\n\nresult = coherence_score(incoherent)\nprint(\"\\nIncoherent Response:\")\nprint(f\"  Coherence Score: {result['coherence_score']:.4f}\")\nprint(f\"  Sentence Count: {result['sentence_count']}\")\n</code></pre>"},{"location":"examples/llm/#hallucination-detection","title":"Hallucination Detection","text":"<p>Identify claims not supported by the provided context.</p> <pre><code>from pyeval import hallucination_score\n\ncontext = \"\"\"\nPython was created by Guido van Rossum and first released in 1991. \nIt is known for its simple syntax and readability. Python supports \nmultiple programming paradigms including procedural, object-oriented, \nand functional programming.\n\"\"\"\n\n# Faithful response\nfaithful = \"\"\"\nPython is a programming language created by Guido van Rossum in 1991. \nIt is known for its readable syntax and supports object-oriented programming.\n\"\"\"\n\nresult = hallucination_score(faithful, context)\nprint(\"Faithful Response:\")\nprint(f\"  Hallucination Score: {result['hallucination_score']:.4f}\")\nprint(f\"  Grounded Ratio: {result['grounded_ratio']:.4f}\")\nprint(f\"  Ungrounded Claims: {result.get('ungrounded_claims', [])}\")\n\n# Hallucinated response\nhallucinated = \"\"\"\nPython was created by Guido van Rossum in 1989 at Microsoft. It is the \nfastest programming language and is primarily used for mobile app development.\n\"\"\"\n\nresult = hallucination_score(hallucinated, context)\nprint(\"\\nHallucinated Response:\")\nprint(f\"  Hallucination Score: {result['hallucination_score']:.4f}\")\nprint(f\"  Grounded Ratio: {result['grounded_ratio']:.4f}\")\nprint(f\"  Ungrounded Claims: {result.get('ungrounded_claims', [])}\")\n</code></pre>"},{"location":"examples/llm/#bias-detection","title":"Bias Detection","text":"<p>Identify potential biases in generated text.</p> <pre><code>from pyeval import bias_detection_score\n\n# Biased text\nbiased_text = \"\"\"\nMen are naturally better at math and science, while women excel at \nnurturing and caregiving. Older employees are typically less adaptable \nto new technology.\n\"\"\"\n\nresult = bias_detection_score(biased_text)\nprint(\"Bias Analysis:\")\nprint(f\"  Overall Bias Score: {result['bias_score']:.4f}\")\nprint(f\"  Categories Detected: {result.get('categories', [])}\")\nprint(f\"  Biased Phrases: {result.get('biased_phrases', [])}\")\n\n# Neutral text\nneutral_text = \"\"\"\nMachine learning algorithms can be applied across various domains.\nResearch shows that diverse teams often produce better results.\nIndividual capabilities vary regardless of demographic factors.\n\"\"\"\n\nresult = bias_detection_score(neutral_text)\nprint(\"\\nNeutral Text:\")\nprint(f\"  Overall Bias Score: {result['bias_score']:.4f}\")\nprint(f\"  Categories Detected: {result.get('categories', [])}\")\n</code></pre>"},{"location":"examples/llm/#fluency-evaluation","title":"Fluency Evaluation","text":"<p>Measure grammatical correctness and natural language quality.</p> <pre><code>from pyeval import fluency_score\n\n# Fluent text\nfluent = \"\"\"\nThe quick brown fox jumps over the lazy dog. This sentence contains\nevery letter of the alphabet and demonstrates proper grammar and \nsentence structure.\n\"\"\"\n\nresult = fluency_score(fluent)\nprint(\"Fluent Text:\")\nprint(f\"  Fluency Score: {result['fluency_score']:.4f}\")\nprint(f\"  Grammar Score: {result.get('grammar_score', 'N/A')}\")\n\n# Disfluent text\ndisfluent = \"\"\"\nThe quick brown fox jump over lazy dog. This sentence contains\nevery letter alphabet and demonstrate proper grammar sentence structure.\n\"\"\"\n\nresult = fluency_score(disfluent)\nprint(\"\\nDisfluent Text:\")\nprint(f\"  Fluency Score: {result['fluency_score']:.4f}\")\n</code></pre>"},{"location":"examples/llm/#comprehensive-llm-evaluation","title":"\ud83d\udcca Comprehensive LLM Evaluation","text":""},{"location":"examples/llm/#using-llmmetrics-class","title":"Using LLMMetrics Class","text":"<pre><code>from pyeval import LLMMetrics\n\n# Sample LLM interaction\ncontext = \"\"\"\nArtificial Intelligence (AI) is intelligence demonstrated by machines.\nAI research began in the 1950s. Machine learning is a subset of AI that\nfocuses on learning from data. Deep learning uses neural networks with\nmultiple layers.\n\"\"\"\n\nresponse = \"\"\"\nAI is demonstrated by machines and began in the 1950s. Machine learning,\na subset of AI, learns from data. Deep learning uses multi-layer neural networks.\nThis technology is transforming healthcare and transportation.\n\"\"\"\n\n# Compute all metrics\nmetrics = LLMMetrics()\nresults = metrics.compute(response, context=context)\n\nprint(\"=== Comprehensive LLM Evaluation ===\\n\")\nfor metric, value in results.items():\n    if isinstance(value, dict):\n        print(f\"{metric}:\")\n        for k, v in value.items():\n            if isinstance(v, float):\n                print(f\"  {k}: {v:.4f}\")\n            else:\n                print(f\"  {k}: {v}\")\n    elif isinstance(value, float):\n        print(f\"{metric}: {value:.4f}\")\n    else:\n        print(f\"{metric}: {value}\")\n</code></pre>"},{"location":"examples/llm/#evaluating-multiple-responses","title":"Evaluating Multiple Responses","text":"<pre><code>from pyeval import (\n    toxicity_score,\n    coherence_score,\n    hallucination_score,\n    fluency_score\n)\n\nquery = \"What is machine learning?\"\n\ncontext = \"\"\"\nMachine learning is a type of artificial intelligence that enables \ncomputers to learn from data without being explicitly programmed.\n\"\"\"\n\nresponses = {\n    'GPT-4': \"\"\"\n        Machine learning is a branch of AI that allows systems to learn \n        and improve from experience automatically. It works by analyzing \n        patterns in data.\n    \"\"\",\n    'Claude': \"\"\"\n        Machine learning enables computers to learn from data without \n        explicit programming. It's a subset of artificial intelligence \n        focused on pattern recognition.\n    \"\"\",\n    'LLaMA': \"\"\"\n        ML is AI that learns from data. Computers can improve automatically\n        by finding patterns. Very useful for predictions.\n    \"\"\",\n}\n\nprint(\"LLM Response Comparison:\")\nprint(\"=\" * 70)\nprint(f\"{'Model':&lt;12} {'Toxicity':&gt;10} {'Coherence':&gt;11} {'Hallucin.':&gt;11} {'Fluency':&gt;10}\")\nprint(\"-\" * 70)\n\nfor model, response in responses.items():\n    tox = toxicity_score(response)['toxicity_score']\n    coh = coherence_score(response)['coherence_score']\n    hal = hallucination_score(response, context)['hallucination_score']\n    flu = fluency_score(response)['fluency_score']\n\n    print(f\"{model:&lt;12} {tox:&gt;10.4f} {coh:&gt;11.4f} {hal:&gt;11.4f} {flu:&gt;10.4f}\")\n</code></pre>"},{"location":"examples/llm/#instruction-following","title":"\ud83d\udd0d Instruction Following","text":""},{"location":"examples/llm/#evaluating-task-completion","title":"Evaluating Task Completion","text":"<pre><code>from pyeval import instruction_following_score\n\ninstruction = \"Write a haiku about programming\"\n\n# Good response (follows instruction)\ngood_response = \"\"\"\nCode flows like water\nBugs hide in the logic maze\nDebug brings the light\n\"\"\"\n\n# Bad response (doesn't follow instruction)\nbad_response = \"\"\"\nProgramming is really fun. I love writing code in Python. \nIt's my favorite language because it's so easy to learn.\n\"\"\"\n\nresult_good = instruction_following_score(instruction, good_response)\nresult_bad = instruction_following_score(instruction, bad_response)\n\nprint(\"Instruction Following Evaluation:\")\nprint(f\"\\nGood Response:\")\nprint(f\"  Score: {result_good['score']:.4f}\")\nprint(f\"  Format Correct: {result_good.get('format_correct', 'N/A')}\")\n\nprint(f\"\\nBad Response:\")\nprint(f\"  Score: {result_bad['score']:.4f}\")\nprint(f\"  Format Correct: {result_bad.get('format_correct', 'N/A')}\")\n</code></pre>"},{"location":"examples/llm/#production-monitoring","title":"\ud83d\udcc8 Production Monitoring","text":""},{"location":"examples/llm/#batch-evaluation-pipeline","title":"Batch Evaluation Pipeline","text":"<pre><code>from pyeval import (\n    toxicity_score,\n    coherence_score,\n    hallucination_score,\n    LLMMetrics,\n    mean\n)\n\n# Simulated production responses\nproduction_logs = [\n    {\n        'query': \"What is Python?\",\n        'context': \"Python is a programming language created in 1991.\",\n        'response': \"Python is a programming language created by Guido van Rossum.\"\n    },\n    {\n        'query': \"Explain ML\",\n        'context': \"Machine learning is a subset of AI.\",\n        'response': \"ML allows computers to learn from data automatically.\"\n    },\n    {\n        'query': \"What is deep learning?\",\n        'context': \"Deep learning uses neural networks.\",\n        'response': \"Deep learning uses multi-layer neural networks for complex patterns.\"\n    },\n]\n\n# Evaluate batch\nprint(\"Production Monitoring Report\")\nprint(\"=\" * 60)\n\ntoxicity_scores = []\ncoherence_scores = []\nhallucination_scores = []\n\nfor i, log in enumerate(production_logs):\n    tox = toxicity_score(log['response'])['toxicity_score']\n    coh = coherence_score(log['response'])['coherence_score']\n    hal = hallucination_score(log['response'], log['context'])['hallucination_score']\n\n    toxicity_scores.append(tox)\n    coherence_scores.append(coh)\n    hallucination_scores.append(hal)\n\n    print(f\"\\nQuery {i+1}: {log['query'][:30]}...\")\n    print(f\"  Toxicity: {tox:.4f} | Coherence: {coh:.4f} | Hallucination: {hal:.4f}\")\n\n# Summary statistics\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SUMMARY STATISTICS\")\nprint(\"-\" * 60)\nprint(f\"Average Toxicity:      {mean(toxicity_scores):.4f}\")\nprint(f\"Average Coherence:     {mean(coherence_scores):.4f}\")\nprint(f\"Average Hallucination: {mean(hallucination_scores):.4f}\")\nprint(f\"Samples Evaluated:     {len(production_logs)}\")\n\n# Alert thresholds\nif mean(toxicity_scores) &gt; 0.3:\n    print(\"\\n\u26a0\ufe0f ALERT: High average toxicity detected!\")\nif mean(hallucination_scores) &gt; 0.5:\n    print(\"\\n\u26a0\ufe0f ALERT: High hallucination rate detected!\")\n</code></pre>"},{"location":"examples/llm/#custom-evaluation-rubric","title":"\ud83c\udfaf Custom Evaluation Rubric","text":"<pre><code>from pyeval import (\n    toxicity_score,\n    coherence_score,\n    hallucination_score,\n    fluency_score\n)\n\ndef evaluate_llm_response(response, context, weights=None):\n    \"\"\"\n    Custom rubric for LLM evaluation.\n\n    Args:\n        response: LLM generated response\n        context: Reference context\n        weights: Dict of metric weights (must sum to 1.0)\n\n    Returns:\n        Dict with individual scores and weighted total\n    \"\"\"\n    if weights is None:\n        weights = {\n            'toxicity': 0.2,      # Weight for (1 - toxicity)\n            'coherence': 0.3,\n            'faithfulness': 0.3,  # 1 - hallucination\n            'fluency': 0.2\n        }\n\n    # Get individual scores\n    tox = 1 - toxicity_score(response)['toxicity_score']  # Invert: higher is better\n    coh = coherence_score(response)['coherence_score']\n    faith = 1 - hallucination_score(response, context)['hallucination_score']\n    flu = fluency_score(response)['fluency_score']\n\n    # Calculate weighted score\n    weighted_total = (\n        weights['toxicity'] * tox +\n        weights['coherence'] * coh +\n        weights['faithfulness'] * faith +\n        weights['fluency'] * flu\n    )\n\n    return {\n        'non_toxicity': tox,\n        'coherence': coh,\n        'faithfulness': faith,\n        'fluency': flu,\n        'weighted_score': weighted_total,\n        'grade': 'A' if weighted_total &gt;= 0.9 else\n                 'B' if weighted_total &gt;= 0.8 else\n                 'C' if weighted_total &gt;= 0.7 else\n                 'D' if weighted_total &gt;= 0.6 else 'F'\n    }\n\n# Example usage\ncontext = \"Python is a programming language created by Guido van Rossum in 1991.\"\nresponse = \"Python is a popular programming language created in 1991 by Guido van Rossum.\"\n\nresult = evaluate_llm_response(response, context)\n\nprint(\"Custom Evaluation Rubric:\")\nprint(\"-\" * 40)\nfor metric, value in result.items():\n    if isinstance(value, float):\n        print(f\"{metric}: {value:.4f}\")\n    else:\n        print(f\"{metric}: {value}\")\n</code></pre>"},{"location":"examples/ml/","title":"ML Examples","text":"<p>Comprehensive examples for Machine Learning evaluation with PyEval.</p>"},{"location":"examples/ml/#binary-classification","title":"\ud83c\udfaf Binary Classification","text":""},{"location":"examples/ml/#complete-evaluation-workflow","title":"Complete Evaluation Workflow","text":"<pre><code>from pyeval import (\n    # Metrics\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    fbeta_score,\n    roc_auc_score,\n    average_precision_score,\n    matthews_corrcoef,\n    cohen_kappa_score,\n    confusion_matrix,\n    balanced_accuracy_score,\n    log_loss,\n\n    # Utilities\n    ClassificationMetrics,\n    classification_report\n)\n\n# Sample data\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1]\ny_prob = [0.9, 0.1, 0.4, 0.8, 0.2, 0.95, 0.6, 0.15, 0.85, 0.3, 0.1, 0.9, 0.2, 0.88, 0.55]\n\n# === Basic Metrics ===\nprint(\"=== Basic Metrics ===\")\nprint(f\"Accuracy:          {accuracy_score(y_true, y_pred):.4f}\")\nprint(f\"Balanced Accuracy: {balanced_accuracy_score(y_true, y_pred):.4f}\")\nprint(f\"Precision:         {precision_score(y_true, y_pred):.4f}\")\nprint(f\"Recall:            {recall_score(y_true, y_pred):.4f}\")\nprint(f\"F1 Score:          {f1_score(y_true, y_pred):.4f}\")\n\n# === Advanced Metrics ===\nprint(\"\\n=== Advanced Metrics ===\")\nprint(f\"F0.5 Score:        {fbeta_score(y_true, y_pred, beta=0.5):.4f}\")  # Precision-weighted\nprint(f\"F2 Score:          {fbeta_score(y_true, y_pred, beta=2):.4f}\")    # Recall-weighted\nprint(f\"MCC:               {matthews_corrcoef(y_true, y_pred):.4f}\")\nprint(f\"Cohen's Kappa:     {cohen_kappa_score(y_true, y_pred):.4f}\")\n\n# === Probability-based Metrics ===\nprint(\"\\n=== Probability Metrics ===\")\nprint(f\"ROC AUC:           {roc_auc_score(y_true, y_prob):.4f}\")\nprint(f\"PR AUC:            {average_precision_score(y_true, y_prob):.4f}\")\nprint(f\"Log Loss:          {log_loss(y_true, y_prob):.4f}\")\n\n# === Confusion Matrix ===\nprint(\"\\n=== Confusion Matrix ===\")\ncm = confusion_matrix(y_true, y_pred)\nprint(f\"TN={cm[0][0]}, FP={cm[0][1]}, FN={cm[1][0]}, TP={cm[1][1]}\")\n\n# === All-in-One ===\nprint(\"\\n=== Classification Report ===\")\nreport = classification_report(y_true, y_pred)\nprint(report)\n</code></pre>"},{"location":"examples/ml/#multi-class-classification","title":"\ud83c\udfa8 Multi-Class Classification","text":"<pre><code>from pyeval import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n    top_k_accuracy_score\n)\n\n# 5-class problem\ny_true = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2]\ny_pred = [0, 2, 2, 3, 4, 0, 1, 1, 3, 3, 1, 1, 2]\n\n# Top-k probabilities (for top-k accuracy)\ny_prob = [\n    [0.9, 0.05, 0.02, 0.02, 0.01],  # Class 0\n    [0.1, 0.3, 0.4, 0.1, 0.1],      # Class 1 (wrong pred)\n    [0.1, 0.1, 0.6, 0.1, 0.1],      # Class 2\n    # ... more samples\n]\n\nprint(\"=== Multi-Class Accuracy ===\")\nprint(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n\nprint(\"\\n=== Averaging Methods ===\")\n# Micro: Global calculation (good for imbalanced classes)\nprint(f\"Micro Precision: {precision_score(y_true, y_pred, average='micro'):.4f}\")\n\n# Macro: Average per class (treats all classes equally)\nprint(f\"Macro Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\nprint(f\"Macro Recall:    {recall_score(y_true, y_pred, average='macro'):.4f}\")\nprint(f\"Macro F1:        {f1_score(y_true, y_pred, average='macro'):.4f}\")\n\n# Weighted: Average weighted by support\nprint(f\"Weighted F1:     {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n\nprint(\"\\n=== Per-Class Metrics ===\")\nlabels = [0, 1, 2, 3, 4]\nfor label in labels:\n    # One-vs-rest for each class\n    y_true_binary = [1 if y == label else 0 for y in y_true]\n    y_pred_binary = [1 if y == label else 0 for y in y_pred]\n    p = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n    r = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n    f = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n    print(f\"Class {label}: P={p:.3f}, R={r:.3f}, F1={f:.3f}\")\n\nprint(\"\\n=== Confusion Matrix ===\")\ncm = confusion_matrix(y_true, y_pred)\nfor row in cm:\n    print(row)\n</code></pre>"},{"location":"examples/ml/#regression","title":"\ud83d\udcc8 Regression","text":"<pre><code>from pyeval import (\n    mean_squared_error,\n    root_mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    r2_score,\n    explained_variance_score,\n    max_error,\n    median_absolute_error,\n    mean_squared_log_error,\n    RegressionMetrics\n)\n\n# Sample regression data\ny_true = [3.0, -0.5, 2.0, 7.0, 4.5, 6.2, 1.8, 5.5, 3.3, 8.1]\ny_pred = [2.5, 0.0, 2.1, 7.8, 4.0, 5.9, 2.1, 5.2, 3.8, 7.5]\n\nprint(\"=== Error Metrics ===\")\nprint(f\"MSE:  {mean_squared_error(y_true, y_pred):.4f}\")\nprint(f\"RMSE: {root_mean_squared_error(y_true, y_pred):.4f}\")\nprint(f\"MAE:  {mean_absolute_error(y_true, y_pred):.4f}\")\nprint(f\"MedAE:{median_absolute_error(y_true, y_pred):.4f}\")\nprint(f\"Max:  {max_error(y_true, y_pred):.4f}\")\n\nprint(\"\\n=== Relative Metrics ===\")\n# Filter positive values for MAPE and MSLE\ny_true_pos = [abs(y) + 0.01 for y in y_true]  # Ensure positive\ny_pred_pos = [abs(y) + 0.01 for y in y_pred]\nprint(f\"MAPE: {mean_absolute_percentage_error(y_true_pos, y_pred_pos):.4f}\")\nprint(f\"MSLE: {mean_squared_log_error(y_true_pos, y_pred_pos):.4f}\")\n\nprint(\"\\n=== Goodness of Fit ===\")\nprint(f\"R\u00b2:   {r2_score(y_true, y_pred):.4f}\")\nprint(f\"Explained Variance: {explained_variance_score(y_true, y_pred):.4f}\")\n\n# All at once\nprint(\"\\n=== All Metrics ===\")\nmetrics = RegressionMetrics()\nresults = metrics.compute(y_true, y_pred)\nfor name, value in results.items():\n    print(f\"{name}: {value:.4f}\")\n</code></pre>"},{"location":"examples/ml/#clustering","title":"\ud83d\udd35 Clustering","text":"<pre><code>from pyeval import (\n    adjusted_rand_index,\n    normalized_mutual_info,\n    homogeneity_score,\n    completeness_score,\n    v_measure_score,\n    fowlkes_mallows_score,\n    silhouette_score,\n    davies_bouldin_index,\n    calinski_harabasz_index\n)\n\n# Clustering results (labels_true = ground truth, labels_pred = predicted)\nlabels_true = [0, 0, 0, 1, 1, 1, 2, 2, 2]\nlabels_pred = [0, 0, 1, 1, 1, 1, 2, 2, 2]\n\n# Data points (for intrinsic metrics)\nX = [\n    [1.0, 2.0], [1.5, 1.8], [1.2, 2.1],  # Cluster 0\n    [5.0, 5.0], [5.2, 5.1], [4.8, 5.2],  # Cluster 1\n    [9.0, 1.0], [9.2, 0.8], [8.8, 1.1],  # Cluster 2\n]\n\nprint(\"=== External Metrics (with ground truth) ===\")\nprint(f\"ARI:         {adjusted_rand_index(labels_true, labels_pred):.4f}\")\nprint(f\"NMI:         {normalized_mutual_info(labels_true, labels_pred):.4f}\")\nprint(f\"Homogeneity: {homogeneity_score(labels_true, labels_pred):.4f}\")\nprint(f\"Completeness:{completeness_score(labels_true, labels_pred):.4f}\")\nprint(f\"V-Measure:   {v_measure_score(labels_true, labels_pred):.4f}\")\nprint(f\"FMI:         {fowlkes_mallows_score(labels_true, labels_pred):.4f}\")\n\nprint(\"\\n=== Internal Metrics (without ground truth) ===\")\nprint(f\"Silhouette:       {silhouette_score(X, labels_pred):.4f}\")\nprint(f\"Davies-Bouldin:   {davies_bouldin_index(X, labels_pred):.4f}\")\nprint(f\"Calinski-Harabasz:{calinski_harabasz_index(X, labels_pred):.4f}\")\n</code></pre>"},{"location":"examples/ml/#model-comparison","title":"\ud83d\udcca Model Comparison","text":"<pre><code>from pyeval import (\n    accuracy_score,\n    f1_score,\n    roc_auc_score,\n    Evaluator\n)\n\n# Ground truth\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n\n# Multiple model predictions\nmodels = {\n    'Logistic Regression': [1, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n    'Random Forest':       [1, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n    'SVM':                 [1, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n    'Neural Network':      [1, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n}\n\n# Compare models\nprint(\"Model Comparison:\")\nprint(\"-\" * 50)\nprint(f\"{'Model':&lt;20} {'Accuracy':&gt;10} {'F1':&gt;10}\")\nprint(\"-\" * 50)\n\nfor name, y_pred in models.items():\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    print(f\"{name:&lt;20} {acc:&gt;10.4f} {f1:&gt;10.4f}\")\n\n# Find best model\nbest_model = max(models.keys(), key=lambda m: f1_score(y_true, models[m]))\nprint(f\"\\nBest model by F1: {best_model}\")\n</code></pre>"},{"location":"examples/ml/#cross-validation-metrics","title":"\ud83c\udfb2 Cross-Validation Metrics","text":"<pre><code>from pyeval import accuracy_score, f1_score, mean, std\n\n# Simulated CV results (5 folds)\ncv_scores = {\n    'accuracy': [0.85, 0.82, 0.88, 0.84, 0.86],\n    'f1': [0.83, 0.80, 0.87, 0.82, 0.85],\n    'precision': [0.84, 0.81, 0.86, 0.83, 0.84],\n    'recall': [0.82, 0.79, 0.88, 0.81, 0.86],\n}\n\nprint(\"Cross-Validation Results (5 folds):\")\nprint(\"-\" * 45)\nprint(f\"{'Metric':&lt;15} {'Mean':&gt;10} {'Std':&gt;10} {'Min':&gt;8} {'Max':&gt;8}\")\nprint(\"-\" * 45)\n\nfor metric, scores in cv_scores.items():\n    print(f\"{metric:&lt;15} {mean(scores):&gt;10.4f} {std(scores):&gt;10.4f} {min(scores):&gt;8.4f} {max(scores):&gt;8.4f}\")\n</code></pre>"},{"location":"examples/ml/#threshold-optimization","title":"\ud83c\udfaf Threshold Optimization","text":"<pre><code>from pyeval import precision_score, recall_score, f1_score\n\n# Ground truth and probabilities\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\ny_prob = [0.9, 0.2, 0.6, 0.8, 0.3, 0.7, 0.4, 0.1, 0.85, 0.55]\n\n# Find optimal threshold\nthresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n\nprint(\"Threshold Analysis:\")\nprint(\"-\" * 55)\nprint(f\"{'Threshold':&gt;10} {'Precision':&gt;12} {'Recall':&gt;10} {'F1':&gt;10}\")\nprint(\"-\" * 55)\n\nbest_threshold = 0.5\nbest_f1 = 0\n\nfor thresh in thresholds:\n    y_pred = [1 if p &gt;= thresh else 0 for p in y_prob]\n    p = precision_score(y_true, y_pred, zero_division=0)\n    r = recall_score(y_true, y_pred, zero_division=0)\n    f = f1_score(y_true, y_pred, zero_division=0)\n\n    print(f\"{thresh:&gt;10.1f} {p:&gt;12.4f} {r:&gt;10.4f} {f:&gt;10.4f}\")\n\n    if f &gt; best_f1:\n        best_f1 = f\n        best_threshold = thresh\n\nprint(f\"\\nOptimal Threshold: {best_threshold} (F1={best_f1:.4f})\")\n</code></pre>"},{"location":"examples/nlp/","title":"NLP Examples","text":"<p>Comprehensive examples for Natural Language Processing evaluation with PyEval.</p>"},{"location":"examples/nlp/#text-generation-metrics","title":"\ud83d\udcdd Text Generation Metrics","text":""},{"location":"examples/nlp/#bleu-score","title":"BLEU Score","text":"<p>BLEU (Bilingual Evaluation Understudy) measures n-gram overlap between reference and hypothesis.</p> <pre><code>from pyeval import bleu_score, sentence_bleu, corpus_bleu\n\n# Single sentence\nreference = \"The cat sat on the mat\"\nhypothesis = \"The cat is on the mat\"\n\n# Default BLEU (uses 1-4 grams with smoothing)\nscore = bleu_score(reference, hypothesis)\nprint(f\"BLEU: {score:.4f}\")\n\n# Sentence-level BLEU with specific n-gram weights\nscore_1 = sentence_bleu(reference, hypothesis, weights=(1, 0, 0, 0))  # BLEU-1\nscore_2 = sentence_bleu(reference, hypothesis, weights=(0.5, 0.5, 0, 0))  # BLEU-2\nscore_4 = sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25))  # BLEU-4\n\nprint(f\"BLEU-1: {score_1:.4f}\")\nprint(f\"BLEU-2: {score_2:.4f}\")\nprint(f\"BLEU-4: {score_4:.4f}\")\n\n# Multiple references (pick best match)\nreferences = [\n    \"The cat sat on the mat\",\n    \"A cat was sitting on the mat\",\n    \"There is a cat on the mat\"\n]\nscore = bleu_score(references, hypothesis)\nprint(f\"Multi-ref BLEU: {score:.4f}\")\n\n# Corpus-level BLEU\ncorpus_refs = [\n    [\"The cat sat on the mat\"],\n    [\"Hello world\"],\n    [\"Machine learning is great\"]\n]\ncorpus_hyps = [\n    \"The cat is on the mat\",\n    \"Hello there world\",\n    \"Machine learning is awesome\"\n]\ncorpus_score = corpus_bleu(corpus_refs, corpus_hyps)\nprint(f\"Corpus BLEU: {corpus_score:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#rouge-score","title":"ROUGE Score","text":"<p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures recall of n-grams.</p> <pre><code>from pyeval import rouge_score, rouge_n, rouge_l\n\nreference = \"The quick brown fox jumps over the lazy dog in the park\"\nhypothesis = \"A quick brown fox jumped over the lazy dog\"\n\n# Full ROUGE scores\nrouge = rouge_score(reference, hypothesis)\n\nprint(\"=== ROUGE-1 (unigram) ===\")\nprint(f\"  Precision: {rouge['1']['p']:.4f}\")\nprint(f\"  Recall:    {rouge['1']['r']:.4f}\")\nprint(f\"  F1:        {rouge['1']['f']:.4f}\")\n\nprint(\"\\n=== ROUGE-2 (bigram) ===\")\nprint(f\"  Precision: {rouge['2']['p']:.4f}\")\nprint(f\"  Recall:    {rouge['2']['r']:.4f}\")\nprint(f\"  F1:        {rouge['2']['f']:.4f}\")\n\nprint(\"\\n=== ROUGE-L (longest common subsequence) ===\")\nprint(f\"  Precision: {rouge['l']['p']:.4f}\")\nprint(f\"  Recall:    {rouge['l']['r']:.4f}\")\nprint(f\"  F1:        {rouge['l']['f']:.4f}\")\n\n# Individual ROUGE variants\nr1 = rouge_n(reference, hypothesis, n=1)\nr2 = rouge_n(reference, hypothesis, n=2)\nrl = rouge_l(reference, hypothesis)\n</code></pre>"},{"location":"examples/nlp/#meteor-score","title":"METEOR Score","text":"<p>METEOR uses stemming, synonyms, and word alignment.</p> <pre><code>from pyeval import meteor_score\n\nreference = \"The cat sat on the mat\"\nhypothesis = \"A cat was sitting on the mat\"\n\n# METEOR handles synonyms and stemming\nscore = meteor_score(reference, hypothesis)\nprint(f\"METEOR: {score:.4f}\")\n\n# Multiple references\nreferences = [\n    \"The cat sat on the mat\",\n    \"A cat was sitting on the mat\",\n]\nscore = meteor_score(references, hypothesis)\nprint(f\"Multi-ref METEOR: {score:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#ter-translation-edit-rate","title":"TER (Translation Edit Rate)","text":"<p>TER measures the number of edits needed to transform hypothesis to reference.</p> <pre><code>from pyeval import ter_score\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A quick brown fox jumps over the lazy dog\"\n\n# TER (lower is better, 0 = perfect match)\nter = ter_score(reference, hypothesis)\nprint(f\"TER: {ter:.4f}\")\n\n# TER can be &gt; 1 if many edits needed\nbad_hypothesis = \"Something completely different\"\nter_bad = ter_score(reference, bad_hypothesis)\nprint(f\"Bad TER: {ter_bad:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#chrf-score","title":"chrF Score","text":"<p>chrF uses character n-grams (language-agnostic).</p> <pre><code>from pyeval import chrf_score\n\nreference = \"The quick brown fox\"\nhypothesis = \"The quick brown dog\"\n\n# chrF (character-level F-score)\nscore = chrf_score(reference, hypothesis)\nprint(f\"chrF: {score:.4f}\")\n\n# chrF++ (with word n-grams)\nscore_pp = chrf_score(reference, hypothesis, word_order=2)\nprint(f\"chrF++: {score_pp:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#bertscore","title":"BERTScore","text":"<p>BERTScore uses contextual embeddings for semantic similarity.</p> <pre><code>from pyeval import bert_score\n\nreference = \"The weather is beautiful today\"\nhypothesis = \"It's a lovely day outside\"\n\n# BERTScore (semantic similarity)\nscore = bert_score(reference, hypothesis)\nprint(f\"BERTScore Precision: {score['precision']:.4f}\")\nprint(f\"BERTScore Recall:    {score['recall']:.4f}\")\nprint(f\"BERTScore F1:        {score['f1']:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#text-similarity","title":"\ud83d\udd24 Text Similarity","text":""},{"location":"examples/nlp/#word-level-metrics","title":"Word-Level Metrics","text":"<pre><code>from pyeval import (\n    jaccard_similarity,\n    cosine_similarity,\n    text_similarity,\n    word_error_rate\n)\n\ntext1 = \"The quick brown fox jumps over the lazy dog\"\ntext2 = \"A fast brown fox leaps over the lazy dog\"\n\n# Jaccard similarity (word overlap)\njaccard = jaccard_similarity(text1.split(), text2.split())\nprint(f\"Jaccard: {jaccard:.4f}\")\n\n# Text similarity (composite score)\nsim = text_similarity(text1, text2)\nprint(f\"Text Similarity: {sim:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#corpus-evaluation","title":"\ud83d\udcca Corpus Evaluation","text":""},{"location":"examples/nlp/#evaluating-a-translation-system","title":"Evaluating a Translation System","text":"<pre><code>from pyeval import corpus_bleu, rouge_score, ter_score, NLPMetrics\n\n# Corpus data\nreferences = [\n    \"The cat sat on the mat.\",\n    \"Hello, how are you today?\",\n    \"Machine learning is transforming industries.\",\n    \"The weather is nice today.\",\n    \"I love programming in Python.\"\n]\n\nhypotheses = [\n    \"A cat was sitting on the mat.\",\n    \"Hello, how are you?\",\n    \"Machine learning is revolutionizing industries.\",\n    \"Today the weather is nice.\",\n    \"I enjoy coding in Python.\"\n]\n\nprint(\"=== Corpus Evaluation ===\\n\")\n\n# BLEU (corpus-level)\nbleu = corpus_bleu([[r] for r in references], hypotheses)\nprint(f\"Corpus BLEU: {bleu:.4f}\")\n\n# Average ROUGE\nrouge_scores = []\nfor ref, hyp in zip(references, hypotheses):\n    rouge = rouge_score(ref, hyp)\n    rouge_scores.append(rouge['l']['f'])\navg_rouge = sum(rouge_scores) / len(rouge_scores)\nprint(f\"Average ROUGE-L F1: {avg_rouge:.4f}\")\n\n# Average TER\nter_scores = [ter_score(ref, hyp) for ref, hyp in zip(references, hypotheses)]\navg_ter = sum(ter_scores) / len(ter_scores)\nprint(f\"Average TER: {avg_ter:.4f}\")\n\n# Using NLPMetrics class\nmetrics = NLPMetrics()\nresults = metrics.compute(references, hypotheses)\nprint(f\"\\n=== All NLP Metrics ===\")\nfor name, value in results.items():\n    if isinstance(value, float):\n        print(f\"{name}: {value:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#summarization-evaluation","title":"\ud83d\udcdd Summarization Evaluation","text":"<pre><code>from pyeval import rouge_score, bleu_score\n\n# Original document\ndocument = \"\"\"\nMachine learning is a subset of artificial intelligence that provides \nsystems the ability to automatically learn and improve from experience \nwithout being explicitly programmed. Machine learning focuses on the \ndevelopment of computer programs that can access data and use it to \nlearn for themselves.\n\"\"\"\n\n# Reference summary (human-written)\nreference_summary = \"\"\"\nMachine learning enables systems to learn from experience automatically, \nwithout explicit programming, by developing programs that access and learn from data.\n\"\"\"\n\n# System-generated summaries\nsummaries = {\n    'Extractive': \"Machine learning is a subset of artificial intelligence. It focuses on developing programs that access data to learn.\",\n    'Abstractive': \"ML allows computers to learn automatically from data without being explicitly programmed.\",\n    'Short': \"Machine learning enables automatic learning from data.\",\n}\n\nprint(\"Summarization Evaluation:\")\nprint(\"=\" * 60)\n\nfor name, summary in summaries.items():\n    rouge = rouge_score(reference_summary, summary)\n    bleu = bleu_score(reference_summary, summary)\n\n    print(f\"\\n{name} Summary:\")\n    print(f\"  ROUGE-1 F1: {rouge['1']['f']:.4f}\")\n    print(f\"  ROUGE-2 F1: {rouge['2']['f']:.4f}\")\n    print(f\"  ROUGE-L F1: {rouge['l']['f']:.4f}\")\n    print(f\"  BLEU:       {bleu:.4f}\")\n</code></pre>"},{"location":"examples/nlp/#machine-translation-evaluation","title":"\ud83c\udf10 Machine Translation Evaluation","text":"<pre><code>from pyeval import bleu_score, ter_score, chrf_score, meteor_score\n\n# Source (for reference)\nsource = \"La casa es grande y bonita.\"\n\n# Reference translations\nreferences = [\n    \"The house is big and beautiful.\",\n    \"The home is large and pretty.\",\n]\n\n# System translations from different MT systems\nmt_outputs = {\n    'Google Translate': \"The house is big and beautiful.\",\n    'DeepL': \"The house is large and beautiful.\",\n    'Custom NMT': \"The home is big and nice.\",\n    'Bad System': \"House big beautiful.\",\n}\n\nprint(\"Machine Translation Evaluation:\")\nprint(\"=\" * 70)\nprint(f\"{'System':&lt;20} {'BLEU':&gt;10} {'TER':&gt;10} {'chrF':&gt;10} {'METEOR':&gt;10}\")\nprint(\"-\" * 70)\n\nfor system, output in mt_outputs.items():\n    bleu = bleu_score(references, output)\n    ter = ter_score(references[0], output)\n    chrf = chrf_score(references[0], output)\n    meteor = meteor_score(references, output)\n\n    print(f\"{system:&lt;20} {bleu:&gt;10.4f} {ter:&gt;10.4f} {chrf:&gt;10.4f} {meteor:&gt;10.4f}\")\n</code></pre>"},{"location":"examples/nlp/#detailed-analysis","title":"\ud83d\udd0d Detailed Analysis","text":""},{"location":"examples/nlp/#n-gram-analysis","title":"N-gram Analysis","text":"<pre><code>from pyeval import ngrams, get_word_frequencies\n\ntext = \"the quick brown fox jumps over the lazy dog\"\n\n# Generate n-grams\nunigrams = list(ngrams(text.split(), 1))\nbigrams = list(ngrams(text.split(), 2))\ntrigrams = list(ngrams(text.split(), 3))\n\nprint(\"N-gram Analysis:\")\nprint(f\"Unigrams: {unigrams}\")\nprint(f\"Bigrams:  {bigrams}\")\nprint(f\"Trigrams: {trigrams}\")\n\n# Word frequencies\nfreq = get_word_frequencies(text)\nprint(f\"\\nWord Frequencies: {freq}\")\n</code></pre>"},{"location":"examples/nlp/#length-and-coverage-analysis","title":"Length and Coverage Analysis","text":"<pre><code>from pyeval import tokenize, word_count\n\nreferences = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning is transforming industries.\",\n]\n\nhypotheses = [\n    \"A fast brown fox leaps over the lazy dog.\",\n    \"ML is revolutionizing business.\",\n]\n\nprint(\"\\nLength Analysis:\")\nprint(\"-\" * 50)\n\nfor i, (ref, hyp) in enumerate(zip(references, hypotheses)):\n    ref_len = word_count(ref)\n    hyp_len = word_count(hyp)\n    ratio = hyp_len / ref_len if ref_len &gt; 0 else 0\n\n    print(f\"Pair {i+1}:\")\n    print(f\"  Reference length: {ref_len} words\")\n    print(f\"  Hypothesis length: {hyp_len} words\")\n    print(f\"  Length ratio: {ratio:.2f}\")\n</code></pre>"},{"location":"examples/quickstart/","title":"Quick Examples","text":"<p>Get started with PyEval in seconds with these copy-paste examples.</p>"},{"location":"examples/quickstart/#one-liner-examples","title":"\ud83d\ude80 One-Liner Examples","text":""},{"location":"examples/quickstart/#classification","title":"Classification","text":"<pre><code>from pyeval import accuracy_score, f1_score\naccuracy_score([1, 0, 1, 1], [1, 0, 0, 1])  # 0.75\nf1_score([1, 0, 1, 1], [1, 0, 0, 1])        # 0.8\n</code></pre>"},{"location":"examples/quickstart/#regression","title":"Regression","text":"<pre><code>from pyeval import mean_squared_error, r2_score\nmean_squared_error([3.0, -0.5, 2.0], [2.5, 0.0, 2.0])  # 0.1667\nr2_score([3.0, -0.5, 2.0, 7.0], [2.5, 0.0, 2.1, 7.8])  # 0.948\n</code></pre>"},{"location":"examples/quickstart/#nlp","title":"NLP","text":"<pre><code>from pyeval import bleu_score, rouge_score\nbleu_score(\"the cat sat on mat\", \"the cat is on the mat\")  # 0.52\nrouge_score(\"the cat sat on mat\", \"the cat is on the mat\")  # {'1': {...}, '2': {...}, 'l': {...}}\n</code></pre>"},{"location":"examples/quickstart/#llm","title":"LLM","text":"<pre><code>from pyeval import toxicity_score, coherence_score\ntoxicity_score(\"This is a helpful response\")  # {'toxicity_score': 0.02, 'is_toxic': False}\ncoherence_score(\"ML is AI. It learns from data.\")  # {'coherence_score': 0.85}\n</code></pre>"},{"location":"examples/quickstart/#complete-examples","title":"\ud83d\udcca Complete Examples","text":""},{"location":"examples/quickstart/#example-1-binary-classification","title":"Example 1: Binary Classification","text":"<pre><code>from pyeval import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n    roc_auc_score,\n    ClassificationMetrics\n)\n\n# Data\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\ny_prob = [0.9, 0.1, 0.4, 0.8, 0.2, 0.9, 0.6, 0.1, 0.85, 0.3]\n\n# Individual metrics\nprint(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")   # 0.7000\nprint(f\"Precision: {precision_score(y_true, y_pred):.4f}\") # 0.6667\nprint(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")    # 0.6667\nprint(f\"F1 Score:  {f1_score(y_true, y_pred):.4f}\")        # 0.6667\nprint(f\"ROC AUC:   {roc_auc_score(y_true, y_prob):.4f}\")   # 0.8500\n\n# Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\nprint(f\"Confusion Matrix: {cm}\")\n# [[3, 1], [2, 4]] =&gt; TN=3, FP=1, FN=2, TP=4\n\n# All at once\nmetrics = ClassificationMetrics()\nresults = metrics.compute(y_true, y_pred)\nprint(results)\n</code></pre>"},{"location":"examples/quickstart/#example-2-multi-class-classification","title":"Example 2: Multi-Class Classification","text":"<pre><code>from pyeval import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix\n)\n\n# Multi-class data (3 classes: 0, 1, 2)\ny_true = [0, 1, 2, 0, 1, 2, 0, 1, 2]\ny_pred = [0, 2, 1, 0, 0, 2, 0, 1, 2]\n\n# Accuracy (same for all)\nprint(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")  # 0.5556\n\n# Macro average (treat all classes equally)\nprint(f\"Precision (macro): {precision_score(y_true, y_pred, average='macro'):.4f}\")\nprint(f\"Recall (macro):    {recall_score(y_true, y_pred, average='macro'):.4f}\")\nprint(f\"F1 (macro):        {f1_score(y_true, y_pred, average='macro'):.4f}\")\n\n# Weighted average (by class frequency)\nprint(f\"F1 (weighted):     {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n</code></pre>"},{"location":"examples/quickstart/#example-3-text-generation-quality","title":"Example 3: Text Generation Quality","text":"<pre><code>from pyeval import bleu_score, rouge_score, meteor_score, ter_score\n\nreference = \"The quick brown fox jumps over the lazy dog\"\nhypothesis = \"A fast brown fox leaps over the lazy dog\"\n\n# BLEU (0-1, higher is better)\nbleu = bleu_score(reference, hypothesis)\nprint(f\"BLEU: {bleu:.4f}\")\n\n# ROUGE (precision, recall, f1 for n-grams)\nrouge = rouge_score(reference, hypothesis)\nprint(f\"ROUGE-1 F1: {rouge['1']['f']:.4f}\")\nprint(f\"ROUGE-2 F1: {rouge['2']['f']:.4f}\")\nprint(f\"ROUGE-L F1: {rouge['l']['f']:.4f}\")\n\n# METEOR\nmeteor = meteor_score(reference, hypothesis)\nprint(f\"METEOR: {meteor:.4f}\")\n\n# TER (0-\u221e, lower is better)\nter = ter_score(reference, hypothesis)\nprint(f\"TER: {ter:.4f}\")\n</code></pre>"},{"location":"examples/quickstart/#example-4-llm-response-evaluation","title":"Example 4: LLM Response Evaluation","text":"<pre><code>from pyeval import (\n    toxicity_score,\n    coherence_score,\n    hallucination_score,\n    bias_detection_score,\n    LLMMetrics\n)\n\nresponse = \"\"\"\nMachine learning is a subset of artificial intelligence \nthat enables computers to learn from data. It has \napplications in image recognition, natural language \nprocessing, and recommendation systems.\n\"\"\"\n\ncontext = \"\"\"\nMachine learning is a type of AI that allows software \nto learn from data. Common applications include image \nclassification and NLP.\n\"\"\"\n\n# Toxicity check\ntox = toxicity_score(response)\nprint(f\"Toxic: {tox['is_toxic']}, Score: {tox['toxicity_score']:.4f}\")\n\n# Coherence check\ncoh = coherence_score(response)\nprint(f\"Coherence: {coh['coherence_score']:.4f}\")\n\n# Hallucination detection\nhall = hallucination_score(response, context)\nprint(f\"Hallucination: {hall['hallucination_score']:.4f}\")\n\n# All LLM metrics at once\nmetrics = LLMMetrics()\nresults = metrics.compute(response, context=context)\n</code></pre>"},{"location":"examples/quickstart/#example-5-rag-pipeline-evaluation","title":"Example 5: RAG Pipeline Evaluation","text":"<pre><code>from pyeval import (\n    context_relevance,\n    groundedness_score,\n    faithfulness_score,\n    answer_correctness,\n    RAGMetrics\n)\n\nquery = \"What is machine learning?\"\ncontext = \"\"\"\nMachine learning is a subset of artificial intelligence (AI) \nthat enables systems to automatically learn and improve from \nexperience without being explicitly programmed. ML algorithms \nbuild mathematical models based on training data.\n\"\"\"\nresponse = \"Machine learning is an AI technique that allows computers to learn from data.\"\nground_truth = \"Machine learning is a type of artificial intelligence.\"\n\n# Individual metrics\nprint(f\"Context Relevance: {context_relevance(query, context):.4f}\")\nprint(f\"Groundedness:      {groundedness_score(response, context):.4f}\")\nprint(f\"Faithfulness:      {faithfulness_score(response, context):.4f}\")\nprint(f\"Correctness:       {answer_correctness(response, ground_truth):.4f}\")\n\n# All RAG metrics at once\nmetrics = RAGMetrics()\nresults = metrics.compute(\n    query=query,\n    context=context,\n    response=response,\n    ground_truth=ground_truth\n)\n</code></pre>"},{"location":"examples/quickstart/#example-6-fairness-evaluation","title":"Example 6: Fairness Evaluation","text":"<pre><code>from pyeval import (\n    demographic_parity,\n    equalized_odds,\n    disparate_impact,\n    FairnessMetrics\n)\n\n# Predictions and sensitive attribute\ny_true = [1, 1, 0, 0, 1, 1, 0, 0]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 0]\nsensitive = ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']\n\n# Demographic Parity (selection rate difference)\ndp = demographic_parity(y_pred, sensitive)\nprint(f\"DP Difference: {dp['dp_difference']:.4f}\")\nprint(f\"Group A rate:  {dp['rates']['A']:.4f}\")\nprint(f\"Group B rate:  {dp['rates']['B']:.4f}\")\n\n# Equalized Odds (TPR/FPR equality)\neo = equalized_odds(y_true, y_pred, sensitive)\nprint(f\"EO Difference: {eo['eo_difference']:.4f}\")\n\n# Disparate Impact (ratio of selection rates)\ndi = disparate_impact(y_pred, sensitive)\nprint(f\"Disparate Impact Ratio: {di['di_ratio']:.4f}\")\n# Rule: ratio should be &gt; 0.8 (80% rule)\n</code></pre>"},{"location":"examples/quickstart/#example-7-recommender-system-evaluation","title":"Example 7: Recommender System Evaluation","text":"<pre><code>from pyeval import (\n    precision_at_k,\n    recall_at_k,\n    ndcg_at_k,\n    mean_reciprocal_rank,\n    mean_average_precision,\n    RecommenderMetrics\n)\n\n# User recommendations (lists of item IDs)\nrecommendations = [\n    [1, 5, 3, 8, 2],  # User 1: recommended items\n    [4, 2, 7, 1, 9],  # User 2\n    [3, 6, 1, 8, 5],  # User 3\n]\n\n# Ground truth relevant items\nrelevant = [\n    [1, 2, 10],       # User 1: actually relevant items\n    [4, 7, 9],        # User 2\n    [6, 8],           # User 3\n]\n\nk = 3  # Evaluate top-3 recommendations\n\nprint(f\"Precision@{k}: {precision_at_k(recommendations, relevant, k):.4f}\")\nprint(f\"Recall@{k}:    {recall_at_k(recommendations, relevant, k):.4f}\")\nprint(f\"NDCG@{k}:      {ndcg_at_k(recommendations, relevant, k):.4f}\")\nprint(f\"MRR:           {mean_reciprocal_rank(recommendations, relevant):.4f}\")\nprint(f\"MAP:           {mean_average_precision(recommendations, relevant):.4f}\")\n</code></pre>"},{"location":"examples/quickstart/#example-8-speech-recognition-evaluation","title":"Example 8: Speech Recognition Evaluation","text":"<pre><code>from pyeval import (\n    word_error_rate,\n    character_error_rate,\n    sentence_error_rate,\n    SpeechMetrics\n)\n\n# Reference (ground truth) and hypothesis (ASR output)\nreferences = [\n    \"the quick brown fox jumps over the lazy dog\",\n    \"hello world how are you today\",\n    \"speech recognition is amazing\"\n]\nhypotheses = [\n    \"the quick brown fox jumped over a lazy dog\",\n    \"hello world how are you\",\n    \"speech recognition is amazing\"\n]\n\n# Per-utterance WER\nfor ref, hyp in zip(references, hypotheses):\n    wer = word_error_rate(ref, hyp)\n    cer = character_error_rate(ref, hyp)\n    print(f\"WER: {wer:.4f}, CER: {cer:.4f}\")\n\n# Corpus-level\nser = sentence_error_rate(references, hypotheses)\nprint(f\"Sentence Error Rate: {ser:.4f}\")\n\n# All metrics\nmetrics = SpeechMetrics()\nresults = metrics.compute(references, hypotheses)\n</code></pre>"},{"location":"examples/quickstart/#using-pipelines","title":"\ud83d\udd04 Using Pipelines","text":"<pre><code>from pyeval import Pipeline, accuracy_score, precision_score, recall_score, f1_score\n\n# Create evaluation pipeline\npipeline = (\n    Pipeline()\n    .add_metric('accuracy', accuracy_score)\n    .add_metric('precision', precision_score)\n    .add_metric('recall', recall_score)\n    .add_metric('f1', f1_score)\n)\n\n# Run on data\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]\n\nresults = pipeline.run(y_true, y_pred)\nprint(results)\n# {'accuracy': 0.8, 'precision': 0.667, 'recall': 0.667, 'f1': 0.667}\n</code></pre>"},{"location":"examples/quickstart/#visualization","title":"\ud83d\udcc8 Visualization","text":"<pre><code>from pyeval import confusion_matrix, classification_report\n\ny_true = [1, 0, 1, 1, 0, 1, 0, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0]\n\n# ASCII confusion matrix\ncm = confusion_matrix(y_true, y_pred, display=True)\n\n# Classification report\nreport = classification_report(y_true, y_pred)\nprint(report)\n</code></pre>"},{"location":"examples/rag/","title":"RAG Examples","text":"<p>Comprehensive examples for Retrieval-Augmented Generation evaluation with PyEval.</p>"},{"location":"examples/rag/#rag-pipeline-overview","title":"\ud83d\udd0d RAG Pipeline Overview","text":"<p>RAG (Retrieval-Augmented Generation) combines retrieval and generation. Evaluation covers:</p> <ol> <li>Retrieval Quality - Did we retrieve relevant documents?</li> <li>Generation Quality - Is the answer faithful to context?</li> <li>End-to-End Quality - Is the final answer correct?</li> </ol> <pre><code>Query \u2192 Retriever \u2192 Context \u2192 Generator \u2192 Response\n         \u2193                      \u2193\n    Retrieval Metrics    Generation Metrics\n</code></pre>"},{"location":"examples/rag/#retrieval-evaluation","title":"\ud83d\udcca Retrieval Evaluation","text":""},{"location":"examples/rag/#context-relevance","title":"Context Relevance","text":"<p>Measure how relevant the retrieved context is to the query.</p> <pre><code>from pyeval import context_relevance\n\nquery = \"What are the benefits of machine learning?\"\n\n# Highly relevant context\nrelevant_context = \"\"\"\nMachine learning offers numerous benefits including automated decision making,\npattern recognition, predictive analytics, and the ability to process large \namounts of data efficiently. It enables personalization and improves over time.\n\"\"\"\n\n# Partially relevant context\npartial_context = \"\"\"\nMachine learning is a subset of artificial intelligence. It was first \nintroduced in the 1950s. Many companies use machine learning today.\n\"\"\"\n\n# Irrelevant context\nirrelevant_context = \"\"\"\nThe weather today is sunny with a high of 75 degrees. Tomorrow will \nbring clouds and possibly rain in the afternoon.\n\"\"\"\n\nprint(\"Context Relevance Scores:\")\nprint(f\"  Relevant:   {context_relevance(query, relevant_context):.4f}\")\nprint(f\"  Partial:    {context_relevance(query, partial_context):.4f}\")\nprint(f\"  Irrelevant: {context_relevance(query, irrelevant_context):.4f}\")\n</code></pre>"},{"location":"examples/rag/#retrieval-precision-recall","title":"Retrieval Precision &amp; Recall","text":"<p>Evaluate document-level retrieval quality.</p> <pre><code>from pyeval import (\n    retrieval_precision,\n    retrieval_recall,\n    retrieval_f1,\n    mean_reciprocal_rank\n)\n\n# For each query: list of retrieved doc IDs\nretrieved_docs = [\n    [1, 5, 3, 8, 2],    # Query 1: top-5 retrieved\n    [4, 2, 7, 1, 9],    # Query 2\n    [3, 6, 1, 8, 5],    # Query 3\n]\n\n# Ground truth: relevant doc IDs for each query\nrelevant_docs = [\n    [1, 2, 10],         # Query 1: actually relevant\n    [4, 7, 9],          # Query 2\n    [6, 8],             # Query 3\n]\n\nprint(\"Retrieval Metrics:\")\nprint(\"-\" * 40)\n\n# Per-query metrics\nfor i, (ret, rel) in enumerate(zip(retrieved_docs, relevant_docs)):\n    p = retrieval_precision(ret, rel)\n    r = retrieval_recall(ret, rel)\n    print(f\"Query {i+1}: Precision={p:.4f}, Recall={r:.4f}\")\n\n# Corpus-level metrics\nprint(f\"\\nCorpus Precision: {retrieval_precision(retrieved_docs[0], relevant_docs[0]):.4f}\")\nprint(f\"Corpus Recall:    {retrieval_recall(retrieved_docs[0], relevant_docs[0]):.4f}\")\n\n# MRR - rank of first relevant document\nmrr = mean_reciprocal_rank(retrieved_docs, relevant_docs)\nprint(f\"\\nMRR: {mrr:.4f}\")\n</code></pre>"},{"location":"examples/rag/#precisionk-and-ndcgk","title":"Precision@K and NDCG@K","text":"<p>Evaluate ranking quality at different cutoffs.</p> <pre><code>from pyeval import (\n    precision_at_k,\n    recall_at_k,\n    ndcg_at_k\n)\n\nretrieved = [[1, 5, 3, 8, 2, 7, 4, 9, 6, 10]]\nrelevant = [[1, 2, 3]]\n\nprint(\"Metrics at Different K:\")\nprint(\"-\" * 45)\nprint(f\"{'K':&lt;5} {'P@K':&gt;10} {'R@K':&gt;10} {'NDCG@K':&gt;10}\")\nprint(\"-\" * 45)\n\nfor k in [1, 3, 5, 10]:\n    p = precision_at_k(retrieved, relevant, k)\n    r = recall_at_k(retrieved, relevant, k)\n    n = ndcg_at_k(retrieved, relevant, k)\n    print(f\"{k:&lt;5} {p:&gt;10.4f} {r:&gt;10.4f} {n:&gt;10.4f}\")\n</code></pre>"},{"location":"examples/rag/#generation-evaluation","title":"\ud83d\udcdd Generation Evaluation","text":""},{"location":"examples/rag/#groundedness-score","title":"Groundedness Score","text":"<p>Check if the response is grounded in the provided context.</p> <pre><code>from pyeval import groundedness_score\n\ncontext = \"\"\"\nPython was created by Guido van Rossum and released in 1991. It emphasizes\ncode readability with significant indentation. Python supports multiple \nprogramming paradigms, including procedural, object-oriented, and functional.\n\"\"\"\n\n# Well-grounded response\ngrounded = \"\"\"\nPython is a programming language released in 1991. It was created by \nGuido van Rossum and is known for its readable code style.\n\"\"\"\n\n# Partially grounded\npartial = \"\"\"\nPython was created in 1991 by Guido van Rossum. It is the most popular\nprogramming language in the world and is used by millions of developers.\n\"\"\"\n\n# Ungrounded\nungrounded = \"\"\"\nPython was created in 2000 by Microsoft. It is primarily used for\nmobile app development and is known for its complex syntax.\n\"\"\"\n\nprint(\"Groundedness Scores:\")\nprint(f\"  Grounded:   {groundedness_score(grounded, context):.4f}\")\nprint(f\"  Partial:    {groundedness_score(partial, context):.4f}\")\nprint(f\"  Ungrounded: {groundedness_score(ungrounded, context):.4f}\")\n</code></pre>"},{"location":"examples/rag/#faithfulness-score","title":"Faithfulness Score","text":"<p>Measure how faithful the response is to the source context.</p> <pre><code>from pyeval import faithfulness_score\n\ncontext = \"\"\"\nThe company reported Q3 revenue of $5.2 billion, up 15% year-over-year.\nOperating income was $1.1 billion with a margin of 21%. The company \nexpects Q4 revenue between $5.5 and $5.8 billion.\n\"\"\"\n\n# Faithful summary\nfaithful = \"\"\"\nQ3 revenue reached $5.2 billion, a 15% increase from last year. \nOperating income was $1.1 billion. Q4 guidance is $5.5-5.8 billion.\n\"\"\"\n\n# Unfaithful (contains errors)\nunfaithful = \"\"\"\nQ3 revenue was $6.2 billion, up 25% year-over-year. Operating income\nreached $2 billion. The company expects Q4 revenue above $6 billion.\n\"\"\"\n\nprint(\"Faithfulness Scores:\")\nprint(f\"  Faithful:   {faithfulness_score(faithful, context):.4f}\")\nprint(f\"  Unfaithful: {faithfulness_score(unfaithful, context):.4f}\")\n</code></pre>"},{"location":"examples/rag/#answer-correctness","title":"Answer Correctness","text":"<p>Compare generated answer against ground truth.</p> <pre><code>from pyeval import answer_correctness\n\nground_truth = \"The capital of France is Paris.\"\n\n# Correct answers\ncorrect1 = \"Paris is the capital of France.\"\ncorrect2 = \"The capital city of France is Paris.\"\n\n# Partially correct\npartial = \"The capital of France is a major European city called Paris.\"\n\n# Incorrect\nwrong = \"The capital of France is Lyon.\"\n\nprint(\"Answer Correctness Scores:\")\nprint(f\"  Correct 1: {answer_correctness(correct1, ground_truth):.4f}\")\nprint(f\"  Correct 2: {answer_correctness(correct2, ground_truth):.4f}\")\nprint(f\"  Partial:   {answer_correctness(partial, ground_truth):.4f}\")\nprint(f\"  Wrong:     {answer_correctness(wrong, ground_truth):.4f}\")\n</code></pre>"},{"location":"examples/rag/#end-to-end-rag-evaluation","title":"\ud83c\udfaf End-to-End RAG Evaluation","text":""},{"location":"examples/rag/#using-ragmetrics-class","title":"Using RAGMetrics Class","text":"<pre><code>from pyeval import RAGMetrics\n\n# Sample RAG interaction\nquery = \"What are the key features of Python programming language?\"\n\ncontext = \"\"\"\nPython is a high-level, interpreted programming language known for:\n1. Simple and readable syntax\n2. Dynamic typing\n3. Extensive standard library\n4. Support for multiple paradigms (OOP, functional, procedural)\n5. Large ecosystem of third-party packages\n6. Strong community support\n\"\"\"\n\nresponse = \"\"\"\nPython's key features include simple and readable syntax, dynamic typing,\nan extensive standard library, and support for multiple programming paradigms\nlike object-oriented and functional programming. It also has a large ecosystem\nof packages and strong community support.\n\"\"\"\n\nground_truth = \"\"\"\nPython features include readable syntax, dynamic typing, extensive libraries,\nmultiple paradigm support, and a large package ecosystem.\n\"\"\"\n\n# Compute all RAG metrics\nmetrics = RAGMetrics()\nresults = metrics.compute(\n    query=query,\n    context=context,\n    response=response,\n    ground_truth=ground_truth\n)\n\nprint(\"=== RAG Evaluation Results ===\\n\")\nfor metric, value in results.items():\n    if isinstance(value, float):\n        print(f\"{metric}: {value:.4f}\")\n    else:\n        print(f\"{metric}: {value}\")\n</code></pre>"},{"location":"examples/rag/#evaluating-multiple-rag-samples","title":"Evaluating Multiple RAG Samples","text":"<pre><code>from pyeval import (\n    context_relevance,\n    groundedness_score,\n    faithfulness_score,\n    answer_correctness,\n    mean\n)\n\n# Test dataset\ntest_cases = [\n    {\n        'query': \"What is machine learning?\",\n        'context': \"Machine learning is a subset of AI that enables systems to learn from data.\",\n        'response': \"Machine learning is an AI technique that allows computers to learn from data.\",\n        'ground_truth': \"Machine learning is a type of artificial intelligence.\"\n    },\n    {\n        'query': \"Who created Python?\",\n        'context': \"Python was created by Guido van Rossum and released in 1991.\",\n        'response': \"Guido van Rossum created Python in 1991.\",\n        'ground_truth': \"Guido van Rossum created Python.\"\n    },\n    {\n        'query': \"What is deep learning?\",\n        'context': \"Deep learning uses neural networks with multiple layers to learn representations.\",\n        'response': \"Deep learning is a type of ML using multi-layer neural networks.\",\n        'ground_truth': \"Deep learning uses neural networks with many layers.\"\n    },\n]\n\nprint(\"RAG Evaluation Report\")\nprint(\"=\" * 75)\nprint(f\"{'#':&lt;3} {'Relevance':&gt;12} {'Groundedness':&gt;14} {'Faithfulness':&gt;14} {'Correctness':&gt;13}\")\nprint(\"-\" * 75)\n\nscores = {'relevance': [], 'groundedness': [], 'faithfulness': [], 'correctness': []}\n\nfor i, tc in enumerate(test_cases):\n    rel = context_relevance(tc['query'], tc['context'])\n    gnd = groundedness_score(tc['response'], tc['context'])\n    fai = faithfulness_score(tc['response'], tc['context'])\n    cor = answer_correctness(tc['response'], tc['ground_truth'])\n\n    scores['relevance'].append(rel)\n    scores['groundedness'].append(gnd)\n    scores['faithfulness'].append(fai)\n    scores['correctness'].append(cor)\n\n    print(f\"{i+1:&lt;3} {rel:&gt;12.4f} {gnd:&gt;14.4f} {fai:&gt;14.4f} {cor:&gt;13.4f}\")\n\nprint(\"-\" * 75)\nprint(f\"{'AVG':&lt;3} {mean(scores['relevance']):&gt;12.4f} {mean(scores['groundedness']):&gt;14.4f} {mean(scores['faithfulness']):&gt;14.4f} {mean(scores['correctness']):&gt;13.4f}\")\n</code></pre>"},{"location":"examples/rag/#rag-pipeline-comparison","title":"\ud83d\udd04 RAG Pipeline Comparison","text":"<pre><code>from pyeval import (\n    context_relevance,\n    groundedness_score,\n    answer_correctness,\n    mean\n)\n\n# Same queries, different RAG configurations\nquery = \"What are the benefits of exercise?\"\n\n# Configuration 1: Dense retriever + GPT-4\nconfig1 = {\n    'name': 'Dense + GPT-4',\n    'context': \"Exercise improves cardiovascular health, builds muscle strength, enhances mood through endorphin release, and helps maintain healthy weight.\",\n    'response': \"Exercise benefits include improved heart health, stronger muscles, better mood from endorphins, and weight management.\"\n}\n\n# Configuration 2: BM25 retriever + GPT-3.5\nconfig2 = {\n    'name': 'BM25 + GPT-3.5',\n    'context': \"Physical activity is good for health. Exercise helps the body. Movement is important for wellness.\",\n    'response': \"Exercise is good for your health and helps your body stay well.\"\n}\n\n# Configuration 3: Hybrid retriever + Claude\nconfig3 = {\n    'name': 'Hybrid + Claude',\n    'context': \"Regular exercise provides numerous benefits: cardiovascular improvement, muscle development, mental health benefits through endorphin release, weight control, and improved sleep quality.\",\n    'response': \"The benefits of exercise include cardiovascular health, muscle strength, mental wellness via endorphins, weight management, and better sleep.\"\n}\n\nground_truth = \"Exercise improves heart health, builds muscles, boosts mood, and helps with weight.\"\n\nconfigs = [config1, config2, config3]\n\nprint(\"RAG Configuration Comparison\")\nprint(\"=\" * 65)\nprint(f\"{'Configuration':&lt;20} {'Relevance':&gt;12} {'Grounded':&gt;12} {'Correct':&gt;12}\")\nprint(\"-\" * 65)\n\nfor cfg in configs:\n    rel = context_relevance(query, cfg['context'])\n    gnd = groundedness_score(cfg['response'], cfg['context'])\n    cor = answer_correctness(cfg['response'], ground_truth)\n\n    print(f\"{cfg['name']:&lt;20} {rel:&gt;12.4f} {gnd:&gt;12.4f} {cor:&gt;12.4f}\")\n</code></pre>"},{"location":"examples/rag/#production-monitoring-dashboard","title":"\ud83d\udcc8 Production Monitoring Dashboard","text":"<pre><code>from pyeval import (\n    context_relevance,\n    groundedness_score,\n    faithfulness_score,\n    answer_correctness,\n    mean,\n    std\n)\n\ndef evaluate_rag_batch(samples):\n    \"\"\"Evaluate a batch of RAG samples and return statistics.\"\"\"\n    metrics = {\n        'context_relevance': [],\n        'groundedness': [],\n        'faithfulness': [],\n        'answer_correctness': []\n    }\n\n    for sample in samples:\n        metrics['context_relevance'].append(\n            context_relevance(sample['query'], sample['context'])\n        )\n        metrics['groundedness'].append(\n            groundedness_score(sample['response'], sample['context'])\n        )\n        metrics['faithfulness'].append(\n            faithfulness_score(sample['response'], sample['context'])\n        )\n        if 'ground_truth' in sample:\n            metrics['answer_correctness'].append(\n                answer_correctness(sample['response'], sample['ground_truth'])\n            )\n\n    return metrics\n\n# Simulated production batch\nproduction_samples = [\n    {\n        'query': \"What is Python?\",\n        'context': \"Python is a programming language created by Guido van Rossum.\",\n        'response': \"Python is a programming language created in 1991.\",\n        'ground_truth': \"Python is a programming language.\"\n    },\n    # Add more samples...\n]\n\n# Evaluate\nresults = evaluate_rag_batch(production_samples)\n\n# Generate report\nprint(\"=\" * 50)\nprint(\"RAG PRODUCTION MONITORING DASHBOARD\")\nprint(\"=\" * 50)\nprint(f\"Samples Evaluated: {len(production_samples)}\")\nprint(\"-\" * 50)\n\nfor metric, scores in results.items():\n    if scores:\n        avg = mean(scores)\n        s = std(scores) if len(scores) &gt; 1 else 0\n        mn = min(scores)\n        mx = max(scores)\n        print(f\"\\n{metric.replace('_', ' ').title()}:\")\n        print(f\"  Mean: {avg:.4f} \u00b1 {s:.4f}\")\n        print(f\"  Range: [{mn:.4f}, {mx:.4f}]\")\n\n        # Alerts\n        if metric == 'groundedness' and avg &lt; 0.7:\n            print(\"  \u26a0\ufe0f WARNING: Low groundedness detected!\")\n        if metric == 'faithfulness' and avg &lt; 0.7:\n            print(\"  \u26a0\ufe0f WARNING: Low faithfulness detected!\")\n</code></pre>"}]}