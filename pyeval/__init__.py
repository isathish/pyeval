"""
PyEval - A Comprehensive Python Evaluation Package
===================================================

A pure Python evaluation library for ML, NLP, LLM, RAG, Fairness, Speech,
and Recommendation systems - no third-party dependencies required.

Features:
---------
- Classical ML Metrics (Classification, Regression, Clustering)
- NLP Metrics (BLEU, ROUGE, METEOR)
- LLM Evaluation (Hallucination, Relevancy, Faithfulness, Toxicity)
- RAG Evaluation (Context Relevance, Answer Correctness)
- Fairness Metrics (Demographic Parity, Equalized Odds)
- Speech Metrics (WER, CER)
- Recommender Metrics (Precision@K, Recall@K, NDCG)
- Experiment Tracking & Reporting
- Design Patterns (Strategy, Factory, Builder, Observer)
- Decorators (Timing, Caching, Validation, Logging)
- Callbacks (Progress, Thresholds, History)
- Validators (Type, Range, Schema)
- Pipelines (Fluent API for evaluation workflows)
- Functional Utilities (Monads, Composition, Currying)
- Aggregators (Statistical, Cross-Validation, Ensemble)

Author: PyEval Team
License: MIT
"""

__version__ = "1.0.0"
__author__ = "PyEval Team"

# Core metrics
from pyeval.ml import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    mean_squared_error,
    root_mean_squared_error,
    mean_absolute_error,
    r2_score,
    silhouette_score,
    ClassificationMetrics,
    RegressionMetrics,
    ClusteringMetrics,
    # New ML metrics
    balanced_accuracy_score,
    log_loss,
    brier_score,
    hamming_loss,
    jaccard_score,
    top_k_accuracy,
    expected_calibration_error,
    mean_squared_log_error,
    symmetric_mape,
    huber_loss,
    quantile_loss,
    normalized_rmse,
    adjusted_rand_score,
    normalized_mutual_info_score,
    homogeneity_score,
    completeness_score as cluster_completeness_score,
    v_measure_score,
    fowlkes_mallows_score,
    # TPR/FPR
    true_positive_rate,
    false_positive_rate,
    # Additional classification metrics
    specificity_score,
    matthews_corrcoef,
    cohen_kappa_score,
    roc_curve,
    precision_recall_curve,
    # Additional regression metrics
    mean_absolute_percentage_error,
    explained_variance_score,
)

from pyeval.nlp import (
    bleu_score,
    rouge_score,
    meteor_score,
    NLPMetrics,
    # New NLP metrics
    chrf_score,
    text_entropy,
    repetition_ratio,
    compression_ratio,
    coverage_score,
    density_score,
    word_mover_distance_approx,
    lexical_diversity,
    sentence_bleu,
    # TER and Distinct-N
    ter_score,
    distinct_n,
)

from pyeval.llm import (
    hallucination_score,
    answer_relevancy,
    faithfulness_score,
    toxicity_score,
    coherence_score,
    fluency_score,
    completeness_score,
    factual_consistency,
    LLMMetrics,
    # New LLM metrics
    bias_detection,
    instruction_following_score,
    multi_turn_coherence,
    summarization_quality,
    response_diversity,
    # Consistency score
    consistency_score,
)

from pyeval.rag import (
    context_relevance,
    context_precision,
    context_recall,
    answer_correctness,
    retrieval_precision,
    retrieval_recall,
    groundedness_score,
    noise_robustness,
    rag_faithfulness,
    RAGMetrics,
    # New RAG metrics
    context_entity_recall,
    answer_attribution,
    context_utilization,
    question_answer_relevance,
    rag_pipeline_score,
    # Context coverage and faithful answer rate
    context_coverage,
    faithful_answer_rate,
    # Retrieval metrics
    retrieval_f1,
    mean_reciprocal_rank as retrieval_mrr,
)

from pyeval.fairness import (
    demographic_parity,
    equalized_odds,
    equal_opportunity,
    disparate_impact,
    statistical_parity_difference,
    calibration_by_group,
    individual_fairness,
    counterfactual_fairness_check,
    comprehensive_fairness_analysis,
    FairnessMetrics,
    # TPR/FPR difference
    true_positive_rate_difference,
    false_positive_rate_difference,
)

from pyeval.speech import (
    word_error_rate,
    character_error_rate,
    SpeechMetrics,
    # New speech metrics
    corpus_wer,
    corpus_cer,
    match_error_rate,
    word_information_lost,
    sentence_error_rate,
    real_time_factor,
    recognition_accuracy,
    align_texts,
    slot_error_rate,
    intent_accuracy,
    phoneme_error_rate,
    diarization_error_rate,
    keyword_spotting_metrics,
    mean_opinion_score,
    speech_intelligibility_index,
    fluency_score as speech_fluency_score,
)

from pyeval.recommender import (
    precision_at_k,
    recall_at_k,
    ndcg_at_k,
    mean_average_precision,
    hit_rate,
    RecommenderMetrics,
    # New recommender metrics
    f1_at_k,
    mean_precision_at_k,
    mean_recall_at_k,
    mean_ndcg_at_k,
    mean_hit_rate,
    reciprocal_rank,
    mean_reciprocal_rank,
    catalog_coverage,
    user_coverage,
    intra_list_diversity,
    personalization,
    novelty,
    serendipity,
    gini_index,
    expected_percentile_ranking,
    auc_score,
    inter_list_diversity,
    entropy_diversity,
    surprisal,
    accuracy_at_k,
    ranking_correlation,
    beyond_accuracy_metrics,
)

# Unified evaluator
from pyeval.evaluator import Evaluator, EvaluationReport

# Experiment tracking
from pyeval.tracking import ExperimentTracker

# Statistical utilities
from pyeval.utils.math_ops import (
    bootstrap_confidence_interval,
    paired_t_test,
    independent_t_test,
    wilcoxon_signed_rank_test,
    mann_whitney_u_test,
    mcnemar_test,
    cohens_d,
    hedges_g,
    glass_delta,
    correlation_coefficient,
    spearman_correlation,
)

# Visualization utilities
from pyeval.utils.viz_ops import (
    confusion_matrix_display,
    classification_report_display,
    horizontal_bar_chart,
    histogram_display,
    roc_curve_display,
    pr_curve_display,
    score_distribution_display,
    metric_comparison_table,
    progress_bar,
    sparkline,
    error_analysis_display,
)

# Decorators
from pyeval.decorators import (
    timed,
    timed_result,
    Timer,
    memoize,
    lru_cache,
    validate_inputs,
    require_same_length,
    require_non_empty,
    check_range,
    retry,
    fallback,
    logged,
    deprecated,
    compose,
    pipe,
    partial_metric,
    vectorize,
    batch_process,
    ensure_list,
    ensure_float,
    MetricRegistry,
    metric_registry,
    MetricLogger,
)

# Design Patterns
from pyeval.patterns import (
    MetricStrategy,
    AccuracyStrategy,
    PrecisionStrategy,
    RecallStrategy,
    F1Strategy,
    MetricCalculator,
    MetricType,
    MetricFactory,
    metric_factory,
    EvaluationPipelineBuilder,
    EvaluationPipeline,
    Event,
    EventData,
    EventObserver,
    LoggingObserver,
    ProgressObserver,
    MetricSubject,
    MetricComponent,
    SingleMetric,
    CompositeMetric,
    ValidationHandler,
    create_validation_chain,
    EvaluationTemplate,
    ClassificationEvaluation,
)

# Validators
from pyeval.validators import (
    ValidationResult,
    Validator,
    TypeValidator,
    ListValidator,
    DictValidator,
    NumericValidator,
    ProbabilityValidator,
    PositiveValidator,
    IntegerValidator,
    StringValidator,
    NonEmptyStringValidator,
    AllOf,
    AnyOf,
    Optional as OptionalValidator,
    PredictionValidator,
    ProbabilityArrayValidator,
    ConfusionMatrixValidator,
    validate_predictions,
    validate_probabilities,
    validate_range,
    FieldSchema,
    SchemaValidator,
)

# Callbacks
from pyeval.callbacks import (
    CallbackEvent,
    CallbackContext,
    Callback,
    CallbackManager,
    LoggingCallback,
    ProgressCallback,
    ThresholdCallback,
    HistoryCallback,
    EarlyStoppingCallback,
    AggregationCallback,
    CompositeCallback,
    LambdaCallback,
    CallbackEvaluator,
)

# Pipelines
from pyeval.pipeline import (
    StepType,
    StepResult,
    Pipeline,
    PipelineResult,
    create_classification_pipeline,
    create_regression_pipeline,
    create_nlp_pipeline,
    PipelineRegistry,
    pipeline_registry,
)

# Functional utilities
from pyeval.functional import (
    Result,
    Option,
    try_catch,
    curry,
    identity,
    constant,
    flip,
    map_list,
    filter_list,
    reduce_list,
    fold_left,
    fold_right,
    zip_with,
    flat_map,
    group_by,
    partition,
    take,
    drop,
    take_while,
    drop_while,
    apply_metric,
    combine_metrics,
    threshold_metric,
    average_metric,
    weighted_average_metric,
)

# Aggregators
from pyeval.aggregators import (
    Aggregator,
    MeanAggregator,
    WeightedMeanAggregator,
    MedianAggregator,
    MinAggregator,
    MaxAggregator,
    SumAggregator,
    StdAggregator,
    VarianceAggregator,
    PercentileAggregator,
    GeometricMeanAggregator,
    HarmonicMeanAggregator,
    TrimmedMeanAggregator,
    WinsorizedMeanAggregator,
    AggregationResult,
    MetricAggregator,
    FoldResult,
    CrossValidationAggregator,
    EnsembleAggregator,
    create_aggregator,
)

__all__ = [
    # Version
    "__version__",
    # ML Metrics
    "accuracy_score",
    "precision_score",
    "recall_score",
    "f1_score",
    "roc_auc_score",
    "confusion_matrix",
    "mean_squared_error",
    "root_mean_squared_error",
    "mean_absolute_error",
    "r2_score",
    "silhouette_score",
    "ClassificationMetrics",
    "RegressionMetrics",
    "ClusteringMetrics",
    # New ML metrics
    "balanced_accuracy_score",
    "log_loss",
    "brier_score",
    "hamming_loss",
    "jaccard_score",
    "top_k_accuracy",
    "expected_calibration_error",
    "mean_squared_log_error",
    "symmetric_mape",
    "huber_loss",
    "quantile_loss",
    "normalized_rmse",
    "adjusted_rand_score",
    "normalized_mutual_info_score",
    "homogeneity_score",
    "cluster_completeness_score",
    "v_measure_score",
    "fowlkes_mallows_score",
    # TPR/FPR
    "true_positive_rate",
    "false_positive_rate",
    # Additional classification metrics
    "specificity_score",
    "matthews_corrcoef",
    "cohen_kappa_score",
    "roc_curve",
    "precision_recall_curve",
    # Additional regression metrics
    "mean_absolute_percentage_error",
    "explained_variance_score",
    # NLP Metrics
    "bleu_score",
    "rouge_score",
    "meteor_score",
    "NLPMetrics",
    # New NLP metrics
    "chrf_score",
    "text_entropy",
    "repetition_ratio",
    "compression_ratio",
    "coverage_score",
    "density_score",
    "word_mover_distance_approx",
    "lexical_diversity",
    "sentence_bleu",
    # TER and Distinct-N
    "ter_score",
    "distinct_n",
    # LLM Metrics
    "hallucination_score",
    "answer_relevancy",
    "faithfulness_score",
    "toxicity_score",
    "coherence_score",
    "fluency_score",
    "completeness_score",
    "factual_consistency",
    "LLMMetrics",
    # New LLM metrics
    "bias_detection",
    "instruction_following_score",
    "multi_turn_coherence",
    "summarization_quality",
    "response_diversity",
    # Consistency score
    "consistency_score",
    # RAG Metrics
    "context_relevance",
    "context_precision",
    "context_recall",
    "answer_correctness",
    "retrieval_precision",
    "retrieval_recall",
    "groundedness_score",
    "noise_robustness",
    "rag_faithfulness",
    "RAGMetrics",
    # New RAG metrics
    "context_entity_recall",
    "answer_attribution",
    "context_utilization",
    "question_answer_relevance",
    "rag_pipeline_score",
    # Context coverage and faithful answer rate
    "context_coverage",
    "faithful_answer_rate",
    # Retrieval metrics
    "retrieval_f1",
    "retrieval_mrr",
    # Fairness Metrics
    "demographic_parity",
    "equalized_odds",
    "equal_opportunity",
    "disparate_impact",
    "statistical_parity_difference",
    "calibration_by_group",
    "individual_fairness",
    "counterfactual_fairness_check",
    "comprehensive_fairness_analysis",
    "FairnessMetrics",
    # TPR/FPR difference
    "true_positive_rate_difference",
    "false_positive_rate_difference",
    # Speech Metrics
    "word_error_rate",
    "character_error_rate",
    "SpeechMetrics",
    # New speech metrics
    "corpus_wer",
    "corpus_cer",
    "match_error_rate",
    "word_information_lost",
    "sentence_error_rate",
    "real_time_factor",
    "recognition_accuracy",
    "align_texts",
    "slot_error_rate",
    "intent_accuracy",
    "phoneme_error_rate",
    "diarization_error_rate",
    "keyword_spotting_metrics",
    "mean_opinion_score",
    "speech_intelligibility_index",
    "speech_fluency_score",
    # Recommender Metrics
    "precision_at_k",
    "recall_at_k",
    "ndcg_at_k",
    "mean_average_precision",
    "hit_rate",
    "RecommenderMetrics",
    # New recommender metrics
    "f1_at_k",
    "mean_precision_at_k",
    "mean_recall_at_k",
    "mean_ndcg_at_k",
    "mean_hit_rate",
    "reciprocal_rank",
    "mean_reciprocal_rank",
    "catalog_coverage",
    "user_coverage",
    "intra_list_diversity",
    "personalization",
    "novelty",
    "serendipity",
    "gini_index",
    "expected_percentile_ranking",
    "auc_score",
    "inter_list_diversity",
    "entropy_diversity",
    "surprisal",
    "accuracy_at_k",
    "ranking_correlation",
    "beyond_accuracy_metrics",
    # Statistical Utilities
    "bootstrap_confidence_interval",
    "paired_t_test",
    "independent_t_test",
    "wilcoxon_signed_rank_test",
    "mann_whitney_u_test",
    "mcnemar_test",
    "cohens_d",
    "hedges_g",
    "glass_delta",
    "correlation_coefficient",
    "spearman_correlation",
    # Visualization Utilities
    "confusion_matrix_display",
    "classification_report_display",
    "horizontal_bar_chart",
    "histogram_display",
    "roc_curve_display",
    "pr_curve_display",
    "score_distribution_display",
    "metric_comparison_table",
    "progress_bar",
    "sparkline",
    "error_analysis_display",
    # Evaluator
    "Evaluator",
    "EvaluationReport",
    "mean_average_precision",
    "hit_rate",
    "RecommenderMetrics",
    # Evaluator
    "Evaluator",
    "EvaluationReport",
    # Tracking
    "ExperimentTracker",
    # Decorators
    "timed",
    "timed_result",
    "Timer",
    "memoize",
    "lru_cache",
    "validate_inputs",
    "require_same_length",
    "require_non_empty",
    "check_range",
    "retry",
    "fallback",
    "logged",
    "deprecated",
    "compose",
    "pipe",
    "partial_metric",
    "vectorize",
    "batch_process",
    "ensure_list",
    "ensure_float",
    "MetricRegistry",
    "metric_registry",
    "MetricLogger",
    # Design Patterns
    "MetricStrategy",
    "AccuracyStrategy",
    "PrecisionStrategy",
    "RecallStrategy",
    "F1Strategy",
    "MetricCalculator",
    "MetricType",
    "MetricFactory",
    "metric_factory",
    "EvaluationPipelineBuilder",
    "EvaluationPipeline",
    "Event",
    "EventData",
    "EventObserver",
    "LoggingObserver",
    "ProgressObserver",
    "MetricSubject",
    "MetricComponent",
    "SingleMetric",
    "CompositeMetric",
    "ValidationHandler",
    "create_validation_chain",
    "EvaluationTemplate",
    "ClassificationEvaluation",
    # Validators
    "ValidationResult",
    "Validator",
    "TypeValidator",
    "ListValidator",
    "DictValidator",
    "NumericValidator",
    "ProbabilityValidator",
    "PositiveValidator",
    "IntegerValidator",
    "StringValidator",
    "NonEmptyStringValidator",
    "AllOf",
    "AnyOf",
    "OptionalValidator",
    "PredictionValidator",
    "ProbabilityArrayValidator",
    "ConfusionMatrixValidator",
    "validate_predictions",
    "validate_probabilities",
    "validate_range",
    "FieldSchema",
    "SchemaValidator",
    # Callbacks
    "CallbackEvent",
    "CallbackContext",
    "Callback",
    "CallbackManager",
    "LoggingCallback",
    "ProgressCallback",
    "ThresholdCallback",
    "HistoryCallback",
    "EarlyStoppingCallback",
    "AggregationCallback",
    "CompositeCallback",
    "LambdaCallback",
    "CallbackEvaluator",
    # Pipelines
    "StepType",
    "StepResult",
    "Pipeline",
    "PipelineResult",
    "create_classification_pipeline",
    "create_regression_pipeline",
    "create_nlp_pipeline",
    "PipelineRegistry",
    "pipeline_registry",
    # Functional utilities
    "Result",
    "Option",
    "try_catch",
    "curry",
    "identity",
    "constant",
    "flip",
    "map_list",
    "filter_list",
    "reduce_list",
    "fold_left",
    "fold_right",
    "zip_with",
    "flat_map",
    "group_by",
    "partition",
    "take",
    "drop",
    "take_while",
    "drop_while",
    "apply_metric",
    "combine_metrics",
    "threshold_metric",
    "average_metric",
    "weighted_average_metric",
    # Aggregators
    "Aggregator",
    "MeanAggregator",
    "WeightedMeanAggregator",
    "MedianAggregator",
    "MinAggregator",
    "MaxAggregator",
    "SumAggregator",
    "StdAggregator",
    "VarianceAggregator",
    "PercentileAggregator",
    "GeometricMeanAggregator",
    "HarmonicMeanAggregator",
    "TrimmedMeanAggregator",
    "WinsorizedMeanAggregator",
    "AggregationResult",
    "MetricAggregator",
    "FoldResult",
    "CrossValidationAggregator",
    "EnsembleAggregator",
    "create_aggregator",
]
