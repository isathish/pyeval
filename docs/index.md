# PyEval

<div align="center">

![PyEval](https://img.shields.io/badge/PyEval-v1.0.0-blue?style=for-the-badge)

**A Comprehensive Pure Python Evaluation Framework**

*Evaluate everything, depend on nothing.*

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Dependencies](https://img.shields.io/badge/dependencies-none-brightgreen.svg)](.)
[![Tests](https://img.shields.io/badge/tests-302%20passed-success.svg)](.)

</div>

---

## What is PyEval?

PyEval is a **zero-dependency** evaluation library for Machine Learning, NLP, LLM, RAG, Fairness, Speech, and Recommender systems. Every metric is implemented in **pure Python** â€” no NumPy, no scikit-learn, no external packages required.

### Why Choose PyEval?

| Feature | Description |
|---------|-------------|
| ğŸš« **Zero Dependencies** | Works anywhere Python runs â€” edge devices, serverless, restricted environments |
| ğŸ“¦ **327+ Public APIs** | The most comprehensive evaluation library available |
| ğŸ”§ **Unified Interface** | Consistent API design across all domains |
| ğŸ§ª **Battle-Tested** | 302 tests ensure reliability and correctness |
| ğŸ“Š **Built-in Viz** | ASCII charts, confusion matrices, sparklines included |

---

## Quick Start

### Installation

**Using pip (recommended):**

```bash
pip install pyeval
```

**From source:**

```bash
git clone https://github.com/isathish/pyeval.git
cd pyeval
pip install -e .
```

### 30-Second Example

```python
from pyeval import accuracy_score, f1_score, bleu_score, confusion_matrix

# ML Classification
y_true = [1, 0, 1, 1, 0, 1]
y_pred = [1, 0, 0, 1, 0, 1]

print(f"Accuracy: {accuracy_score(y_true, y_pred):.2%}")  # 83.33%
print(f"F1 Score: {f1_score(y_true, y_pred):.4f}")       # 0.8571

# NLP Generation
ref = "The quick brown fox jumps over the lazy dog"
hyp = "A fast brown fox leaps over the lazy dog"
print(f"BLEU: {bleu_score(ref, hyp):.4f}")              # 0.4234

# Confusion Matrix (ASCII visualization!)
print(confusion_matrix(y_true, y_pred, display=True))
```

---

## Documentation Overview

### Getting Started

New to PyEval? Start with the [Getting Started Guide](getting-started.md) for installation and basic usage.

### API Reference

| Module | Description | Link |
|--------|-------------|------|
| **ML Metrics** | Classification, Regression, Clustering â€” 40+ metrics | [View API](api/ml.md) |
| **NLP Metrics** | BLEU, ROUGE, METEOR, TER, BERTScore, and more | [View API](api/nlp.md) |
| **LLM Evaluation** | Toxicity, Hallucination, Coherence, Bias detection | [View API](api/llm.md) |
| **RAG Evaluation** | Context Relevance, Groundedness, Faithfulness | [View API](api/rag.md) |
| **Fairness Metrics** | Demographic Parity, Equalized Odds, Disparate Impact | [View API](api/fairness.md) |
| **Speech Metrics** | WER, CER, MER, and speech quality evaluation | [View API](api/speech.md) |
| **Recommender Metrics** | Precision@K, NDCG, MAP, MRR, Diversity, Coverage | [View API](api/recommender.md) |
| **Statistical Utilities** | Hypothesis testing, confidence intervals, distributions | [View API](api/statistical.md) |
| **Visualization** | ASCII charts, sparklines, progress bars | [View API](api/visualization.md) |

### Advanced Features

| Feature | Description | Link |
|---------|-------------|------|
| **Pipelines** | Chain evaluation steps together | [Learn More](advanced/pipelines.md) |
| **Decorators** | Add validation, logging, retry logic | [Learn More](advanced/decorators.md) |
| **Validators** | Type checking and data validation | [Learn More](advanced/validators.md) |
| **Design Patterns** | Reusable patterns for evaluation | [Learn More](advanced/patterns.md) |
| **Functional Utilities** | Map, filter, reduce for metrics | [Learn More](advanced/functional.md) |

---

## Feature Highlights

### Complete Domain Coverage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PyEval v1.0.0                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ML            â”‚  NLP           â”‚  LLM           â”‚  RAG         â”‚
â”‚  â”œâ”€ classify   â”‚  â”œâ”€ bleu       â”‚  â”œâ”€ toxicity   â”‚  â”œâ”€ context  â”‚
â”‚  â”œâ”€ regress    â”‚  â”œâ”€ rouge      â”‚  â”œâ”€ coherence  â”‚  â”œâ”€ ground   â”‚
â”‚  â”œâ”€ cluster    â”‚  â”œâ”€ meteor     â”‚  â”œâ”€ hallucin   â”‚  â”œâ”€ faithful â”‚
â”‚  â””â”€ rank       â”‚  â””â”€ ter        â”‚  â””â”€ bias       â”‚  â””â”€ answer   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Fairness      â”‚  Speech        â”‚  Recommender   â”‚  Utilities   â”‚
â”‚  â”œâ”€ parity     â”‚  â”œâ”€ wer        â”‚  â”œâ”€ precision  â”‚  â”œâ”€ stats    â”‚
â”‚  â”œâ”€ equality   â”‚  â”œâ”€ cer        â”‚  â”œâ”€ ndcg       â”‚  â”œâ”€ viz      â”‚
â”‚  â””â”€ calibrate  â”‚  â””â”€ mer        â”‚  â””â”€ diversity  â”‚  â””â”€ valid    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Zero Dependencies

PyEval works in environments where other libraries can't:

- **Edge Devices** â€” Raspberry Pi, microcontrollers, IoT
- **Serverless** â€” AWS Lambda, Azure Functions, Google Cloud Functions
- **Restricted Environments** â€” Air-gapped systems, secure facilities
- **Embedded Systems** â€” No package manager required
- **Minimal Docker** â€” Tiny container images

### Consistent API Design

All metrics follow the same patterns:

```python
# Pattern 1: Simple comparison
score = metric_function(y_true, y_pred)

# Pattern 2: With options
score = metric_function(y_true, y_pred, **options)

# Pattern 3: Batch processing  
scores = [metric_function(t, p) for t, p in zip(true_batch, pred_batch)]
```

---

## Quick Examples by Domain

### Machine Learning

```python
from pyeval import (
    accuracy_score, precision_score, recall_score, f1_score,
    mean_squared_error, r2_score, silhouette_score
)

# Classification
y_true = [1, 0, 1, 1, 0, 1, 0, 0]
y_pred = [1, 0, 0, 1, 0, 1, 1, 0]

print(f"Accuracy:  {accuracy_score(y_true, y_pred):.4f}")
print(f"Precision: {precision_score(y_true, y_pred):.4f}")
print(f"Recall:    {recall_score(y_true, y_pred):.4f}")
print(f"F1:        {f1_score(y_true, y_pred):.4f}")
```

### Natural Language Processing

```python
from pyeval import bleu_score, rouge_score, meteor_score

reference = "The cat sat on the mat"
hypothesis = "A cat was sitting on the mat"

print(f"BLEU:   {bleu_score(reference, hypothesis):.4f}")
print(f"ROUGE:  {rouge_score(reference, hypothesis)}")
print(f"METEOR: {meteor_score(reference, hypothesis):.4f}")
```

### LLM Evaluation

```python
from pyeval import (
    toxicity_score, coherence_score, 
    hallucination_score, readability_score
)

text = "This is a sample generated response from an LLM."
context = "Information about the topic being discussed."

print(f"Toxicity:     {toxicity_score(text):.4f}")
print(f"Coherence:    {coherence_score(text):.4f}")
print(f"Readability:  {readability_score(text):.4f}")
```

### RAG Evaluation

```python
from pyeval import (
    context_relevance_score, groundedness_score,
    answer_relevance_score, faithfulness_score
)

query = "What is machine learning?"
context = "Machine learning is a subset of AI that enables systems to learn."
answer = "Machine learning is an AI technique for learning from data."

print(f"Context Relevance: {context_relevance_score(query, context):.4f}")
print(f"Groundedness:      {groundedness_score(answer, context):.4f}")
print(f"Answer Relevance:  {answer_relevance_score(query, answer):.4f}")
```

---

## Comparison with Other Libraries

| Feature | PyEval | scikit-learn | Evaluate (HF) | TorchMetrics |
|---------|--------|--------------|---------------|--------------|
| **Dependencies** | None | NumPy, SciPy | 15+ packages | PyTorch |
| **ML Metrics** | âœ… 40+ | âœ… 30+ | âš ï¸ Limited | âœ… 25+ |
| **NLP Metrics** | âœ… 20+ | âŒ | âœ… 20+ | âš ï¸ Limited |
| **LLM Metrics** | âœ… 15+ | âŒ | âš ï¸ Limited | âŒ |
| **RAG Metrics** | âœ… 10+ | âŒ | âŒ | âŒ |
| **Fairness** | âœ… 10+ | âŒ | âŒ | âŒ |
| **Speech** | âœ… 5+ | âŒ | âš ï¸ Limited | âŒ |
| **Recommender** | âœ… 10+ | âŒ | âŒ | âŒ |
| **Edge Deploy** | âœ… | âŒ | âŒ | âŒ |
| **Serverless** | âœ… | âš ï¸ | âš ï¸ | âŒ |

---

## Get Involved

- **[GitHub Repository](https://github.com/isathish/pyeval)** â€” Star us, report issues, contribute
- **[Contributing Guide](contributing.md)** â€” How to contribute to PyEval
- **[Changelog](changelog.md)** â€” What's new in each release

---

## License

PyEval is released under the **MIT License**. See the [LICENSE](https://github.com/isathish/pyeval/blob/main/LICENSE) file for details.

---

<div align="center">

**Made with â¤ï¸ for the ML community**

*Zero dependencies. Maximum evaluation coverage.*

</div>
